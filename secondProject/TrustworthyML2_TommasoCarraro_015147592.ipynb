{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "TrustworthyML2_TommasoCarraro_015147592.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22FjQHc6qXvh",
        "colab_type": "text"
      },
      "source": [
        "University of Helsinki, Master's Programme in Data Science  \n",
        "DATA20019 Trustworthy Machine Learning, Autumn 2019  \n",
        "Antti Honkela and Razane Tajeddine  \n",
        "\n",
        "# Project 2: Real-life privacy-preserving machine learning\n",
        "\n",
        "Deadline for returning the solutions: 24 November 23:55.\n",
        "\n",
        "## General instructions (IMPORTANT!)\n",
        "\n",
        "1. This is an individual project. You can discuss the solutions with other students, but everyone needs to write their own code and answers.\n",
        "2. Please return your solutions as a notebook. When returning your solutions, please leave all output in the notebook.\n",
        "3. When returning your solutions, please make sure the notebook can be run cleanly using \"Cell\" / \"Run All\".\n",
        "4. Please make sure there are no dependencies between solutions to different problems.\n",
        "5. Please make sure that your notebook will not depend on any local files.\n",
        "6. Please make sure that the solutions for each problem in your notebook will produce the same results when run multiple times, i.e. remember to seed any random number generators you use (`numpy.random.seed()`!).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTYoUQUJqXvm",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Differentially private logistic regression with DP-SGD and synthetic data\n",
        "\n",
        "TensorFlow Privacy (https://github.com/tensorflow/privacy) library provides implementations of many differentially private optimisation algorithms for deep learning and other models. In order to perform these exercises, you will need to install TensorFlow Privacy and its dependencies according to instructions given on the website.\n",
        "\n",
        "In order to study TensorFlow Privacy, we will use logistic regression on a small synthetic data set. This will be faster to run than larger neural network models. A simple example implementation of the model is available at https://github.com/ahonkela/privacy/blob/master/tutorials/toy_lr_tutorial.py\n",
        "The code has been adapted from tutorials provided with TensorFlow privacy.\n",
        "\n",
        "The definition of the logistic regression model binary classification is itself very straightforward in TensorFlow, simply using a single fully connected linear layer with cross entropy loss:\n",
        "```{python}\n",
        "  # Define logistic regression model using tf.keras.layers.\n",
        "  logits = tf.keras.layers.Dense(2).apply(features['x'])\n",
        "\n",
        "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
        "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits)\n",
        "```\n",
        "\n",
        "The rest of the example provides supporting architecture. Key parameters of the algorithm are defined as `flags` at the beginning of the file. These include:\n",
        "```{python}\n",
        "flags.DEFINE_float('learning_rate', .05, 'Learning rate for training')\n",
        "flags.DEFINE_float('noise_multiplier', 2.0,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
        "flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
        "flags.DEFINE_integer('epochs', 2, 'Number of epochs')\n",
        "```\n",
        "\n",
        "`learning_rate` is the initial learning rate for the Adam optimiser. Larger value means faster learning but can cause instability.  \n",
        "`noise_multiplier` controls the amount of noise added in DP-SGD: higher value means more noise. The value is defined relative to the gradient clipping norm.  \n",
        "`l2_norm_clip` is the maximum norm at which per-example gradients are clipped. Smaller values mean less noise with the same level of privacy, but too small values can bias the results and make learning impossible.  \n",
        "`batch_size` is the minibatch size which impacts privacy via amplification from subsampling. Smaller batch sizes increase privacy for equal number of epochs, but too small batches can make the learning unstable.  \n",
        "`epochs` controls the length of training as a number of passes over the entire data.\n",
        "\n",
        "Test how these parameters (clipping threshold, batch size, noise multiplier and learning rate) affect the accuracy of the classifier and the privacy. Plot all your results to a privacy ($\\epsilon$) vs. accuracy plot to trace the optimal accuracy achievable under a specific level of privacy.\n",
        "\n",
        "You can limit the number of experiments to keep the runtimes reasonable: it is not necessary to try every combination of parameters but you can focus on testing the effect of one variable at a time. TensorFlow can use GPUs which can speed up learning significantly.\n",
        "\n",
        "Note: testing several hyperparameters and choosing the best has an impact on the privacy guarantees. There are methods for dealing with this (e.g. https://arxiv.org/abs/1811.07971) but the field is still under active development.\n",
        "\n",
        "**Considerations**\n",
        "\n",
        "To do this task I followed the instructions provided. To keep the runtimes reasonable I tested the effect of one hyperparameter at a time keeping the others to their default values (values provided in the code). I tried these values for the hyperparameters:\n",
        "1. Learning rate: 0.01, 0.05, 0.1, 0.5;\n",
        "2. Norm clipping: 0.5, 1.0, 1.5, 2.0;\n",
        "3. Batch size: 32, 64, 128, 256;\n",
        "4. Noise multiplier: 1.5, 2.0, 2.5, 3.0.\n",
        "\n",
        "These are the considerations about the plots I have provided in the code cell related to this task:\n",
        "\n",
        "**Learning rate**\n",
        "\n",
        "The best accuracy (0.835) has been obtained by the model trained with 0.5 learning rate after the first epoch of training. This accuracy corresponds to the best $\\epsilon$ achieved by the model too (0.47). The other models arrive at the same accuracy after many epochs causing the $\\epsilon$ to grow during the training, so we can say that the model trained with 0.5 learning rate has the best test accuracy/$\\epsilon$ ratio.\n",
        "\n",
        "**Norm clipping**\n",
        "\n",
        "The best accuracy (0.835) has been obtained by the model trained with 1.0 norm clipping after five epochs of training. This accuracy is related to a small value of $\\epsilon$ too (0.59). The other models arrive at the same accuracy after many epochs causing the $\\epsilon$ to grow during the training, so we can say that the model trained with 1.0 norm clipping has the best test accuracy/$\\epsilon$ ratio. Finally, the model with 2.0 norm clipping has a similar behaviour but I decided to keep the 1.0 norm clipping model as the best because it starts the training with an higher test accuracy.\n",
        "\n",
        "**Batch size**\n",
        "\n",
        "The best accuracy (0.840) has been obtained by the model trained with 32 batch size after ten epochs of training. This accuracy is related to a small value of $\\epsilon$ too (0.48). The other models arrive at the same accuracy with a similar number of epochs but with an higher value of $\\epsilon$, so we can say that the model trained with 32 batch size has the best test accuracy/$\\epsilon$ ratio. This is due to the fact that the other models start the training with an higher value of $\\epsilon$ compared to the best model.\n",
        "\n",
        "**Noise multiplier**\n",
        "\n",
        "The best accuracy (0.840) has been obtained by the model trained with 3.0 noise multiplier after eleven epochs of training. This accuracy is related to a small value of $\\epsilon$ too (0.40). The other models arrive at the same accuracy with a similar number of epochs but with an higher value of $\\epsilon$, so we can say that the model trained with 3.0 noise multiplier has the best test accuracy/$\\epsilon$ ratio. This is due to the fact that the other models start the training with an higher value of $\\epsilon$ compared to the best model.\n",
        "\n",
        "**Other experiment**\n",
        "\n",
        "After the experiment for the first task has been performed I tried to train a model with the best values of the hyperparameters found on the first task. These are the best values found for the hyperparameters:\n",
        "1. Learning rate: 0.5;\n",
        "2. Norm clipping: 1.0;\n",
        "3. Batch size: 32;\n",
        "4. Noise multiplier: 3.0.\n",
        "\n",
        "In the second code cell related to this task it is possible to observe the plot of this final experiment. It is possible to observe that the hyperparameters values found in the first task work well when they are put together, in fact after six epochs of training the model obtains its best accuracy (0.835) with a really small value of $\\epsilon$ (0.24). Finally, it is interesting to observe that this final model has obtained the smallest value of $\\epsilon$ seen in these experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmemiA8EqXvp",
        "colab_type": "code",
        "outputId": "eea49f4d-54cc-48fd-9d51-7eb9fd9ec216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# Modified to use logistic regression instead of CNN\n",
        "# and synthetic data instead of MNIST by Antti Honkela, 2019\n",
        "\n",
        "\"\"\"Training a logistic regression model with differentially private SGD optimizer.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from tensorflow_privacy.privacy.optimizers import dp_optimizer\n",
        "\n",
        "AdamOptimizer = tf.compat.v1.train.AdamOptimizer\n",
        "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
        "flags.DEFINE_float('learning_rate', .05, 'Learning rate for training')\n",
        "flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
        "flags.DEFINE_float('noise_multiplier', 2.0,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "\n",
        "def set_hyperparameters(norm_clip, lr, batch_size, noise_mult):\n",
        "  # clean flags\n",
        "  delattr(flags.FLAGS, 'l2_norm_clip')\n",
        "  delattr(flags.FLAGS, 'learning_rate')\n",
        "  delattr(flags.FLAGS, 'batch_size')\n",
        "  delattr(flags.FLAGS, 'noise_multiplier')\n",
        "  # redifine flags\n",
        "  print(\"Learning rate:\", lr)\n",
        "  print(\"Norm clipping:\", norm_clip)\n",
        "  print(\"Batch size:\", batch_size)\n",
        "  print(\"Noise multiplier\", noise_mult)\n",
        "  flags.DEFINE_float('l2_norm_clip', norm_clip, 'Clipping norm')\n",
        "  flags.DEFINE_float('learning_rate', lr, 'Learning rate for training')\n",
        "  flags.DEFINE_integer('batch_size', batch_size, 'Batch size')\n",
        "  flags.DEFINE_float('noise_multiplier', noise_mult,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
        "    'train with vanilla SGD.')\n",
        "flags.DEFINE_integer('epochs', 2, 'Number of epochs')\n",
        "flags.DEFINE_integer('training_data_size', 2000, 'Training data size')\n",
        "flags.DEFINE_integer('test_data_size', 2000, 'Test data size')\n",
        "flags.DEFINE_integer('input_dimension', 5, 'Input dimension')\n",
        "flags.DEFINE_string('model_dir', None, 'Model directory')\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "def task(norm_clip=1.0, lr=.05, batch_size=64, noise_mult=2.0):\n",
        "  test_acc = []\n",
        "  set_hyperparameters(norm_clip, lr, batch_size, noise_mult)\n",
        "  # Instantiate the tf.Estimator.\n",
        "  lr_classifier = tf.estimator.Estimator(model_fn=lr_model_fn,\n",
        "                                        model_dir=FLAGS.model_dir)\n",
        "\n",
        "  # Create tf.Estimator input functions for the training and test data.\n",
        "  train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': train_data},\n",
        "      y=train_labels,\n",
        "      batch_size=FLAGS.batch_size,\n",
        "      num_epochs=FLAGS.epochs,\n",
        "      shuffle=True)\n",
        "  eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': test_data},\n",
        "      y=test_labels,\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "\n",
        "  # Training loop.\n",
        "  steps_per_epoch = FLAGS.training_data_size // FLAGS.batch_size / 10\n",
        "  for epoch in range(1, 10*FLAGS.epochs + 1):\n",
        "    # Train the model for one epoch.\n",
        "    lr_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
        "\n",
        "    # Evaluate the model and print results\n",
        "    eval_results = lr_classifier.evaluate(input_fn=eval_input_fn)\n",
        "    test_accuracy = eval_results['accuracy']\n",
        "    test_acc.append(test_accuracy)\n",
        "    print('Test accuracy after %.1f epochs is: %.3f' % (epoch/10, test_accuracy))\n",
        "  return test_acc\n",
        "  \n",
        "\n",
        "class EpsilonPrintingTrainingHook(tf.estimator.SessionRunHook):\n",
        "  \"\"\"Training hook to print current value of epsilon after an epoch.\"\"\"\n",
        "\n",
        "  def __init__(self, ledger):\n",
        "    \"\"\"Initalizes the EpsilonPrintingTrainingHook.\n",
        "    Args:\n",
        "      ledger: The privacy ledger.\n",
        "    \"\"\"\n",
        "    self._samples, self._queries = ledger.get_unformatted_ledger()\n",
        "\n",
        "  def end(self, session):\n",
        "    global eps_arr\n",
        "    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
        "    samples = session.run(self._samples)\n",
        "    queries = session.run(self._queries)\n",
        "    formatted_ledger = privacy_ledger.format_ledger(samples, queries)\n",
        "    rdp = compute_rdp_from_ledger(formatted_ledger, orders)\n",
        "    eps = get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "    eps_arr.append(eps)\n",
        "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
        "\n",
        "\n",
        "def lr_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for a LR.\"\"\"\n",
        "\n",
        "  # Define logistic regression model using tf.keras.layers.\n",
        "  logits = tf.keras.layers.Dense(2).apply(features['x'])\n",
        "\n",
        "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
        "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits)\n",
        "  # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
        "  scalar_loss = tf.reduce_mean(input_tensor=vector_loss)\n",
        "\n",
        "  # Configure the training op (for TRAIN mode).\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "    if FLAGS.dpsgd:\n",
        "      ledger = privacy_ledger.PrivacyLedger(\n",
        "          population_size=FLAGS.training_data_size,\n",
        "          selection_probability=(FLAGS.batch_size / FLAGS.training_data_size))\n",
        "\n",
        "      # Use DP version of AdamOptimizer. Other optimizers are\n",
        "      # available in dp_optimizer. Most optimizers inheriting from\n",
        "      # tf.train.Optimizer should be wrappable in differentially private\n",
        "      # counterparts by calling dp_optimizer.optimizer_from_args().\n",
        "      # Setting num_microbatches to None is necessary for DP and\n",
        "      # per-example gradients\n",
        "      optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
        "          l2_norm_clip=FLAGS.l2_norm_clip,\n",
        "          noise_multiplier=FLAGS.noise_multiplier,\n",
        "          num_microbatches=None,\n",
        "          ledger=ledger,\n",
        "          learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = [\n",
        "          EpsilonPrintingTrainingHook(ledger)\n",
        "      ]\n",
        "      opt_loss = vector_loss\n",
        "    else:\n",
        "      optimizer = AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = []\n",
        "      opt_loss = scalar_loss\n",
        "    global_step = tf.compat.v1.train.get_global_step()\n",
        "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
        "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
        "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
        "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
        "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      train_op=train_op,\n",
        "                                      training_hooks=training_hooks)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode).\n",
        "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "    eval_metric_ops = {\n",
        "        'accuracy':\n",
        "            tf.compat.v1.metrics.accuracy(\n",
        "                labels=labels,\n",
        "                predictions=tf.argmax(input=logits, axis=1))\n",
        "    }\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "def generate_data():\n",
        "  npr.seed(4242)\n",
        "  N_train = FLAGS.training_data_size\n",
        "  N_test = FLAGS.test_data_size\n",
        "  N = N_train + N_test\n",
        "  X0 = npr.randn(N, FLAGS.input_dimension)\n",
        "  temp = X0 @ npr.randn(FLAGS.input_dimension, 1) + npr.randn(N, 1)\n",
        "  Y0 = np.round(1/(1+np.exp(-temp)))\n",
        "\n",
        "  train_X = X0[0:N_train, :]\n",
        "  test_X = X0[N_train:N, :]\n",
        "  train_Y = Y0[0:N_train, 0]\n",
        "  test_Y = Y0[N_train:N, 0]\n",
        "  train_X = np.array(train_X, dtype=np.float32)\n",
        "  test_X = np.array(test_X, dtype=np.float32)\n",
        "  train_Y = np.array(train_Y, dtype=np.int32)\n",
        "  test_Y = np.array(test_Y, dtype=np.int32)\n",
        "  return train_X, train_Y, test_X, test_Y\n",
        "  \n",
        "\n",
        "def main(unused_argv):\n",
        "  global eps_arr\n",
        "  eps_arr = []\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "  # Load training and test data.\n",
        "  global train_data\n",
        "  global train_labels\n",
        "  global test_data\n",
        "  global test_labels\n",
        "  train_data, train_labels, test_data, test_labels = generate_data()\n",
        "\n",
        "  # find best learning rate keeping other parametes untouched\n",
        "  fig = plt.figure()\n",
        "  plt.subplot(2, 2, 1)\n",
        "  plt.title(\"Learning rate\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.ylabel(\"Test accuracy\")\n",
        "  test_acc = task(lr=.01)\n",
        "  plt.plot(eps_arr, test_acc, color='b', label=\"0.01\")\n",
        "  eps_arr = []\n",
        "  test_acc = task(lr=.05)\n",
        "  plt.plot(eps_arr, test_acc, color='r', label=\"0.05\")\n",
        "  eps_arr = []\n",
        "  test_acc = task(lr=.1)\n",
        "  plt.plot(eps_arr, test_acc, color='g', label=\"0.1\")\n",
        "  eps_arr = []\n",
        "  test_acc = task(lr=.5)\n",
        "  plt.plot(eps_arr, test_acc, color='y', label=\"0.5\")\n",
        "  plt.legend()\n",
        "  \n",
        "    \n",
        "  # find best norm clipping keeping other parameters untouched\n",
        "  c = ['b', 'r', 'g', 'y']\n",
        "  plt.subplot(2,2,2)\n",
        "  plt.title(\"Norm clipping\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.ylabel(\"Test accuracy\")\n",
        "  i = 0\n",
        "  for norm_clipping in np.arange(0.5, 2.1, 0.5):\n",
        "    eps_arr = []\n",
        "    test_acc = task(norm_clip=norm_clipping)\n",
        "    plt.plot(eps_arr, test_acc, color=c[i], label=str(norm_clipping))\n",
        "    i += 1\n",
        "  plt.legend()\n",
        "\n",
        "  # find best batch size keeping other parameters untouched\n",
        "  i = 0\n",
        "  plt.subplot(2,2,3)\n",
        "  plt.title(\"Batch size\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.ylabel(\"Test accuracy\")\n",
        "  for batch_s in range(0, 4):\n",
        "    eps_arr = []\n",
        "    test_acc = task(batch_size=int(32*math.pow(2,batch_s)))\n",
        "    plt.plot(eps_arr, test_acc, color=c[i], label=str(int(32*math.pow(2,batch_s))))\n",
        "    i += 1\n",
        "  plt.legend()\n",
        "\n",
        "  # find best noise multiplier keeping the other parameters untouched\n",
        "  i = 0\n",
        "  plt.subplot(2,2,4)\n",
        "  plt.title(\"Noise multiplier\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.ylabel(\"Test accuracy\")\n",
        "  for noise_m in np.arange(1.5, 3.1, 0.5):\n",
        "    eps_arr = []\n",
        "    test_acc = task(noise_mult=noise_m)\n",
        "    plt.plot(eps_arr, test_acc, color=c[i], label=str(noise_m))\n",
        "    i += 1\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  app.run(main)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Learning rate: 0.01\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.563\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.584\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.605\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.618\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.640\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.659\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.682\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.697\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.706\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.723\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.734\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.751\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.765\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.773\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.783\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.793\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.797\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.804\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.812\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.821\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.492\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.592\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.675\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.749\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.793\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.821\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.839\n",
            "Learning rate: 0.1\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.449\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.685\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.781\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.818\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.827\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.826\n",
            "Learning rate: 0.5\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.830\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.821\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.814\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.812\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.813\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.811\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.822\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.810\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.802\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.791\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 0.5\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.504\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.692\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.754\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.792\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.818\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.840\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.742\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.784\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.815\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.830\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.838\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.5\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.507\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.593\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.664\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.720\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.750\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.772\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.790\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.808\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.817\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.825\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.827\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.840\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 2.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.669\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.782\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.825\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.840\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 32\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.37\n",
            "Test accuracy after 0.1 epochs is: 0.601\n",
            "For delta=1e-5, the current epsilon is: 0.39\n",
            "Test accuracy after 0.2 epochs is: 0.712\n",
            "For delta=1e-5, the current epsilon is: 0.40\n",
            "Test accuracy after 0.3 epochs is: 0.771\n",
            "For delta=1e-5, the current epsilon is: 0.41\n",
            "Test accuracy after 0.4 epochs is: 0.805\n",
            "For delta=1e-5, the current epsilon is: 0.42\n",
            "Test accuracy after 0.5 epochs is: 0.824\n",
            "For delta=1e-5, the current epsilon is: 0.43\n",
            "Test accuracy after 0.6 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.44\n",
            "Test accuracy after 0.7 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.45\n",
            "Test accuracy after 0.8 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.46\n",
            "Test accuracy after 0.9 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 1.0 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.48\n",
            "Test accuracy after 1.1 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 1.2 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.50\n",
            "Test accuracy after 1.3 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.51\n",
            "Test accuracy after 1.4 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 1.5 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.53\n",
            "Test accuracy after 1.6 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 1.7 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.55\n",
            "Test accuracy after 1.8 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 1.9 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.57\n",
            "Test accuracy after 2.0 epochs is: 0.834\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.756\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.807\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.822\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.837\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 128\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.1 epochs is: 0.424\n",
            "For delta=1e-5, the current epsilon is: 0.68\n",
            "Test accuracy after 0.2 epochs is: 0.472\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 0.3 epochs is: 0.522\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 0.4 epochs is: 0.578\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 0.5 epochs is: 0.646\n",
            "For delta=1e-5, the current epsilon is: 0.86\n",
            "Test accuracy after 0.6 epochs is: 0.695\n",
            "For delta=1e-5, the current epsilon is: 0.89\n",
            "Test accuracy after 0.7 epochs is: 0.726\n",
            "For delta=1e-5, the current epsilon is: 0.93\n",
            "Test accuracy after 0.8 epochs is: 0.758\n",
            "For delta=1e-5, the current epsilon is: 0.97\n",
            "Test accuracy after 0.9 epochs is: 0.784\n",
            "For delta=1e-5, the current epsilon is: 1.00\n",
            "Test accuracy after 1.0 epochs is: 0.798\n",
            "For delta=1e-5, the current epsilon is: 1.03\n",
            "Test accuracy after 1.1 epochs is: 0.812\n",
            "For delta=1e-5, the current epsilon is: 1.07\n",
            "Test accuracy after 1.2 epochs is: 0.824\n",
            "For delta=1e-5, the current epsilon is: 1.10\n",
            "Test accuracy after 1.3 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 1.13\n",
            "Test accuracy after 1.4 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 1.16\n",
            "Test accuracy after 1.5 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 1.19\n",
            "Test accuracy after 1.6 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 1.22\n",
            "Test accuracy after 1.7 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 1.24\n",
            "Test accuracy after 1.8 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 1.27\n",
            "Test accuracy after 1.9 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 1.30\n",
            "Test accuracy after 2.0 epochs is: 0.839\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 256\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 0.1 epochs is: 0.585\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.2 epochs is: 0.603\n",
            "For delta=1e-5, the current epsilon is: 1.07\n",
            "Test accuracy after 0.3 epochs is: 0.620\n",
            "For delta=1e-5, the current epsilon is: 1.15\n",
            "Test accuracy after 0.4 epochs is: 0.639\n",
            "For delta=1e-5, the current epsilon is: 1.21\n",
            "Test accuracy after 0.5 epochs is: 0.654\n",
            "For delta=1e-5, the current epsilon is: 1.28\n",
            "Test accuracy after 0.6 epochs is: 0.669\n",
            "For delta=1e-5, the current epsilon is: 1.34\n",
            "Test accuracy after 0.7 epochs is: 0.680\n",
            "For delta=1e-5, the current epsilon is: 1.40\n",
            "Test accuracy after 0.8 epochs is: 0.689\n",
            "For delta=1e-5, the current epsilon is: 1.45\n",
            "Test accuracy after 0.9 epochs is: 0.698\n",
            "For delta=1e-5, the current epsilon is: 1.50\n",
            "Test accuracy after 1.0 epochs is: 0.709\n",
            "For delta=1e-5, the current epsilon is: 1.55\n",
            "Test accuracy after 1.1 epochs is: 0.721\n",
            "For delta=1e-5, the current epsilon is: 1.60\n",
            "Test accuracy after 1.2 epochs is: 0.734\n",
            "For delta=1e-5, the current epsilon is: 1.64\n",
            "Test accuracy after 1.3 epochs is: 0.749\n",
            "For delta=1e-5, the current epsilon is: 1.69\n",
            "Test accuracy after 1.4 epochs is: 0.760\n",
            "For delta=1e-5, the current epsilon is: 1.73\n",
            "Test accuracy after 1.5 epochs is: 0.771\n",
            "For delta=1e-5, the current epsilon is: 1.78\n",
            "Test accuracy after 1.6 epochs is: 0.775\n",
            "For delta=1e-5, the current epsilon is: 1.81\n",
            "Test accuracy after 1.7 epochs is: 0.785\n",
            "For delta=1e-5, the current epsilon is: 1.85\n",
            "Test accuracy after 1.8 epochs is: 0.795\n",
            "For delta=1e-5, the current epsilon is: 1.89\n",
            "Test accuracy after 1.9 epochs is: 0.803\n",
            "For delta=1e-5, the current epsilon is: 1.93\n",
            "Test accuracy after 2.0 epochs is: 0.809\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 1.5\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 0.1 epochs is: 0.544\n",
            "For delta=1e-5, the current epsilon is: 0.89\n",
            "Test accuracy after 0.2 epochs is: 0.678\n",
            "For delta=1e-5, the current epsilon is: 0.92\n",
            "Test accuracy after 0.3 epochs is: 0.734\n",
            "For delta=1e-5, the current epsilon is: 0.95\n",
            "Test accuracy after 0.4 epochs is: 0.789\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.5 epochs is: 0.809\n",
            "For delta=1e-5, the current epsilon is: 1.01\n",
            "Test accuracy after 0.6 epochs is: 0.820\n",
            "For delta=1e-5, the current epsilon is: 1.05\n",
            "Test accuracy after 0.7 epochs is: 0.827\n",
            "For delta=1e-5, the current epsilon is: 1.07\n",
            "Test accuracy after 0.8 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 1.10\n",
            "Test accuracy after 0.9 epochs is: 0.846\n",
            "For delta=1e-5, the current epsilon is: 1.12\n",
            "Test accuracy after 1.0 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 1.15\n",
            "Test accuracy after 1.1 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 1.17\n",
            "Test accuracy after 1.2 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 1.19\n",
            "Test accuracy after 1.3 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 1.22\n",
            "Test accuracy after 1.4 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 1.24\n",
            "Test accuracy after 1.5 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 1.26\n",
            "Test accuracy after 1.6 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 1.29\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 1.31\n",
            "Test accuracy after 1.8 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 1.33\n",
            "Test accuracy after 1.9 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 1.36\n",
            "Test accuracy after 2.0 epochs is: 0.839\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.0\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 0.1 epochs is: 0.567\n",
            "For delta=1e-5, the current epsilon is: 0.49\n",
            "Test accuracy after 0.2 epochs is: 0.658\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 0.3 epochs is: 0.728\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 0.4 epochs is: 0.771\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 0.5 epochs is: 0.796\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 0.6 epochs is: 0.809\n",
            "For delta=1e-5, the current epsilon is: 0.61\n",
            "Test accuracy after 0.7 epochs is: 0.817\n",
            "For delta=1e-5, the current epsilon is: 0.63\n",
            "Test accuracy after 0.8 epochs is: 0.826\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 0.9 epochs is: 0.824\n",
            "For delta=1e-5, the current epsilon is: 0.67\n",
            "Test accuracy after 1.0 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.69\n",
            "Test accuracy after 1.1 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.71\n",
            "Test accuracy after 1.2 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.73\n",
            "Test accuracy after 1.3 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.75\n",
            "Test accuracy after 1.4 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.77\n",
            "Test accuracy after 1.5 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.79\n",
            "Test accuracy after 1.6 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.81\n",
            "Test accuracy after 1.7 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.83\n",
            "Test accuracy after 1.8 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.85\n",
            "Test accuracy after 1.9 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.87\n",
            "Test accuracy after 2.0 epochs is: 0.839\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 2.5\n",
            "For delta=1e-5, the current epsilon is: 0.30\n",
            "Test accuracy after 0.1 epochs is: 0.766\n",
            "For delta=1e-5, the current epsilon is: 0.32\n",
            "Test accuracy after 0.2 epochs is: 0.806\n",
            "For delta=1e-5, the current epsilon is: 0.34\n",
            "Test accuracy after 0.3 epochs is: 0.825\n",
            "For delta=1e-5, the current epsilon is: 0.36\n",
            "Test accuracy after 0.4 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.38\n",
            "Test accuracy after 0.5 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.40\n",
            "Test accuracy after 0.6 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.42\n",
            "Test accuracy after 0.7 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.44\n",
            "Test accuracy after 0.8 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.46\n",
            "Test accuracy after 0.9 epochs is: 0.838\n",
            "For delta=1e-5, the current epsilon is: 0.48\n",
            "Test accuracy after 1.0 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.50\n",
            "Test accuracy after 1.1 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 1.2 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.54\n",
            "Test accuracy after 1.3 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.56\n",
            "Test accuracy after 1.4 epochs is: 0.845\n",
            "For delta=1e-5, the current epsilon is: 0.57\n",
            "Test accuracy after 1.5 epochs is: 0.842\n",
            "For delta=1e-5, the current epsilon is: 0.59\n",
            "Test accuracy after 1.6 epochs is: 0.841\n",
            "For delta=1e-5, the current epsilon is: 0.60\n",
            "Test accuracy after 1.7 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.62\n",
            "Test accuracy after 1.8 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.64\n",
            "Test accuracy after 1.9 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.65\n",
            "Test accuracy after 2.0 epochs is: 0.839\n",
            "Learning rate: 0.05\n",
            "Norm clipping: 1.0\n",
            "Batch size: 64\n",
            "Noise multiplier 3.0\n",
            "For delta=1e-5, the current epsilon is: 0.21\n",
            "Test accuracy after 0.1 epochs is: 0.518\n",
            "For delta=1e-5, the current epsilon is: 0.23\n",
            "Test accuracy after 0.2 epochs is: 0.665\n",
            "For delta=1e-5, the current epsilon is: 0.25\n",
            "Test accuracy after 0.3 epochs is: 0.753\n",
            "For delta=1e-5, the current epsilon is: 0.27\n",
            "Test accuracy after 0.4 epochs is: 0.791\n",
            "For delta=1e-5, the current epsilon is: 0.29\n",
            "Test accuracy after 0.5 epochs is: 0.806\n",
            "For delta=1e-5, the current epsilon is: 0.31\n",
            "Test accuracy after 0.6 epochs is: 0.816\n",
            "For delta=1e-5, the current epsilon is: 0.33\n",
            "Test accuracy after 0.7 epochs is: 0.827\n",
            "For delta=1e-5, the current epsilon is: 0.35\n",
            "Test accuracy after 0.8 epochs is: 0.831\n",
            "For delta=1e-5, the current epsilon is: 0.37\n",
            "Test accuracy after 0.9 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.38\n",
            "Test accuracy after 1.0 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.40\n",
            "Test accuracy after 1.1 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.41\n",
            "Test accuracy after 1.2 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.43\n",
            "Test accuracy after 1.3 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.44\n",
            "Test accuracy after 1.4 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.46\n",
            "Test accuracy after 1.5 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.47\n",
            "Test accuracy after 1.6 epochs is: 0.837\n",
            "For delta=1e-5, the current epsilon is: 0.48\n",
            "Test accuracy after 1.7 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.50\n",
            "Test accuracy after 1.8 epochs is: 0.836\n",
            "For delta=1e-5, the current epsilon is: 0.51\n",
            "Test accuracy after 1.9 epochs is: 0.834\n",
            "For delta=1e-5, the current epsilon is: 0.52\n",
            "Test accuracy after 2.0 epochs is: 0.834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLPwSy0BZWFhYWPgl/w90SHS+Mm761QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNrOF_QwEixE",
        "colab_type": "code",
        "outputId": "4a71768c-0051-4846-9616-43227ff007fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training a model with the best values found for the hyperparameters\n",
        "%tensorflow_version 2.x\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# Modified to use logistic regression instead of CNN\n",
        "# and synthetic data instead of MNIST by Antti Honkela, 2019\n",
        "\n",
        "\"\"\"Training a logistic regression model with differentially private SGD optimizer.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from tensorflow_privacy.privacy.optimizers import dp_optimizer\n",
        "\n",
        "AdamOptimizer = tf.compat.v1.train.AdamOptimizer\n",
        "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
        "flags.DEFINE_float('learning_rate', .05, 'Learning rate for training')\n",
        "flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
        "flags.DEFINE_float('noise_multiplier', 2.0,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "\n",
        "def set_hyperparameters(norm_clip, lr, batch_size, noise_mult):\n",
        "  # clean flags\n",
        "  delattr(flags.FLAGS, 'l2_norm_clip')\n",
        "  delattr(flags.FLAGS, 'learning_rate')\n",
        "  delattr(flags.FLAGS, 'batch_size')\n",
        "  delattr(flags.FLAGS, 'noise_multiplier')\n",
        "  # redifine flags\n",
        "  print(\"Learning rate:\", lr)\n",
        "  print(\"Norm clipping:\", norm_clip)\n",
        "  print(\"Batch size:\", batch_size)\n",
        "  print(\"Noise multiplier\", noise_mult)\n",
        "  flags.DEFINE_float('l2_norm_clip', norm_clip, 'Clipping norm')\n",
        "  flags.DEFINE_float('learning_rate', lr, 'Learning rate for training')\n",
        "  flags.DEFINE_integer('batch_size', batch_size, 'Batch size')\n",
        "  flags.DEFINE_float('noise_multiplier', noise_mult,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
        "    'train with vanilla SGD.')\n",
        "flags.DEFINE_integer('epochs', 2, 'Number of epochs')\n",
        "flags.DEFINE_integer('training_data_size', 2000, 'Training data size')\n",
        "flags.DEFINE_integer('test_data_size', 2000, 'Test data size')\n",
        "flags.DEFINE_integer('input_dimension', 5, 'Input dimension')\n",
        "flags.DEFINE_string('model_dir', None, 'Model directory')\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "def task(norm_clip=1.0, lr=.05, batch_size=64, noise_mult=2.0):\n",
        "  test_acc = []\n",
        "  set_hyperparameters(norm_clip, lr, batch_size, noise_mult)\n",
        "  # Instantiate the tf.Estimator.\n",
        "  lr_classifier = tf.estimator.Estimator(model_fn=lr_model_fn,\n",
        "                                        model_dir=FLAGS.model_dir)\n",
        "\n",
        "  # Create tf.Estimator input functions for the training and test data.\n",
        "  train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': train_data},\n",
        "      y=train_labels,\n",
        "      batch_size=FLAGS.batch_size,\n",
        "      num_epochs=FLAGS.epochs,\n",
        "      shuffle=True)\n",
        "  eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': test_data},\n",
        "      y=test_labels,\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "\n",
        "  # Training loop.\n",
        "  steps_per_epoch = FLAGS.training_data_size // FLAGS.batch_size / 10\n",
        "  for epoch in range(1, 10*FLAGS.epochs + 1):\n",
        "    # Train the model for one epoch.\n",
        "    lr_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
        "\n",
        "    # Evaluate the model and print results\n",
        "    eval_results = lr_classifier.evaluate(input_fn=eval_input_fn)\n",
        "    test_accuracy = eval_results['accuracy']\n",
        "    test_acc.append(test_accuracy)\n",
        "    print('Test accuracy after %.1f epochs is: %.3f' % (epoch/10, test_accuracy))\n",
        "  return test_acc\n",
        "  \n",
        "\n",
        "class EpsilonPrintingTrainingHook(tf.estimator.SessionRunHook):\n",
        "  \"\"\"Training hook to print current value of epsilon after an epoch.\"\"\"\n",
        "\n",
        "  def __init__(self, ledger):\n",
        "    \"\"\"Initalizes the EpsilonPrintingTrainingHook.\n",
        "    Args:\n",
        "      ledger: The privacy ledger.\n",
        "    \"\"\"\n",
        "    self._samples, self._queries = ledger.get_unformatted_ledger()\n",
        "\n",
        "  def end(self, session):\n",
        "    global eps_arr\n",
        "    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
        "    samples = session.run(self._samples)\n",
        "    queries = session.run(self._queries)\n",
        "    formatted_ledger = privacy_ledger.format_ledger(samples, queries)\n",
        "    rdp = compute_rdp_from_ledger(formatted_ledger, orders)\n",
        "    eps = get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "    eps_arr.append(eps)\n",
        "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
        "\n",
        "\n",
        "def lr_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for a LR.\"\"\"\n",
        "\n",
        "  # Define logistic regression model using tf.keras.layers.\n",
        "  logits = tf.keras.layers.Dense(2).apply(features['x'])\n",
        "\n",
        "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
        "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits)\n",
        "  # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
        "  scalar_loss = tf.reduce_mean(input_tensor=vector_loss)\n",
        "\n",
        "  # Configure the training op (for TRAIN mode).\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "    if FLAGS.dpsgd:\n",
        "      ledger = privacy_ledger.PrivacyLedger(\n",
        "          population_size=FLAGS.training_data_size,\n",
        "          selection_probability=(FLAGS.batch_size / FLAGS.training_data_size))\n",
        "\n",
        "      # Use DP version of AdamOptimizer. Other optimizers are\n",
        "      # available in dp_optimizer. Most optimizers inheriting from\n",
        "      # tf.train.Optimizer should be wrappable in differentially private\n",
        "      # counterparts by calling dp_optimizer.optimizer_from_args().\n",
        "      # Setting num_microbatches to None is necessary for DP and\n",
        "      # per-example gradients\n",
        "      optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
        "          l2_norm_clip=FLAGS.l2_norm_clip,\n",
        "          noise_multiplier=FLAGS.noise_multiplier,\n",
        "          num_microbatches=None,\n",
        "          ledger=ledger,\n",
        "          learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = [\n",
        "          EpsilonPrintingTrainingHook(ledger)\n",
        "      ]\n",
        "      opt_loss = vector_loss\n",
        "    else:\n",
        "      optimizer = AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = []\n",
        "      opt_loss = scalar_loss\n",
        "    global_step = tf.compat.v1.train.get_global_step()\n",
        "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
        "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
        "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
        "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
        "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      train_op=train_op,\n",
        "                                      training_hooks=training_hooks)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode).\n",
        "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "    eval_metric_ops = {\n",
        "        'accuracy':\n",
        "            tf.compat.v1.metrics.accuracy(\n",
        "                labels=labels,\n",
        "                predictions=tf.argmax(input=logits, axis=1))\n",
        "    }\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "def generate_data():\n",
        "  npr.seed(4242)\n",
        "  N_train = FLAGS.training_data_size\n",
        "  N_test = FLAGS.test_data_size\n",
        "  N = N_train + N_test\n",
        "  X0 = npr.randn(N, FLAGS.input_dimension)\n",
        "  temp = X0 @ npr.randn(FLAGS.input_dimension, 1) + npr.randn(N, 1)\n",
        "  Y0 = np.round(1/(1+np.exp(-temp)))\n",
        "\n",
        "  train_X = X0[0:N_train, :]\n",
        "  test_X = X0[N_train:N, :]\n",
        "  train_Y = Y0[0:N_train, 0]\n",
        "  test_Y = Y0[N_train:N, 0]\n",
        "  train_X = np.array(train_X, dtype=np.float32)\n",
        "  test_X = np.array(test_X, dtype=np.float32)\n",
        "  train_Y = np.array(train_Y, dtype=np.int32)\n",
        "  test_Y = np.array(test_Y, dtype=np.int32)\n",
        "  return train_X, train_Y, test_X, test_Y\n",
        "  \n",
        "\n",
        "def main(unused_argv):\n",
        "  global eps_arr\n",
        "  eps_arr = []\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "  # Load training and test data.\n",
        "  global train_data\n",
        "  global train_labels\n",
        "  global test_data\n",
        "  global test_labels\n",
        "  train_data, train_labels, test_data, test_labels = generate_data()\n",
        "\n",
        "  # model with best hyperparameters discovered\n",
        "  fig = plt.figure()\n",
        "  plt.title(\"Best model\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.ylabel(\"Test accuracy\")\n",
        "  test_acc = task(norm_clip=1.0, lr=.5, batch_size=32, noise_mult=3.0)\n",
        "  plt.plot(eps_arr, test_acc)\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  app.run(main)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Learning rate: 0.5\n",
            "Norm clipping: 1.0\n",
            "Batch size: 32\n",
            "Noise multiplier 3.0\n",
            "For delta=1e-5, the current epsilon is: 0.19\n",
            "Test accuracy after 0.1 epochs is: 0.682\n",
            "For delta=1e-5, the current epsilon is: 0.20\n",
            "Test accuracy after 0.2 epochs is: 0.779\n",
            "For delta=1e-5, the current epsilon is: 0.21\n",
            "Test accuracy after 0.3 epochs is: 0.796\n",
            "For delta=1e-5, the current epsilon is: 0.22\n",
            "Test accuracy after 0.4 epochs is: 0.795\n",
            "For delta=1e-5, the current epsilon is: 0.22\n",
            "Test accuracy after 0.5 epochs is: 0.815\n",
            "For delta=1e-5, the current epsilon is: 0.23\n",
            "Test accuracy after 0.6 epochs is: 0.835\n",
            "For delta=1e-5, the current epsilon is: 0.24\n",
            "Test accuracy after 0.7 epochs is: 0.828\n",
            "For delta=1e-5, the current epsilon is: 0.25\n",
            "Test accuracy after 0.8 epochs is: 0.789\n",
            "For delta=1e-5, the current epsilon is: 0.25\n",
            "Test accuracy after 0.9 epochs is: 0.783\n",
            "For delta=1e-5, the current epsilon is: 0.26\n",
            "Test accuracy after 1.0 epochs is: 0.782\n",
            "For delta=1e-5, the current epsilon is: 0.27\n",
            "Test accuracy after 1.1 epochs is: 0.814\n",
            "For delta=1e-5, the current epsilon is: 0.28\n",
            "Test accuracy after 1.2 epochs is: 0.823\n",
            "For delta=1e-5, the current epsilon is: 0.28\n",
            "Test accuracy after 1.3 epochs is: 0.833\n",
            "For delta=1e-5, the current epsilon is: 0.29\n",
            "Test accuracy after 1.4 epochs is: 0.822\n",
            "For delta=1e-5, the current epsilon is: 0.30\n",
            "Test accuracy after 1.5 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.31\n",
            "Test accuracy after 1.6 epochs is: 0.840\n",
            "For delta=1e-5, the current epsilon is: 0.31\n",
            "Test accuracy after 1.7 epochs is: 0.839\n",
            "For delta=1e-5, the current epsilon is: 0.32\n",
            "Test accuracy after 1.8 epochs is: 0.832\n",
            "For delta=1e-5, the current epsilon is: 0.33\n",
            "Test accuracy after 1.9 epochs is: 0.823\n",
            "For delta=1e-5, the current epsilon is: 0.34\n",
            "Test accuracy after 2.0 epochs is: 0.806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-09MlXwqXvy",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: DP logistic regression on realistic data\n",
        "\n",
        "Using the above code as a basis, build a DP logistic regression classifier for the UCI Adult data set (https://archive.ics.uci.edu/ml/datasets/Adult). (The data set is a standard benchmark data set that is available in various packages - feel free to use one of those.)\n",
        "\n",
        "How accurate classifier can you build to predict if an individual has an income of at most 50k, using DP with $\\epsilon=1, \\delta = 10^{-5}$? Report your accuracy on the separate test set not used in learning.\n",
        "\n",
        "Hint: the data set includes many categorical variables. In order to use these, you will need to use a one-hot encoding with $n-1$ variables used to denote $n$ values so that $k$th value is represented by value 1 in $k-1$st variable and zeros otherwise.\n",
        "\n",
        "**Considerations**\n",
        "\n",
        "Tensorflow Privacy doesn't allow to set both $\\epsilon$ and $\\delta$ for the training of the model. It allows only to compute the value of $\\epsilon$ in function of $\\delta$ or the value of $\\delta$ in function of $\\epsilon$. So, to do this task I've set $\\delta$ to $10e^{-5}$ and then I tuned the hyperparameters of the model to find a combination that was able to reach an $\\epsilon$ equal to 1 and then I stopped the training and I returned the requested test accuracy. The test accuracy obtained with $\\delta=10e^{-5}$ and $\\epsilon=1$ is 0.77224433 and it has been obtained after 16 epochs of training of the model trained with the following values of the hyperparameters:\n",
        "1. Learning rate: 0.05;\n",
        "2. Norm clipping: 0.1;\n",
        "3. Batch size: 64;\n",
        "4. Noise multiplier: 1.05."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVENSbOdqXv0",
        "colab_type": "code",
        "outputId": "a7e79f2a-419e-4161-9513-56d6d31d65b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# Copyright 2018, The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# Modified to use logistic regression instead of CNN\n",
        "# and synthetic data instead of MNIST by Antti Honkela, 2019\n",
        "\n",
        "\"\"\"Training a logistic regression model with differentially private SGD optimizer.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.random as npr\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp_from_ledger\n",
        "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "from tensorflow_privacy.privacy.optimizers import dp_optimizer\n",
        "\n",
        "global epsilon\n",
        "\n",
        "AdamOptimizer = tf.compat.v1.train.AdamOptimizer\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_boolean(\n",
        "    'dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
        "    'train with vanilla SGD.')\n",
        "flags.DEFINE_float('learning_rate', .05, 'Learning rate for training')\n",
        "flags.DEFINE_float('noise_multiplier', 1.05,\n",
        "                   'Ratio of the standard deviation to the clipping norm')\n",
        "flags.DEFINE_float('l2_norm_clip', 0.1, 'Clipping norm')\n",
        "flags.DEFINE_integer('batch_size', 64, 'Batch size')\n",
        "flags.DEFINE_integer('epochs', 2, 'Number of epochs')\n",
        "flags.DEFINE_integer('training_data_size', 30162, 'Training data size')\n",
        "flags.DEFINE_integer('test_data_size', 15060, 'Test data size')\n",
        "flags.DEFINE_integer('input_dimension', 104, 'Input dimension')\n",
        "flags.DEFINE_string('model_dir', None, 'Model directory')\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "\n",
        "class EpsilonPrintingTrainingHook(tf.estimator.SessionRunHook):\n",
        "  \"\"\"Training hook to print current value of epsilon after an epoch.\"\"\"\n",
        "\n",
        "  def __init__(self, ledger):\n",
        "    \"\"\"Initalizes the EpsilonPrintingTrainingHook.\n",
        "    Args:\n",
        "      ledger: The privacy ledger.\n",
        "    \"\"\"\n",
        "    self._samples, self._queries = ledger.get_unformatted_ledger()\n",
        "\n",
        "  def end(self, session):\n",
        "    global epsilon\n",
        "    orders = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
        "    samples = session.run(self._samples)\n",
        "    queries = session.run(self._queries)\n",
        "    formatted_ledger = privacy_ledger.format_ledger(samples, queries)\n",
        "    rdp = compute_rdp_from_ledger(formatted_ledger, orders)\n",
        "    eps = get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
        "    epsilon = eps\n",
        "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
        "\n",
        "\n",
        "def lr_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for a LR.\"\"\"\n",
        "\n",
        "  # Define logistic regression model using tf.keras.layers.\n",
        "  logits = tf.keras.layers.Dense(2).apply(features['x'])\n",
        "\n",
        "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
        "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits)\n",
        "  # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
        "  scalar_loss = tf.reduce_mean(input_tensor=vector_loss)\n",
        "\n",
        "  # Configure the training op (for TRAIN mode).\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "    if FLAGS.dpsgd:\n",
        "      ledger = privacy_ledger.PrivacyLedger(\n",
        "          population_size=FLAGS.training_data_size,\n",
        "          selection_probability=(FLAGS.batch_size / FLAGS.training_data_size))\n",
        "\n",
        "      # Use DP version of AdamOptimizer. Other optimizers are\n",
        "      # available in dp_optimizer. Most optimizers inheriting from\n",
        "      # tf.train.Optimizer should be wrappable in differentially private\n",
        "      # counterparts by calling dp_optimizer.optimizer_from_args().\n",
        "      # Setting num_microbatches to None is necessary for DP and\n",
        "      # per-example gradients\n",
        "      optimizer = dp_optimizer.DPAdamGaussianOptimizer(\n",
        "          l2_norm_clip=FLAGS.l2_norm_clip,\n",
        "          noise_multiplier=FLAGS.noise_multiplier,\n",
        "          num_microbatches=None,\n",
        "          ledger=ledger,\n",
        "          learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = [\n",
        "          EpsilonPrintingTrainingHook(ledger)\n",
        "      ]\n",
        "      opt_loss = vector_loss\n",
        "    else:\n",
        "      optimizer = AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "      training_hooks = []\n",
        "      opt_loss = scalar_loss\n",
        "    global_step = tf.compat.v1.train.get_global_step()\n",
        "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
        "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
        "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
        "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
        "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      train_op=train_op,\n",
        "                                      training_hooks=training_hooks)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode).\n",
        "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "    eval_metric_ops = {\n",
        "        'accuracy':\n",
        "            tf.compat.v1.metrics.accuracy(\n",
        "                labels=labels,\n",
        "                predictions=tf.argmax(input=logits, axis=1))\n",
        "    }\n",
        "\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=scalar_loss,\n",
        "                                      eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "def generate_data():\n",
        "  training_set = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"salary\"], index_col=False, header=None, sep=',')\n",
        "  test_set = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', names=[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"salary\"], index_col=False, header=None, sep=',')\n",
        "  # data preprocessing\n",
        "  training_set = training_set.replace(\" ?\", pd.np.nan).dropna(axis=0)\n",
        "  training_set = training_set.replace(\" Holand-Netherlands\", pd.np.nan).dropna(axis=0)\n",
        "  test_set = test_set.replace(\" ?\", pd.np.nan).dropna(axis=0)\n",
        "  training_set = pd.get_dummies(training_set, columns=['workclass', 'education', \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"])\n",
        "  test_set = pd.get_dummies(test_set, columns=['workclass', 'education', \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"])\n",
        "  training_set = training_set.replace(\" >50K\", 1)\n",
        "  training_set = training_set.replace(\" <=50K\", 0)\n",
        "  test_set = test_set.replace(\" >50K.\", 1)\n",
        "  test_set = test_set.replace(\" <=50K.\", 0)\n",
        "  training_set_Y = training_set[\"salary\"]\n",
        "  training_set = training_set.drop(columns=[\"salary\"])\n",
        "  test_set_Y = test_set[\"salary\"]\n",
        "  test_set = test_set.drop(columns=[\"salary\"])\n",
        "  return np.array(training_set, dtype=np.float32), np.array(training_set_Y, dtype=np.int32), np.array(test_set, dtype=np.float32), np.array(test_set_Y, dtype=np.int32)\n",
        "\n",
        "def main(unused_argv):\n",
        "  global epsilon\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "  # Load training and test data.\n",
        "  train_data, train_labels, test_data, test_labels = generate_data()\n",
        "\n",
        "  # Instantiate the tf.Estimator.\n",
        "  lr_classifier = tf.estimator.Estimator(model_fn=lr_model_fn,\n",
        "                                         model_dir=FLAGS.model_dir)\n",
        "\n",
        "  # Create tf.Estimator input functions for the training and test data.\n",
        "  train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': train_data},\n",
        "      y=train_labels,\n",
        "      batch_size=FLAGS.batch_size,\n",
        "      num_epochs=FLAGS.epochs,\n",
        "      shuffle=True)\n",
        "  eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
        "      x={'x': test_data},\n",
        "      y=test_labels,\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "\n",
        "  # Training loop.\n",
        "  steps_per_epoch = FLAGS.training_data_size // FLAGS.batch_size / 10\n",
        "  for epoch in range(1, 10*FLAGS.epochs + 1):\n",
        "    # Train the model for one epoch.\n",
        "    lr_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
        "\n",
        "    # Evaluate the model and print results\n",
        "    eval_results = lr_classifier.evaluate(input_fn=eval_input_fn)\n",
        "    test_accuracy = eval_results['accuracy']\n",
        "    print('Test accuracy after %.1f epochs is: %.3f' % (epoch/10, test_accuracy))\n",
        "    # stop training when epsilon achieve the desired value\n",
        "    if epsilon >= 1.0:\n",
        "      print(\"Test accuracy obtained with epsilon=1.00 and delta=10e-5 is:\",test_accuracy)\n",
        "      break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  app.run(main)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "For delta=1e-5, the current epsilon is: 0.96\n",
            "Test accuracy after 0.1 epochs is: 0.246\n",
            "For delta=1e-5, the current epsilon is: 0.96\n",
            "Test accuracy after 0.2 epochs is: 0.242\n",
            "For delta=1e-5, the current epsilon is: 0.97\n",
            "Test accuracy after 0.3 epochs is: 0.787\n",
            "For delta=1e-5, the current epsilon is: 0.97\n",
            "Test accuracy after 0.4 epochs is: 0.777\n",
            "For delta=1e-5, the current epsilon is: 0.97\n",
            "Test accuracy after 0.5 epochs is: 0.774\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.6 epochs is: 0.771\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.7 epochs is: 0.786\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.8 epochs is: 0.770\n",
            "For delta=1e-5, the current epsilon is: 0.98\n",
            "Test accuracy after 0.9 epochs is: 0.754\n",
            "For delta=1e-5, the current epsilon is: 0.99\n",
            "Test accuracy after 1.0 epochs is: 0.246\n",
            "For delta=1e-5, the current epsilon is: 0.99\n",
            "Test accuracy after 1.1 epochs is: 0.246\n",
            "For delta=1e-5, the current epsilon is: 0.99\n",
            "Test accuracy after 1.2 epochs is: 0.754\n",
            "For delta=1e-5, the current epsilon is: 0.99\n",
            "Test accuracy after 1.3 epochs is: 0.786\n",
            "For delta=1e-5, the current epsilon is: 1.00\n",
            "Test accuracy after 1.4 epochs is: 0.246\n",
            "For delta=1e-5, the current epsilon is: 1.00\n",
            "Test accuracy after 1.5 epochs is: 0.788\n",
            "For delta=1e-5, the current epsilon is: 1.00\n",
            "Test accuracy after 1.6 epochs is: 0.772\n",
            "Test accuracy obtained with epsilon=1.00 and delta=10e-5 is: 0.77224433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBh3_HslqXv3",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Your own problem in privacy-preserving machine learning\n",
        "\n",
        "State and solve your own problem related to privacy-preserving machine learning.\n",
        "\n",
        "You can use code available online, as long as you cite the source.\n",
        "\n",
        "You can for example try reproducing the results of some interesting paper using their data or your own data, try out some of the privacy attacks, or simply try the above examples using more complex models and/or on different data sets.\n",
        "\n",
        "If your problem is based on some previous problem, it should extend it in a non-trivial manner (not just running exact same code with new parameters or data).\n",
        "\n",
        "The evaluation of the project will take the difficulty of your chosen problem into account.\n",
        "\n",
        "This task is worth as much as two regular problems.\n",
        "\n",
        "**My problem**\n",
        "\n",
        "The problem I've chosen is to perform a membership inference attack on the cifar10 dataset using the code provided at https://github.com/spring-epfl/mia/tree/master/examples. It is important to note that in the provided code the target algorithm is a white-box and that means that the attacker knows the learning procedure and the network architecture. This is different from the paper on the membership inference we have read for one of the first lectures, where a black-box access was assumed for the attacker. To make the problem more interesting I performed two modifications on the provided code:\n",
        "1. To keep the runtime reasonable I cut the training set size from 50000 to 5000 examples and the test set size from 10000 to 1000 examples, in fact training a deep neural network with Google Colab on a big dataset can take a lot of time; \n",
        "2. I have increased the number of epochs for the training of the target model (from 12 to 30) in such a way that the target model can begin doing overfitting. In fact, if the target model begins to overfit this means that the attack model should obtain a better accuracy, increasing the effect of the attack (we have seen that the overfitting is one of the issue that makes the attack model better). I decided to make the target model begin to overfit because the idea of this task is to see if with the overfitting the attack model obtains an higher accuracy and after that if with the regularization it is possible to mitigate this attack.\n",
        " \n",
        "After running this modified example I tried to perform two mitigation techniques to prevent the target model to overfit:\n",
        "1. I tried to reduce the overfitting of the target model increasing the percentange of frozen units in the dropout layers of the model architecture. Reducing the overfitting should prevent the attack model to get an high accuracy, reducing the effect of the attack. The percentange of frozen units in the dropout layers has been changed from 0.25 to 0.5 in the convolutional layers, while it has been changed from 0.5 to 0.8 in the classification part of the target model architecture;\n",
        "2. I tried to reduce the overfitting of the target model stopping the training if after 2 epochs the validation accuracy didn't increase (this technique is called early stopping). This is a regularization technique that consists in simply stopping the training process if overfitting has been identified. The overfitting begins when the accuracy on the training set becomes large, while the accuracy on the validation set begins to stabilize. This means that the model is adapting to the training set and it isn't generalizing well.\n",
        "\n",
        "**Results obtained**\n",
        "\n",
        "It is possible to find the results of my experiments in the following four code cells.\n",
        "\n",
        "**First cell**\n",
        "\n",
        "The accuracy of the attack with the default settings (12 epochs of training) for the training of the target model is 0.6075.\n",
        "\n",
        "**Second cell**\n",
        "\n",
        "The accuracy of the attack with my settings (30 epochs of training) for the training of the target model has increased to 0.77625. This proves the fact that if the target model begins to overfit during its training, then the attack model obtains a better accuracy. The overfitting has been obtained simply increasing the number of epochs because this helps the model to adapt to the training set and avoid it to generalize well when we move to new data.\n",
        "\n",
        "**Third and fourth cells**\n",
        "\n",
        "With the regularization techniques I've obtained the following results:\n",
        "1. Increasing the percentange of frozen units in the dropout layers of the target model architecture I've obtained an attack accuracy of 0.5775. The overfitting of the target model has significantly reduced with the usage of dropout layers in a intelligent way; \n",
        "2. Stopping the training when overfitting has been detected I've obtained an attack accuracy of 0.54125. This seems to be a better approach to avoid the target model to overfit since it is better reducing the accuracy of the attack compared to the usage of dropout units.\n",
        "\n",
        "These experiments prove that the regularization is a good technique to prevent the membership inference attack but also to make the model able to generalize better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEv9P8Ii0TAK",
        "colab_type": "code",
        "outputId": "31e74262-8480-444e-fea9-1eb5c17e466b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "pip install mia"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mia\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/12/f149a7cd43e49725921e9884363aa3cbfea8a49c319a944eb71d48973fa9/mia-0.1.2.tar.gz\n",
            "Requirement already satisfied: numpy in /tensorflow-2.0.0/python3.6 (from mia) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from mia) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mia) (0.21.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from mia) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from mia) (4.28.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mia) (0.14.0)\n",
            "Building wheels for collected packages: mia\n",
            "  Building wheel for mia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mia: filename=mia-0.1.2-cp36-none-any.whl size=11079 sha256=3cd30f30554e7e156e3195a117287d197c4cc9976deef513ffd2d4777a26356a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/83/e4/baae7782aa0d2e45af485d25a7994bab3f76428e483252ce82\n",
            "Successfully built mia\n",
            "Installing collected packages: mia\n",
            "Successfully installed mia-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Gmgn4MgfTb",
        "colab_type": "code",
        "outputId": "26b43685-e104-4239-94f5-2b6fe56bcfee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train of the target model for 12 epochs (default settings)\n",
        "%tensorflow_version 2.x\n",
        "\"\"\"\n",
        "Example membership inference attack against a deep net classifier on the CIFAR10 dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "WIDTH = 32\n",
        "HEIGHT = 32\n",
        "CHANNELS = 3\n",
        "SHADOW_DATASET_SIZE = 400\n",
        "ATTACK_TEST_DATASET_SIZE = 400\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer(\n",
        "    \"target_epochs\", 12, \"Number of epochs to train target and shadow models.\"\n",
        ")\n",
        "flags.DEFINE_integer(\"attack_epochs\", 12, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_integer(\"num_shadows\", 3, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    X_train = X_train.astype(\"float32\")\n",
        "    X_test = X_test.astype(\"float32\")\n",
        "    y_train = y_train.astype(\"float32\")\n",
        "    y_test = y_test.astype(\"float32\")\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    return (X_train[0:5000], y_train[0:5000]), (X_test[0:1000], y_test[0:1000])\n",
        "\n",
        "\n",
        "def target_model_fn():\n",
        "    \"\"\"The architecture of the target (victim) model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(\n",
        "        layers.Conv2D(\n",
        "            32,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def attack_model_fn():\n",
        "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
        "    Following the original paper, this attack model is specific to the class of the input.\n",
        "    AttachModelBundle creates multiple instances of this model for each class.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
        "\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def demo(argv):\n",
        "    del argv  # Unused.\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = get_data()\n",
        "\n",
        "    # Train the target model.\n",
        "    print(\"Training the target model...\")\n",
        "    target_model = target_model_fn()\n",
        "    target_model.fit(\n",
        "        X_train, y_train, epochs=FLAGS.target_epochs, validation_split=0.1, verbose=True\n",
        "    )\n",
        "\n",
        "    # Train the shadow models.\n",
        "    smb = ShadowModelBundle(\n",
        "        target_model_fn,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=FLAGS.num_shadows,\n",
        "    )\n",
        "\n",
        "    # We assume that attacker's data were not seen in target's training.\n",
        "    attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
        "        X_test, y_test, test_size=0.1\n",
        "    )\n",
        "    print(attacker_X_train.shape, attacker_X_test.shape)\n",
        "\n",
        "    print(\"Training the shadow models...\")\n",
        "    X_shadow, y_shadow = smb.fit_transform(\n",
        "        attacker_X_train,\n",
        "        attacker_y_train,\n",
        "        fit_kwargs=dict(\n",
        "            epochs=FLAGS.target_epochs,\n",
        "            verbose=True,\n",
        "            validation_data=(attacker_X_test, attacker_y_test),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
        "    amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
        "\n",
        "    # Fit the attack models.\n",
        "    print(\"Training the attack models...\")\n",
        "    amb.fit(\n",
        "        X_shadow, y_shadow, fit_kwargs=dict(epochs=FLAGS.attack_epochs, verbose=True)\n",
        "    )\n",
        "\n",
        "    # Test the success of the attack.\n",
        "\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "        target_model, data_in, data_out\n",
        "    )\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = amb.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print(\"Attack accuracy:\",attack_accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Training the target model...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/12\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 2.1327 - accuracy: 0.1976 - val_loss: 1.8727 - val_accuracy: 0.3300\n",
            "Epoch 2/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.8408 - accuracy: 0.3178 - val_loss: 1.6527 - val_accuracy: 0.4380\n",
            "Epoch 3/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.6574 - accuracy: 0.3944 - val_loss: 1.5994 - val_accuracy: 0.4520\n",
            "Epoch 4/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.5207 - accuracy: 0.4453 - val_loss: 1.4637 - val_accuracy: 0.5040\n",
            "Epoch 5/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.4153 - accuracy: 0.4804 - val_loss: 1.4266 - val_accuracy: 0.5060\n",
            "Epoch 6/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.3415 - accuracy: 0.5062 - val_loss: 1.3559 - val_accuracy: 0.5280\n",
            "Epoch 7/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.2208 - accuracy: 0.5558 - val_loss: 1.3693 - val_accuracy: 0.5440\n",
            "Epoch 8/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.1598 - accuracy: 0.5780 - val_loss: 1.2564 - val_accuracy: 0.5820\n",
            "Epoch 9/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.0914 - accuracy: 0.6102 - val_loss: 1.2847 - val_accuracy: 0.5580\n",
            "Epoch 10/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.9911 - accuracy: 0.6422 - val_loss: 1.2345 - val_accuracy: 0.5560\n",
            "Epoch 11/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.8856 - accuracy: 0.6836 - val_loss: 1.2620 - val_accuracy: 0.5800\n",
            "Epoch 12/12\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.8185 - accuracy: 0.7036 - val_loss: 1.2385 - val_accuracy: 0.6000\n",
            "(900, 32, 32, 3) (100, 32, 32, 3)\n",
            "Training the shadow models...\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/12\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3137 - accuracy: 0.0875 - val_loss: 2.2972 - val_accuracy: 0.1800\n",
            "Epoch 2/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2876 - accuracy: 0.1450 - val_loss: 2.2807 - val_accuracy: 0.2000\n",
            "Epoch 3/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2365 - accuracy: 0.2025 - val_loss: 2.1815 - val_accuracy: 0.2000\n",
            "Epoch 4/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1147 - accuracy: 0.2325 - val_loss: 2.1256 - val_accuracy: 0.2000\n",
            "Epoch 5/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0379 - accuracy: 0.2300 - val_loss: 2.0524 - val_accuracy: 0.2500\n",
            "Epoch 6/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9585 - accuracy: 0.3000 - val_loss: 2.0770 - val_accuracy: 0.2600\n",
            "Epoch 7/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9750 - accuracy: 0.2875 - val_loss: 2.0203 - val_accuracy: 0.2900\n",
            "Epoch 8/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8569 - accuracy: 0.3225 - val_loss: 1.9857 - val_accuracy: 0.3600\n",
            "Epoch 9/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7763 - accuracy: 0.3575 - val_loss: 1.9509 - val_accuracy: 0.3700\n",
            "Epoch 10/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6860 - accuracy: 0.3775 - val_loss: 1.7897 - val_accuracy: 0.4000\n",
            "Epoch 11/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5924 - accuracy: 0.4650 - val_loss: 1.9871 - val_accuracy: 0.3600\n",
            "Epoch 12/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4922 - accuracy: 0.4675 - val_loss: 1.9390 - val_accuracy: 0.3800\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/12\n",
            "400/400 [==============================] - 3s 9ms/sample - loss: 2.3226 - accuracy: 0.0925 - val_loss: 2.3022 - val_accuracy: 0.0500\n",
            "Epoch 2/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.3008 - accuracy: 0.1150 - val_loss: 2.2908 - val_accuracy: 0.1000\n",
            "Epoch 3/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2506 - accuracy: 0.1500 - val_loss: 2.1817 - val_accuracy: 0.1800\n",
            "Epoch 4/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1839 - accuracy: 0.1925 - val_loss: 2.1377 - val_accuracy: 0.2200\n",
            "Epoch 5/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1220 - accuracy: 0.2050 - val_loss: 1.9845 - val_accuracy: 0.2900\n",
            "Epoch 6/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9946 - accuracy: 0.2750 - val_loss: 2.1203 - val_accuracy: 0.2500\n",
            "Epoch 7/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9693 - accuracy: 0.2950 - val_loss: 1.8973 - val_accuracy: 0.3400\n",
            "Epoch 8/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8833 - accuracy: 0.3225 - val_loss: 1.8827 - val_accuracy: 0.3300\n",
            "Epoch 9/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7570 - accuracy: 0.3550 - val_loss: 1.8115 - val_accuracy: 0.3500\n",
            "Epoch 10/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7126 - accuracy: 0.3850 - val_loss: 1.8406 - val_accuracy: 0.3700\n",
            "Epoch 11/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6382 - accuracy: 0.4075 - val_loss: 1.7448 - val_accuracy: 0.4300\n",
            "Epoch 12/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5894 - accuracy: 0.3800 - val_loss: 1.6410 - val_accuracy: 0.4300\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/12\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3067 - accuracy: 0.1275 - val_loss: 2.2864 - val_accuracy: 0.1200\n",
            "Epoch 2/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2891 - accuracy: 0.1400 - val_loss: 2.2797 - val_accuracy: 0.1200\n",
            "Epoch 3/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2564 - accuracy: 0.1600 - val_loss: 2.2042 - val_accuracy: 0.2500\n",
            "Epoch 4/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1018 - accuracy: 0.2600 - val_loss: 2.2320 - val_accuracy: 0.2300\n",
            "Epoch 5/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1214 - accuracy: 0.2325 - val_loss: 2.0901 - val_accuracy: 0.2500\n",
            "Epoch 6/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0181 - accuracy: 0.2525 - val_loss: 1.9747 - val_accuracy: 0.2800\n",
            "Epoch 7/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9106 - accuracy: 0.3175 - val_loss: 1.9776 - val_accuracy: 0.2900\n",
            "Epoch 8/12\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 1.8884 - accuracy: 0.3200 - val_loss: 1.8614 - val_accuracy: 0.2900\n",
            "Epoch 9/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7689 - accuracy: 0.3525 - val_loss: 1.7091 - val_accuracy: 0.3900\n",
            "Epoch 10/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6581 - accuracy: 0.3825 - val_loss: 1.7954 - val_accuracy: 0.3500\n",
            "Epoch 11/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6289 - accuracy: 0.4025 - val_loss: 1.8226 - val_accuracy: 0.3600\n",
            "Epoch 12/12\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5524 - accuracy: 0.4225 - val_loss: 1.7289 - val_accuracy: 0.3800\n",
            "Training the attack models...\n",
            "Train on 232 samples\n",
            "Epoch 1/12\n",
            "232/232 [==============================] - 1s 3ms/sample - loss: 0.6897 - accuracy: 0.5431\n",
            "Epoch 2/12\n",
            "232/232 [==============================] - 0s 120us/sample - loss: 0.6817 - accuracy: 0.5819\n",
            "Epoch 3/12\n",
            "232/232 [==============================] - 0s 104us/sample - loss: 0.6765 - accuracy: 0.5948\n",
            "Epoch 4/12\n",
            "232/232 [==============================] - 0s 108us/sample - loss: 0.6731 - accuracy: 0.5776\n",
            "Epoch 5/12\n",
            "232/232 [==============================] - 0s 109us/sample - loss: 0.6649 - accuracy: 0.5948\n",
            "Epoch 6/12\n",
            "232/232 [==============================] - 0s 102us/sample - loss: 0.6609 - accuracy: 0.6422\n",
            "Epoch 7/12\n",
            "232/232 [==============================] - 0s 112us/sample - loss: 0.6500 - accuracy: 0.6336\n",
            "Epoch 8/12\n",
            "232/232 [==============================] - 0s 111us/sample - loss: 0.6576 - accuracy: 0.6293\n",
            "Epoch 9/12\n",
            "232/232 [==============================] - 0s 122us/sample - loss: 0.6434 - accuracy: 0.6422\n",
            "Epoch 10/12\n",
            "232/232 [==============================] - 0s 138us/sample - loss: 0.6441 - accuracy: 0.6422\n",
            "Epoch 11/12\n",
            "232/232 [==============================] - 0s 106us/sample - loss: 0.6382 - accuracy: 0.6638\n",
            "Epoch 12/12\n",
            "232/232 [==============================] - 0s 133us/sample - loss: 0.6431 - accuracy: 0.6336\n",
            "Train on 206 samples\n",
            "Epoch 1/12\n",
            "206/206 [==============================] - 1s 3ms/sample - loss: 0.6864 - accuracy: 0.5825\n",
            "Epoch 2/12\n",
            "206/206 [==============================] - 0s 127us/sample - loss: 0.6741 - accuracy: 0.6262\n",
            "Epoch 3/12\n",
            "206/206 [==============================] - 0s 101us/sample - loss: 0.6587 - accuracy: 0.6602\n",
            "Epoch 4/12\n",
            "206/206 [==============================] - 0s 120us/sample - loss: 0.6445 - accuracy: 0.6699\n",
            "Epoch 5/12\n",
            "206/206 [==============================] - 0s 101us/sample - loss: 0.6290 - accuracy: 0.6796\n",
            "Epoch 6/12\n",
            "206/206 [==============================] - 0s 111us/sample - loss: 0.6108 - accuracy: 0.6748\n",
            "Epoch 7/12\n",
            "206/206 [==============================] - 0s 115us/sample - loss: 0.6008 - accuracy: 0.6650\n",
            "Epoch 8/12\n",
            "206/206 [==============================] - 0s 121us/sample - loss: 0.5996 - accuracy: 0.6990\n",
            "Epoch 9/12\n",
            "206/206 [==============================] - 0s 115us/sample - loss: 0.6018 - accuracy: 0.6699\n",
            "Epoch 10/12\n",
            "206/206 [==============================] - 0s 104us/sample - loss: 0.5922 - accuracy: 0.6748\n",
            "Epoch 11/12\n",
            "206/206 [==============================] - 0s 154us/sample - loss: 0.5738 - accuracy: 0.6845\n",
            "Epoch 12/12\n",
            "206/206 [==============================] - 0s 97us/sample - loss: 0.5814 - accuracy: 0.6845\n",
            "Train on 235 samples\n",
            "Epoch 1/12\n",
            "235/235 [==============================] - 1s 3ms/sample - loss: 0.6931 - accuracy: 0.5149\n",
            "Epoch 2/12\n",
            "235/235 [==============================] - 0s 110us/sample - loss: 0.6883 - accuracy: 0.6043\n",
            "Epoch 3/12\n",
            "235/235 [==============================] - 0s 96us/sample - loss: 0.6872 - accuracy: 0.5319\n",
            "Epoch 4/12\n",
            "235/235 [==============================] - 0s 94us/sample - loss: 0.6802 - accuracy: 0.5957\n",
            "Epoch 5/12\n",
            "235/235 [==============================] - 0s 106us/sample - loss: 0.6778 - accuracy: 0.6128\n",
            "Epoch 6/12\n",
            "235/235 [==============================] - 0s 122us/sample - loss: 0.6719 - accuracy: 0.6638\n",
            "Epoch 7/12\n",
            "235/235 [==============================] - 0s 102us/sample - loss: 0.6618 - accuracy: 0.6426\n",
            "Epoch 8/12\n",
            "235/235 [==============================] - 0s 92us/sample - loss: 0.6554 - accuracy: 0.6809\n",
            "Epoch 9/12\n",
            "235/235 [==============================] - 0s 103us/sample - loss: 0.6510 - accuracy: 0.6553\n",
            "Epoch 10/12\n",
            "235/235 [==============================] - 0s 94us/sample - loss: 0.6425 - accuracy: 0.6979\n",
            "Epoch 11/12\n",
            "235/235 [==============================] - 0s 120us/sample - loss: 0.6298 - accuracy: 0.6851\n",
            "Epoch 12/12\n",
            "235/235 [==============================] - 0s 100us/sample - loss: 0.6410 - accuracy: 0.6468\n",
            "Train on 259 samples\n",
            "Epoch 1/12\n",
            "259/259 [==============================] - 1s 2ms/sample - loss: 0.6911 - accuracy: 0.5174\n",
            "Epoch 2/12\n",
            "259/259 [==============================] - 0s 109us/sample - loss: 0.6881 - accuracy: 0.5792\n",
            "Epoch 3/12\n",
            "259/259 [==============================] - 0s 106us/sample - loss: 0.6819 - accuracy: 0.6100\n",
            "Epoch 4/12\n",
            "259/259 [==============================] - 0s 101us/sample - loss: 0.6757 - accuracy: 0.6062\n",
            "Epoch 5/12\n",
            "259/259 [==============================] - 0s 97us/sample - loss: 0.6698 - accuracy: 0.5869\n",
            "Epoch 6/12\n",
            "259/259 [==============================] - 0s 108us/sample - loss: 0.6564 - accuracy: 0.6293\n",
            "Epoch 7/12\n",
            "259/259 [==============================] - 0s 119us/sample - loss: 0.6492 - accuracy: 0.6371\n",
            "Epoch 8/12\n",
            "259/259 [==============================] - 0s 106us/sample - loss: 0.6298 - accuracy: 0.6873\n",
            "Epoch 9/12\n",
            "259/259 [==============================] - 0s 102us/sample - loss: 0.6281 - accuracy: 0.6795\n",
            "Epoch 10/12\n",
            "259/259 [==============================] - 0s 111us/sample - loss: 0.6216 - accuracy: 0.6834\n",
            "Epoch 11/12\n",
            "259/259 [==============================] - 0s 106us/sample - loss: 0.6025 - accuracy: 0.6988\n",
            "Epoch 12/12\n",
            "259/259 [==============================] - 0s 99us/sample - loss: 0.6056 - accuracy: 0.6834\n",
            "Train on 195 samples\n",
            "Epoch 1/12\n",
            "195/195 [==============================] - 1s 3ms/sample - loss: 0.6934 - accuracy: 0.5077\n",
            "Epoch 2/12\n",
            "195/195 [==============================] - 0s 127us/sample - loss: 0.6929 - accuracy: 0.5385\n",
            "Epoch 3/12\n",
            "195/195 [==============================] - 0s 125us/sample - loss: 0.6899 - accuracy: 0.5333\n",
            "Epoch 4/12\n",
            "195/195 [==============================] - 0s 119us/sample - loss: 0.6902 - accuracy: 0.5538\n",
            "Epoch 5/12\n",
            "195/195 [==============================] - 0s 133us/sample - loss: 0.6890 - accuracy: 0.5487\n",
            "Epoch 6/12\n",
            "195/195 [==============================] - 0s 136us/sample - loss: 0.6884 - accuracy: 0.5179\n",
            "Epoch 7/12\n",
            "195/195 [==============================] - 0s 129us/sample - loss: 0.6871 - accuracy: 0.5385\n",
            "Epoch 8/12\n",
            "195/195 [==============================] - 0s 125us/sample - loss: 0.6824 - accuracy: 0.5846\n",
            "Epoch 9/12\n",
            "195/195 [==============================] - 0s 116us/sample - loss: 0.6800 - accuracy: 0.5590\n",
            "Epoch 10/12\n",
            "195/195 [==============================] - 0s 114us/sample - loss: 0.6767 - accuracy: 0.5897\n",
            "Epoch 11/12\n",
            "195/195 [==============================] - 0s 138us/sample - loss: 0.6746 - accuracy: 0.5897\n",
            "Epoch 12/12\n",
            "195/195 [==============================] - 0s 111us/sample - loss: 0.6733 - accuracy: 0.6000\n",
            "Train on 225 samples\n",
            "Epoch 1/12\n",
            "225/225 [==============================] - 1s 3ms/sample - loss: 0.6927 - accuracy: 0.5067\n",
            "Epoch 2/12\n",
            "225/225 [==============================] - 0s 99us/sample - loss: 0.6892 - accuracy: 0.5156\n",
            "Epoch 3/12\n",
            "225/225 [==============================] - 0s 111us/sample - loss: 0.6859 - accuracy: 0.5556\n",
            "Epoch 4/12\n",
            "225/225 [==============================] - 0s 116us/sample - loss: 0.6807 - accuracy: 0.5511\n",
            "Epoch 5/12\n",
            "225/225 [==============================] - 0s 102us/sample - loss: 0.6806 - accuracy: 0.5244\n",
            "Epoch 6/12\n",
            "225/225 [==============================] - 0s 122us/sample - loss: 0.6728 - accuracy: 0.5467\n",
            "Epoch 7/12\n",
            "225/225 [==============================] - 0s 113us/sample - loss: 0.6678 - accuracy: 0.6000\n",
            "Epoch 8/12\n",
            "225/225 [==============================] - 0s 105us/sample - loss: 0.6633 - accuracy: 0.6222\n",
            "Epoch 9/12\n",
            "225/225 [==============================] - 0s 105us/sample - loss: 0.6632 - accuracy: 0.6267\n",
            "Epoch 10/12\n",
            "225/225 [==============================] - 0s 108us/sample - loss: 0.6526 - accuracy: 0.6578\n",
            "Epoch 11/12\n",
            "225/225 [==============================] - 0s 106us/sample - loss: 0.6395 - accuracy: 0.6711\n",
            "Epoch 12/12\n",
            "225/225 [==============================] - 0s 106us/sample - loss: 0.6262 - accuracy: 0.6800\n",
            "Train on 273 samples\n",
            "Epoch 1/12\n",
            "273/273 [==============================] - 1s 3ms/sample - loss: 0.6925 - accuracy: 0.4908\n",
            "Epoch 2/12\n",
            "273/273 [==============================] - 0s 95us/sample - loss: 0.6907 - accuracy: 0.5092\n",
            "Epoch 3/12\n",
            "273/273 [==============================] - 0s 92us/sample - loss: 0.6893 - accuracy: 0.5568\n",
            "Epoch 4/12\n",
            "273/273 [==============================] - 0s 96us/sample - loss: 0.6890 - accuracy: 0.5421\n",
            "Epoch 5/12\n",
            "273/273 [==============================] - 0s 91us/sample - loss: 0.6866 - accuracy: 0.5604\n",
            "Epoch 6/12\n",
            "273/273 [==============================] - 0s 98us/sample - loss: 0.6838 - accuracy: 0.5495\n",
            "Epoch 7/12\n",
            "273/273 [==============================] - 0s 95us/sample - loss: 0.6810 - accuracy: 0.5714\n",
            "Epoch 8/12\n",
            "273/273 [==============================] - 0s 99us/sample - loss: 0.6785 - accuracy: 0.5714\n",
            "Epoch 9/12\n",
            "273/273 [==============================] - 0s 106us/sample - loss: 0.6808 - accuracy: 0.5714\n",
            "Epoch 10/12\n",
            "273/273 [==============================] - 0s 102us/sample - loss: 0.6734 - accuracy: 0.5971\n",
            "Epoch 11/12\n",
            "273/273 [==============================] - 0s 99us/sample - loss: 0.6749 - accuracy: 0.5495\n",
            "Epoch 12/12\n",
            "273/273 [==============================] - 0s 98us/sample - loss: 0.6705 - accuracy: 0.5788\n",
            "Train on 236 samples\n",
            "Epoch 1/12\n",
            "236/236 [==============================] - 1s 3ms/sample - loss: 0.6926 - accuracy: 0.4915\n",
            "Epoch 2/12\n",
            "236/236 [==============================] - 0s 139us/sample - loss: 0.6896 - accuracy: 0.5847\n",
            "Epoch 3/12\n",
            "236/236 [==============================] - 0s 111us/sample - loss: 0.6853 - accuracy: 0.6271\n",
            "Epoch 4/12\n",
            "236/236 [==============================] - 0s 125us/sample - loss: 0.6858 - accuracy: 0.5932\n",
            "Epoch 5/12\n",
            "236/236 [==============================] - 0s 108us/sample - loss: 0.6764 - accuracy: 0.6653\n",
            "Epoch 6/12\n",
            "236/236 [==============================] - 0s 109us/sample - loss: 0.6774 - accuracy: 0.5975\n",
            "Epoch 7/12\n",
            "236/236 [==============================] - 0s 110us/sample - loss: 0.6706 - accuracy: 0.6059\n",
            "Epoch 8/12\n",
            "236/236 [==============================] - 0s 105us/sample - loss: 0.6648 - accuracy: 0.6186\n",
            "Epoch 9/12\n",
            "236/236 [==============================] - 0s 114us/sample - loss: 0.6579 - accuracy: 0.6653\n",
            "Epoch 10/12\n",
            "236/236 [==============================] - 0s 137us/sample - loss: 0.6602 - accuracy: 0.6271\n",
            "Epoch 11/12\n",
            "236/236 [==============================] - 0s 104us/sample - loss: 0.6499 - accuracy: 0.6441\n",
            "Epoch 12/12\n",
            "236/236 [==============================] - 0s 115us/sample - loss: 0.6554 - accuracy: 0.6144\n",
            "Train on 250 samples\n",
            "Epoch 1/12\n",
            "250/250 [==============================] - 1s 3ms/sample - loss: 0.6945 - accuracy: 0.4840\n",
            "Epoch 2/12\n",
            "250/250 [==============================] - 0s 113us/sample - loss: 0.6894 - accuracy: 0.6240\n",
            "Epoch 3/12\n",
            "250/250 [==============================] - 0s 96us/sample - loss: 0.6852 - accuracy: 0.6200\n",
            "Epoch 4/12\n",
            "250/250 [==============================] - 0s 104us/sample - loss: 0.6785 - accuracy: 0.6120\n",
            "Epoch 5/12\n",
            "250/250 [==============================] - 0s 101us/sample - loss: 0.6748 - accuracy: 0.6360\n",
            "Epoch 6/12\n",
            "250/250 [==============================] - 0s 98us/sample - loss: 0.6646 - accuracy: 0.6440\n",
            "Epoch 7/12\n",
            "250/250 [==============================] - 0s 105us/sample - loss: 0.6582 - accuracy: 0.6240\n",
            "Epoch 8/12\n",
            "250/250 [==============================] - 0s 121us/sample - loss: 0.6536 - accuracy: 0.6480\n",
            "Epoch 9/12\n",
            "250/250 [==============================] - 0s 103us/sample - loss: 0.6532 - accuracy: 0.6320\n",
            "Epoch 10/12\n",
            "250/250 [==============================] - 0s 122us/sample - loss: 0.6365 - accuracy: 0.6360\n",
            "Epoch 11/12\n",
            "250/250 [==============================] - 0s 108us/sample - loss: 0.6399 - accuracy: 0.6280\n",
            "Epoch 12/12\n",
            "250/250 [==============================] - 0s 90us/sample - loss: 0.6277 - accuracy: 0.6360\n",
            "Train on 289 samples\n",
            "Epoch 1/12\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.6917 - accuracy: 0.5398\n",
            "Epoch 2/12\n",
            "289/289 [==============================] - 0s 106us/sample - loss: 0.6861 - accuracy: 0.5571\n",
            "Epoch 3/12\n",
            "289/289 [==============================] - 0s 106us/sample - loss: 0.6840 - accuracy: 0.5606\n",
            "Epoch 4/12\n",
            "289/289 [==============================] - 0s 105us/sample - loss: 0.6751 - accuracy: 0.5606\n",
            "Epoch 5/12\n",
            "289/289 [==============================] - 0s 105us/sample - loss: 0.6792 - accuracy: 0.5606\n",
            "Epoch 6/12\n",
            "289/289 [==============================] - 0s 90us/sample - loss: 0.6741 - accuracy: 0.5571\n",
            "Epoch 7/12\n",
            "289/289 [==============================] - 0s 109us/sample - loss: 0.6698 - accuracy: 0.5709\n",
            "Epoch 8/12\n",
            "289/289 [==============================] - 0s 119us/sample - loss: 0.6715 - accuracy: 0.5813\n",
            "Epoch 9/12\n",
            "289/289 [==============================] - 0s 126us/sample - loss: 0.6690 - accuracy: 0.6125\n",
            "Epoch 10/12\n",
            "289/289 [==============================] - 0s 115us/sample - loss: 0.6673 - accuracy: 0.5917\n",
            "Epoch 11/12\n",
            "289/289 [==============================] - 0s 112us/sample - loss: 0.6676 - accuracy: 0.5848\n",
            "Epoch 12/12\n",
            "289/289 [==============================] - 0s 108us/sample - loss: 0.6560 - accuracy: 0.6159\n",
            "Attack accuracy: 0.6075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQslENUS0ER6",
        "colab_type": "code",
        "outputId": "4b0809d5-6d49-495a-ac5a-a5bd88c4f904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the target model for 30 epochs (my settings to get overfitting)\n",
        "%tensorflow_version 2.x\n",
        "\"\"\"\n",
        "Example membership inference attack against a deep net classifier on the CIFAR10 dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "WIDTH = 32\n",
        "HEIGHT = 32\n",
        "CHANNELS = 3\n",
        "SHADOW_DATASET_SIZE = 400\n",
        "ATTACK_TEST_DATASET_SIZE = 400\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer(\n",
        "    \"target_epochs\", 30, \"Number of epochs to train target and shadow models.\"\n",
        ")\n",
        "flags.DEFINE_integer(\"attack_epochs\", 30, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_integer(\"num_shadows\", 3, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    X_train = X_train.astype(\"float32\")\n",
        "    X_test = X_test.astype(\"float32\")\n",
        "    y_train = y_train.astype(\"float32\")\n",
        "    y_test = y_test.astype(\"float32\")\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    return (X_train[0:5000], y_train[0:5000]), (X_test[0:1000], y_test[0:1000])\n",
        "\n",
        "\n",
        "def target_model_fn():\n",
        "    \"\"\"The architecture of the target (victim) model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(\n",
        "        layers.Conv2D(\n",
        "            32,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def attack_model_fn():\n",
        "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
        "    Following the original paper, this attack model is specific to the class of the input.\n",
        "    AttachModelBundle creates multiple instances of this model for each class.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
        "\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def demo(argv):\n",
        "    del argv  # Unused.\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = get_data()\n",
        "\n",
        "    # Train the target model.\n",
        "    print(\"Training the target model...\")\n",
        "    target_model = target_model_fn()\n",
        "    target_model.fit(\n",
        "        X_train, y_train, epochs=FLAGS.target_epochs, validation_split=0.1, verbose=True\n",
        "    )\n",
        "\n",
        "    # Train the shadow models.\n",
        "    smb = ShadowModelBundle(\n",
        "        target_model_fn,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=FLAGS.num_shadows,\n",
        "    )\n",
        "\n",
        "    # We assume that attacker's data were not seen in target's training.\n",
        "    attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
        "        X_test, y_test, test_size=0.1\n",
        "    )\n",
        "    print(attacker_X_train.shape, attacker_X_test.shape)\n",
        "\n",
        "    print(\"Training the shadow models...\")\n",
        "    X_shadow, y_shadow = smb.fit_transform(\n",
        "        attacker_X_train,\n",
        "        attacker_y_train,\n",
        "        fit_kwargs=dict(\n",
        "            epochs=FLAGS.target_epochs,\n",
        "            verbose=True,\n",
        "            validation_data=(attacker_X_test, attacker_y_test),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
        "    amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
        "\n",
        "    # Fit the attack models.\n",
        "    print(\"Training the attack models...\")\n",
        "    amb.fit(\n",
        "        X_shadow, y_shadow, fit_kwargs=dict(epochs=FLAGS.attack_epochs, verbose=True)\n",
        "    )\n",
        "\n",
        "    # Test the success of the attack.\n",
        "\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "        target_model, data_in, data_out\n",
        "    )\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = amb.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print(\"Attack accuracy:\",attack_accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Training the target model...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/30\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 2.1037 - accuracy: 0.2178 - val_loss: 1.9692 - val_accuracy: 0.2820\n",
            "Epoch 2/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.7437 - accuracy: 0.3520 - val_loss: 1.6211 - val_accuracy: 0.4260\n",
            "Epoch 3/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.5875 - accuracy: 0.4216 - val_loss: 1.6266 - val_accuracy: 0.4500\n",
            "Epoch 4/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.4767 - accuracy: 0.4600 - val_loss: 1.5044 - val_accuracy: 0.4900\n",
            "Epoch 5/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.3832 - accuracy: 0.4984 - val_loss: 1.4288 - val_accuracy: 0.4760\n",
            "Epoch 6/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.2960 - accuracy: 0.5287 - val_loss: 1.3451 - val_accuracy: 0.5420\n",
            "Epoch 7/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.2043 - accuracy: 0.5607 - val_loss: 1.3504 - val_accuracy: 0.5200\n",
            "Epoch 8/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.1332 - accuracy: 0.5898 - val_loss: 1.2851 - val_accuracy: 0.5540\n",
            "Epoch 9/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.0639 - accuracy: 0.6100 - val_loss: 1.3700 - val_accuracy: 0.5520\n",
            "Epoch 10/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.9648 - accuracy: 0.6524 - val_loss: 1.3381 - val_accuracy: 0.5700\n",
            "Epoch 11/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.8913 - accuracy: 0.6742 - val_loss: 1.3087 - val_accuracy: 0.5780\n",
            "Epoch 12/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.8225 - accuracy: 0.6984 - val_loss: 1.3341 - val_accuracy: 0.5620\n",
            "Epoch 13/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 0.7497 - accuracy: 0.7351 - val_loss: 1.2556 - val_accuracy: 0.6100\n",
            "Epoch 14/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.6924 - accuracy: 0.7516 - val_loss: 1.4755 - val_accuracy: 0.5800\n",
            "Epoch 15/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.5920 - accuracy: 0.7887 - val_loss: 1.3502 - val_accuracy: 0.5900\n",
            "Epoch 16/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.5406 - accuracy: 0.8144 - val_loss: 1.3638 - val_accuracy: 0.6080\n",
            "Epoch 17/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.5180 - accuracy: 0.8187 - val_loss: 1.4153 - val_accuracy: 0.6020\n",
            "Epoch 18/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.4709 - accuracy: 0.8282 - val_loss: 1.3678 - val_accuracy: 0.6120\n",
            "Epoch 19/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 0.4046 - accuracy: 0.8584 - val_loss: 1.5313 - val_accuracy: 0.6080\n",
            "Epoch 20/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.3864 - accuracy: 0.8636 - val_loss: 1.7086 - val_accuracy: 0.5600\n",
            "Epoch 21/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.3542 - accuracy: 0.8747 - val_loss: 1.6612 - val_accuracy: 0.5880\n",
            "Epoch 22/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.3277 - accuracy: 0.8796 - val_loss: 1.5443 - val_accuracy: 0.5940\n",
            "Epoch 23/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.2972 - accuracy: 0.8956 - val_loss: 1.7443 - val_accuracy: 0.5900\n",
            "Epoch 24/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.3126 - accuracy: 0.8933 - val_loss: 1.6750 - val_accuracy: 0.5760\n",
            "Epoch 25/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.2684 - accuracy: 0.9058 - val_loss: 1.6936 - val_accuracy: 0.6280\n",
            "Epoch 26/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.2810 - accuracy: 0.9018 - val_loss: 1.6551 - val_accuracy: 0.5940\n",
            "Epoch 27/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 0.2492 - accuracy: 0.9131 - val_loss: 1.6705 - val_accuracy: 0.5980\n",
            "Epoch 28/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 0.2333 - accuracy: 0.9153 - val_loss: 1.7937 - val_accuracy: 0.6120\n",
            "Epoch 29/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.2337 - accuracy: 0.9178 - val_loss: 1.7035 - val_accuracy: 0.6240\n",
            "Epoch 30/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 0.2114 - accuracy: 0.9258 - val_loss: 1.7643 - val_accuracy: 0.6100\n",
            "(900, 32, 32, 3) (100, 32, 32, 3)\n",
            "Training the shadow models...\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3017 - accuracy: 0.1450 - val_loss: 2.2900 - val_accuracy: 0.1200\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1975 - accuracy: 0.1950 - val_loss: 2.1759 - val_accuracy: 0.2100\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0658 - accuracy: 0.2375 - val_loss: 2.0830 - val_accuracy: 0.2700\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0352 - accuracy: 0.2400 - val_loss: 2.1230 - val_accuracy: 0.2400\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9640 - accuracy: 0.3000 - val_loss: 2.0343 - val_accuracy: 0.2600\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9264 - accuracy: 0.2850 - val_loss: 2.0268 - val_accuracy: 0.2900\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8561 - accuracy: 0.3075 - val_loss: 1.9591 - val_accuracy: 0.3100\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7788 - accuracy: 0.3500 - val_loss: 1.9533 - val_accuracy: 0.3100\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6559 - accuracy: 0.4125 - val_loss: 1.8703 - val_accuracy: 0.3000\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5749 - accuracy: 0.4075 - val_loss: 1.8470 - val_accuracy: 0.3200\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4834 - accuracy: 0.4925 - val_loss: 1.7783 - val_accuracy: 0.3100\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3025 - accuracy: 0.5400 - val_loss: 1.8093 - val_accuracy: 0.3200\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2971 - accuracy: 0.5550 - val_loss: 1.7881 - val_accuracy: 0.3300\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1754 - accuracy: 0.5725 - val_loss: 1.9620 - val_accuracy: 0.3600\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.0706 - accuracy: 0.5775 - val_loss: 1.9683 - val_accuracy: 0.3600\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.9209 - accuracy: 0.6550 - val_loss: 2.0101 - val_accuracy: 0.3900\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8542 - accuracy: 0.7050 - val_loss: 1.8882 - val_accuracy: 0.4400\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.7261 - accuracy: 0.7625 - val_loss: 1.8376 - val_accuracy: 0.4100\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5603 - accuracy: 0.7975 - val_loss: 2.0918 - val_accuracy: 0.4000\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5527 - accuracy: 0.8200 - val_loss: 2.0432 - val_accuracy: 0.3600\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4833 - accuracy: 0.8400 - val_loss: 1.9877 - val_accuracy: 0.4200\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3957 - accuracy: 0.8675 - val_loss: 2.1619 - val_accuracy: 0.3800\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3906 - accuracy: 0.8575 - val_loss: 2.1127 - val_accuracy: 0.4000\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3163 - accuracy: 0.9175 - val_loss: 2.1945 - val_accuracy: 0.4100\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2592 - accuracy: 0.9050 - val_loss: 2.8799 - val_accuracy: 0.4000\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2729 - accuracy: 0.9075 - val_loss: 2.5529 - val_accuracy: 0.4000\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2362 - accuracy: 0.9100 - val_loss: 2.5903 - val_accuracy: 0.4100\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.1735 - accuracy: 0.9450 - val_loss: 2.8217 - val_accuracy: 0.3700\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2204 - accuracy: 0.9225 - val_loss: 2.9115 - val_accuracy: 0.3900\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.1762 - accuracy: 0.9500 - val_loss: 2.8358 - val_accuracy: 0.3500\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3052 - accuracy: 0.0850 - val_loss: 2.3035 - val_accuracy: 0.0900\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2741 - accuracy: 0.1375 - val_loss: 2.2799 - val_accuracy: 0.2300\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2021 - accuracy: 0.2150 - val_loss: 2.1463 - val_accuracy: 0.2700\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1148 - accuracy: 0.2475 - val_loss: 2.1081 - val_accuracy: 0.2600\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9975 - accuracy: 0.2725 - val_loss: 2.1392 - val_accuracy: 0.2400\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8706 - accuracy: 0.3325 - val_loss: 1.9518 - val_accuracy: 0.3100\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7772 - accuracy: 0.3475 - val_loss: 1.8835 - val_accuracy: 0.3400\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6671 - accuracy: 0.4375 - val_loss: 1.8435 - val_accuracy: 0.3300\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5752 - accuracy: 0.4650 - val_loss: 1.8453 - val_accuracy: 0.3600\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5404 - accuracy: 0.4600 - val_loss: 1.7232 - val_accuracy: 0.3800\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4525 - accuracy: 0.4675 - val_loss: 1.8291 - val_accuracy: 0.3700\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4219 - accuracy: 0.4900 - val_loss: 1.7501 - val_accuracy: 0.3100\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3307 - accuracy: 0.5300 - val_loss: 1.8513 - val_accuracy: 0.4200\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2421 - accuracy: 0.5575 - val_loss: 1.7210 - val_accuracy: 0.3700\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1323 - accuracy: 0.5975 - val_loss: 1.8840 - val_accuracy: 0.3700\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1089 - accuracy: 0.6250 - val_loss: 1.7540 - val_accuracy: 0.3800\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.9338 - accuracy: 0.6950 - val_loss: 1.9991 - val_accuracy: 0.3100\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8862 - accuracy: 0.6800 - val_loss: 1.9345 - val_accuracy: 0.3600\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.7020 - accuracy: 0.7750 - val_loss: 2.1298 - val_accuracy: 0.4000\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6836 - accuracy: 0.7575 - val_loss: 2.0176 - val_accuracy: 0.4000\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5897 - accuracy: 0.7750 - val_loss: 2.0941 - val_accuracy: 0.3600\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5053 - accuracy: 0.8100 - val_loss: 2.1642 - val_accuracy: 0.3900\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4105 - accuracy: 0.8575 - val_loss: 2.3045 - val_accuracy: 0.4100\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4326 - accuracy: 0.8400 - val_loss: 2.2422 - val_accuracy: 0.4000\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3971 - accuracy: 0.8875 - val_loss: 2.0800 - val_accuracy: 0.3900\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2786 - accuracy: 0.9075 - val_loss: 2.5666 - val_accuracy: 0.3800\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2853 - accuracy: 0.9050 - val_loss: 2.4354 - val_accuracy: 0.4300\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.1840 - accuracy: 0.9600 - val_loss: 2.6351 - val_accuracy: 0.4200\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.1859 - accuracy: 0.9250 - val_loss: 2.7859 - val_accuracy: 0.4100\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.1306 - accuracy: 0.9600 - val_loss: 2.8298 - val_accuracy: 0.4500\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3093 - accuracy: 0.1000 - val_loss: 2.3105 - val_accuracy: 0.0900\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2853 - accuracy: 0.1175 - val_loss: 2.3019 - val_accuracy: 0.0800\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2262 - accuracy: 0.1625 - val_loss: 2.2260 - val_accuracy: 0.2400\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1295 - accuracy: 0.2625 - val_loss: 2.1037 - val_accuracy: 0.2500\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9872 - accuracy: 0.2625 - val_loss: 1.9777 - val_accuracy: 0.2800\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8202 - accuracy: 0.3175 - val_loss: 1.9747 - val_accuracy: 0.2900\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8113 - accuracy: 0.3225 - val_loss: 1.9006 - val_accuracy: 0.2800\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7358 - accuracy: 0.3325 - val_loss: 1.8650 - val_accuracy: 0.2700\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6848 - accuracy: 0.3925 - val_loss: 1.8558 - val_accuracy: 0.3500\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5194 - accuracy: 0.4325 - val_loss: 1.9904 - val_accuracy: 0.3000\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4785 - accuracy: 0.4725 - val_loss: 1.9013 - val_accuracy: 0.3800\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4105 - accuracy: 0.4850 - val_loss: 1.8358 - val_accuracy: 0.3000\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2742 - accuracy: 0.5450 - val_loss: 1.9081 - val_accuracy: 0.3100\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2352 - accuracy: 0.5775 - val_loss: 1.9543 - val_accuracy: 0.3500\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.0508 - accuracy: 0.6200 - val_loss: 2.1505 - val_accuracy: 0.3200\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.0476 - accuracy: 0.6550 - val_loss: 2.0419 - val_accuracy: 0.3800\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8989 - accuracy: 0.6850 - val_loss: 2.2311 - val_accuracy: 0.3300\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8048 - accuracy: 0.6975 - val_loss: 2.2563 - val_accuracy: 0.3700\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.7751 - accuracy: 0.7225 - val_loss: 2.2718 - val_accuracy: 0.3400\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6703 - accuracy: 0.7525 - val_loss: 2.5778 - val_accuracy: 0.3600\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5991 - accuracy: 0.7925 - val_loss: 2.2140 - val_accuracy: 0.3900\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5247 - accuracy: 0.8075 - val_loss: 2.4987 - val_accuracy: 0.3200\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3880 - accuracy: 0.8750 - val_loss: 2.4525 - val_accuracy: 0.3400\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3215 - accuracy: 0.9075 - val_loss: 3.2136 - val_accuracy: 0.3300\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2820 - accuracy: 0.9100 - val_loss: 2.7920 - val_accuracy: 0.3100\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2638 - accuracy: 0.9125 - val_loss: 2.9659 - val_accuracy: 0.3100\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2972 - accuracy: 0.8950 - val_loss: 3.0435 - val_accuracy: 0.3700\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2688 - accuracy: 0.9200 - val_loss: 2.8412 - val_accuracy: 0.3400\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2130 - accuracy: 0.9400 - val_loss: 3.1923 - val_accuracy: 0.3500\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2582 - accuracy: 0.9300 - val_loss: 3.0759 - val_accuracy: 0.3400\n",
            "Training the attack models...\n",
            "Train on 246 samples\n",
            "Epoch 1/30\n",
            "246/246 [==============================] - 1s 2ms/sample - loss: 0.6693 - accuracy: 0.7154\n",
            "Epoch 2/30\n",
            "246/246 [==============================] - 0s 90us/sample - loss: 0.6256 - accuracy: 0.8211\n",
            "Epoch 3/30\n",
            "246/246 [==============================] - 0s 100us/sample - loss: 0.5667 - accuracy: 0.8211\n",
            "Epoch 4/30\n",
            "246/246 [==============================] - 0s 96us/sample - loss: 0.4973 - accuracy: 0.8211\n",
            "Epoch 5/30\n",
            "246/246 [==============================] - 0s 106us/sample - loss: 0.4353 - accuracy: 0.8211\n",
            "Epoch 6/30\n",
            "246/246 [==============================] - 0s 98us/sample - loss: 0.4220 - accuracy: 0.8171\n",
            "Epoch 7/30\n",
            "246/246 [==============================] - 0s 100us/sample - loss: 0.3723 - accuracy: 0.8252\n",
            "Epoch 8/30\n",
            "246/246 [==============================] - 0s 97us/sample - loss: 0.3905 - accuracy: 0.8252\n",
            "Epoch 9/30\n",
            "246/246 [==============================] - 0s 107us/sample - loss: 0.3912 - accuracy: 0.8211\n",
            "Epoch 10/30\n",
            "246/246 [==============================] - 0s 122us/sample - loss: 0.3876 - accuracy: 0.8252\n",
            "Epoch 11/30\n",
            "246/246 [==============================] - 0s 124us/sample - loss: 0.3872 - accuracy: 0.8333\n",
            "Epoch 12/30\n",
            "246/246 [==============================] - 0s 105us/sample - loss: 0.3778 - accuracy: 0.8333\n",
            "Epoch 13/30\n",
            "246/246 [==============================] - 0s 116us/sample - loss: 0.3873 - accuracy: 0.8537\n",
            "Epoch 14/30\n",
            "246/246 [==============================] - 0s 107us/sample - loss: 0.3692 - accuracy: 0.8455\n",
            "Epoch 15/30\n",
            "246/246 [==============================] - 0s 101us/sample - loss: 0.3743 - accuracy: 0.8496\n",
            "Epoch 16/30\n",
            "246/246 [==============================] - 0s 99us/sample - loss: 0.3719 - accuracy: 0.8415\n",
            "Epoch 17/30\n",
            "246/246 [==============================] - 0s 123us/sample - loss: 0.3660 - accuracy: 0.8415\n",
            "Epoch 18/30\n",
            "246/246 [==============================] - 0s 96us/sample - loss: 0.3787 - accuracy: 0.8415\n",
            "Epoch 19/30\n",
            "246/246 [==============================] - 0s 96us/sample - loss: 0.3610 - accuracy: 0.8496\n",
            "Epoch 20/30\n",
            "246/246 [==============================] - 0s 101us/sample - loss: 0.3458 - accuracy: 0.8618\n",
            "Epoch 21/30\n",
            "246/246 [==============================] - 0s 105us/sample - loss: 0.3715 - accuracy: 0.8415\n",
            "Epoch 22/30\n",
            "246/246 [==============================] - 0s 109us/sample - loss: 0.3666 - accuracy: 0.8455\n",
            "Epoch 23/30\n",
            "246/246 [==============================] - 0s 109us/sample - loss: 0.3691 - accuracy: 0.8496\n",
            "Epoch 24/30\n",
            "246/246 [==============================] - 0s 92us/sample - loss: 0.3675 - accuracy: 0.8496\n",
            "Epoch 25/30\n",
            "246/246 [==============================] - 0s 99us/sample - loss: 0.3654 - accuracy: 0.8496\n",
            "Epoch 26/30\n",
            "246/246 [==============================] - 0s 100us/sample - loss: 0.3740 - accuracy: 0.8496\n",
            "Epoch 27/30\n",
            "246/246 [==============================] - 0s 109us/sample - loss: 0.3760 - accuracy: 0.8496\n",
            "Epoch 28/30\n",
            "246/246 [==============================] - 0s 116us/sample - loss: 0.3619 - accuracy: 0.8537\n",
            "Epoch 29/30\n",
            "246/246 [==============================] - 0s 115us/sample - loss: 0.3658 - accuracy: 0.8455\n",
            "Epoch 30/30\n",
            "246/246 [==============================] - 0s 104us/sample - loss: 0.3490 - accuracy: 0.8537\n",
            "Train on 217 samples\n",
            "Epoch 1/30\n",
            "217/217 [==============================] - 1s 3ms/sample - loss: 0.6749 - accuracy: 0.5161\n",
            "Epoch 2/30\n",
            "217/217 [==============================] - 0s 100us/sample - loss: 0.6463 - accuracy: 0.6129\n",
            "Epoch 3/30\n",
            "217/217 [==============================] - 0s 105us/sample - loss: 0.6153 - accuracy: 0.7465\n",
            "Epoch 4/30\n",
            "217/217 [==============================] - 0s 102us/sample - loss: 0.5881 - accuracy: 0.7742\n",
            "Epoch 5/30\n",
            "217/217 [==============================] - 0s 107us/sample - loss: 0.5552 - accuracy: 0.8018\n",
            "Epoch 6/30\n",
            "217/217 [==============================] - 0s 84us/sample - loss: 0.5133 - accuracy: 0.8065\n",
            "Epoch 7/30\n",
            "217/217 [==============================] - 0s 93us/sample - loss: 0.4820 - accuracy: 0.8065\n",
            "Epoch 8/30\n",
            "217/217 [==============================] - 0s 86us/sample - loss: 0.4506 - accuracy: 0.7972\n",
            "Epoch 9/30\n",
            "217/217 [==============================] - 0s 99us/sample - loss: 0.4493 - accuracy: 0.8018\n",
            "Epoch 10/30\n",
            "217/217 [==============================] - 0s 111us/sample - loss: 0.4401 - accuracy: 0.8157\n",
            "Epoch 11/30\n",
            "217/217 [==============================] - 0s 89us/sample - loss: 0.4247 - accuracy: 0.8018\n",
            "Epoch 12/30\n",
            "217/217 [==============================] - 0s 109us/sample - loss: 0.4227 - accuracy: 0.8249\n",
            "Epoch 13/30\n",
            "217/217 [==============================] - 0s 107us/sample - loss: 0.4218 - accuracy: 0.8111\n",
            "Epoch 14/30\n",
            "217/217 [==============================] - 0s 98us/sample - loss: 0.4102 - accuracy: 0.8065\n",
            "Epoch 15/30\n",
            "217/217 [==============================] - 0s 97us/sample - loss: 0.3973 - accuracy: 0.8111\n",
            "Epoch 16/30\n",
            "217/217 [==============================] - 0s 96us/sample - loss: 0.3994 - accuracy: 0.8065\n",
            "Epoch 17/30\n",
            "217/217 [==============================] - 0s 108us/sample - loss: 0.3970 - accuracy: 0.8203\n",
            "Epoch 18/30\n",
            "217/217 [==============================] - 0s 93us/sample - loss: 0.4151 - accuracy: 0.8203\n",
            "Epoch 19/30\n",
            "217/217 [==============================] - 0s 118us/sample - loss: 0.4185 - accuracy: 0.8157\n",
            "Epoch 20/30\n",
            "217/217 [==============================] - 0s 101us/sample - loss: 0.4010 - accuracy: 0.8295\n",
            "Epoch 21/30\n",
            "217/217 [==============================] - 0s 109us/sample - loss: 0.4004 - accuracy: 0.8111\n",
            "Epoch 22/30\n",
            "217/217 [==============================] - 0s 117us/sample - loss: 0.4002 - accuracy: 0.8157\n",
            "Epoch 23/30\n",
            "217/217 [==============================] - 0s 107us/sample - loss: 0.3948 - accuracy: 0.8157\n",
            "Epoch 24/30\n",
            "217/217 [==============================] - 0s 102us/sample - loss: 0.4055 - accuracy: 0.8249\n",
            "Epoch 25/30\n",
            "217/217 [==============================] - 0s 96us/sample - loss: 0.4084 - accuracy: 0.8203\n",
            "Epoch 26/30\n",
            "217/217 [==============================] - 0s 107us/sample - loss: 0.4022 - accuracy: 0.8249\n",
            "Epoch 27/30\n",
            "217/217 [==============================] - 0s 96us/sample - loss: 0.4076 - accuracy: 0.8203\n",
            "Epoch 28/30\n",
            "217/217 [==============================] - 0s 137us/sample - loss: 0.4042 - accuracy: 0.8157\n",
            "Epoch 29/30\n",
            "217/217 [==============================] - 0s 108us/sample - loss: 0.3821 - accuracy: 0.8203\n",
            "Epoch 30/30\n",
            "217/217 [==============================] - 0s 97us/sample - loss: 0.3917 - accuracy: 0.8111\n",
            "Train on 233 samples\n",
            "Epoch 1/30\n",
            "233/233 [==============================] - 1s 4ms/sample - loss: 0.6878 - accuracy: 0.5923\n",
            "Epoch 2/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.6466 - accuracy: 0.7811\n",
            "Epoch 3/30\n",
            "233/233 [==============================] - 0s 110us/sample - loss: 0.5962 - accuracy: 0.8069\n",
            "Epoch 4/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.5377 - accuracy: 0.8283\n",
            "Epoch 5/30\n",
            "233/233 [==============================] - 0s 105us/sample - loss: 0.4685 - accuracy: 0.8326\n",
            "Epoch 6/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.4253 - accuracy: 0.8283\n",
            "Epoch 7/30\n",
            "233/233 [==============================] - 0s 100us/sample - loss: 0.3873 - accuracy: 0.8412\n",
            "Epoch 8/30\n",
            "233/233 [==============================] - 0s 95us/sample - loss: 0.3567 - accuracy: 0.8455\n",
            "Epoch 9/30\n",
            "233/233 [==============================] - 0s 105us/sample - loss: 0.3751 - accuracy: 0.8455\n",
            "Epoch 10/30\n",
            "233/233 [==============================] - 0s 91us/sample - loss: 0.3851 - accuracy: 0.8369\n",
            "Epoch 11/30\n",
            "233/233 [==============================] - 0s 113us/sample - loss: 0.3696 - accuracy: 0.8541\n",
            "Epoch 12/30\n",
            "233/233 [==============================] - 0s 104us/sample - loss: 0.3615 - accuracy: 0.8498\n",
            "Epoch 13/30\n",
            "233/233 [==============================] - 0s 95us/sample - loss: 0.3512 - accuracy: 0.8541\n",
            "Epoch 14/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.3571 - accuracy: 0.8541\n",
            "Epoch 15/30\n",
            "233/233 [==============================] - 0s 106us/sample - loss: 0.3552 - accuracy: 0.8584\n",
            "Epoch 16/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.3632 - accuracy: 0.8412\n",
            "Epoch 17/30\n",
            "233/233 [==============================] - 0s 97us/sample - loss: 0.3618 - accuracy: 0.8412\n",
            "Epoch 18/30\n",
            "233/233 [==============================] - 0s 110us/sample - loss: 0.3485 - accuracy: 0.8584\n",
            "Epoch 19/30\n",
            "233/233 [==============================] - 0s 104us/sample - loss: 0.3563 - accuracy: 0.8498\n",
            "Epoch 20/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.3491 - accuracy: 0.8498\n",
            "Epoch 21/30\n",
            "233/233 [==============================] - 0s 105us/sample - loss: 0.3422 - accuracy: 0.8498\n",
            "Epoch 22/30\n",
            "233/233 [==============================] - 0s 102us/sample - loss: 0.3485 - accuracy: 0.8584\n",
            "Epoch 23/30\n",
            "233/233 [==============================] - 0s 94us/sample - loss: 0.3435 - accuracy: 0.8627\n",
            "Epoch 24/30\n",
            "233/233 [==============================] - 0s 106us/sample - loss: 0.3305 - accuracy: 0.8712\n",
            "Epoch 25/30\n",
            "233/233 [==============================] - 0s 108us/sample - loss: 0.3606 - accuracy: 0.8541\n",
            "Epoch 26/30\n",
            "233/233 [==============================] - 0s 99us/sample - loss: 0.3419 - accuracy: 0.8670\n",
            "Epoch 27/30\n",
            "233/233 [==============================] - 0s 105us/sample - loss: 0.3428 - accuracy: 0.8670\n",
            "Epoch 28/30\n",
            "233/233 [==============================] - 0s 95us/sample - loss: 0.3475 - accuracy: 0.8584\n",
            "Epoch 29/30\n",
            "233/233 [==============================] - 0s 108us/sample - loss: 0.3441 - accuracy: 0.8584\n",
            "Epoch 30/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.3363 - accuracy: 0.8712\n",
            "Train on 249 samples\n",
            "Epoch 1/30\n",
            "249/249 [==============================] - 1s 2ms/sample - loss: 0.6929 - accuracy: 0.5703\n",
            "Epoch 2/30\n",
            "249/249 [==============================] - 0s 101us/sample - loss: 0.6442 - accuracy: 0.8755\n",
            "Epoch 3/30\n",
            "249/249 [==============================] - 0s 98us/sample - loss: 0.5858 - accuracy: 0.9036\n",
            "Epoch 4/30\n",
            "249/249 [==============================] - 0s 88us/sample - loss: 0.4883 - accuracy: 0.9036\n",
            "Epoch 5/30\n",
            "249/249 [==============================] - 0s 99us/sample - loss: 0.3665 - accuracy: 0.9197\n",
            "Epoch 6/30\n",
            "249/249 [==============================] - 0s 95us/sample - loss: 0.2701 - accuracy: 0.9237\n",
            "Epoch 7/30\n",
            "249/249 [==============================] - 0s 96us/sample - loss: 0.2081 - accuracy: 0.9277\n",
            "Epoch 8/30\n",
            "249/249 [==============================] - 0s 90us/sample - loss: 0.1916 - accuracy: 0.9398\n",
            "Epoch 9/30\n",
            "249/249 [==============================] - 0s 103us/sample - loss: 0.1834 - accuracy: 0.9357\n",
            "Epoch 10/30\n",
            "249/249 [==============================] - 0s 104us/sample - loss: 0.1858 - accuracy: 0.9438\n",
            "Epoch 11/30\n",
            "249/249 [==============================] - 0s 111us/sample - loss: 0.1745 - accuracy: 0.9478\n",
            "Epoch 12/30\n",
            "249/249 [==============================] - 0s 89us/sample - loss: 0.1603 - accuracy: 0.9558\n",
            "Epoch 13/30\n",
            "249/249 [==============================] - 0s 99us/sample - loss: 0.1787 - accuracy: 0.9398\n",
            "Epoch 14/30\n",
            "249/249 [==============================] - 0s 102us/sample - loss: 0.1691 - accuracy: 0.9398\n",
            "Epoch 15/30\n",
            "249/249 [==============================] - 0s 84us/sample - loss: 0.1601 - accuracy: 0.9438\n",
            "Epoch 16/30\n",
            "249/249 [==============================] - 0s 100us/sample - loss: 0.1652 - accuracy: 0.9518\n",
            "Epoch 17/30\n",
            "249/249 [==============================] - 0s 106us/sample - loss: 0.1621 - accuracy: 0.9398\n",
            "Epoch 18/30\n",
            "249/249 [==============================] - 0s 111us/sample - loss: 0.1546 - accuracy: 0.9478\n",
            "Epoch 19/30\n",
            "249/249 [==============================] - 0s 122us/sample - loss: 0.1421 - accuracy: 0.9518\n",
            "Epoch 20/30\n",
            "249/249 [==============================] - 0s 125us/sample - loss: 0.1633 - accuracy: 0.9438\n",
            "Epoch 21/30\n",
            "249/249 [==============================] - 0s 106us/sample - loss: 0.1487 - accuracy: 0.9518\n",
            "Epoch 22/30\n",
            "249/249 [==============================] - 0s 103us/sample - loss: 0.1473 - accuracy: 0.9598\n",
            "Epoch 23/30\n",
            "249/249 [==============================] - 0s 98us/sample - loss: 0.1540 - accuracy: 0.9518\n",
            "Epoch 24/30\n",
            "249/249 [==============================] - 0s 101us/sample - loss: 0.1546 - accuracy: 0.9398\n",
            "Epoch 25/30\n",
            "249/249 [==============================] - 0s 115us/sample - loss: 0.1536 - accuracy: 0.9558\n",
            "Epoch 26/30\n",
            "249/249 [==============================] - 0s 93us/sample - loss: 0.1530 - accuracy: 0.9478\n",
            "Epoch 27/30\n",
            "249/249 [==============================] - 0s 100us/sample - loss: 0.1649 - accuracy: 0.9438\n",
            "Epoch 28/30\n",
            "249/249 [==============================] - 0s 100us/sample - loss: 0.1491 - accuracy: 0.9478\n",
            "Epoch 29/30\n",
            "249/249 [==============================] - 0s 101us/sample - loss: 0.1532 - accuracy: 0.9598\n",
            "Epoch 30/30\n",
            "249/249 [==============================] - 0s 97us/sample - loss: 0.1535 - accuracy: 0.9438\n",
            "Train on 194 samples\n",
            "Epoch 1/30\n",
            "194/194 [==============================] - 1s 3ms/sample - loss: 0.6846 - accuracy: 0.6237\n",
            "Epoch 2/30\n",
            "194/194 [==============================] - 0s 105us/sample - loss: 0.6420 - accuracy: 0.9330\n",
            "Epoch 3/30\n",
            "194/194 [==============================] - 0s 104us/sample - loss: 0.5852 - accuracy: 0.9536\n",
            "Epoch 4/30\n",
            "194/194 [==============================] - 0s 103us/sample - loss: 0.5194 - accuracy: 0.9639\n",
            "Epoch 5/30\n",
            "194/194 [==============================] - 0s 112us/sample - loss: 0.4163 - accuracy: 0.9485\n",
            "Epoch 6/30\n",
            "194/194 [==============================] - 0s 108us/sample - loss: 0.2969 - accuracy: 0.9588\n",
            "Epoch 7/30\n",
            "194/194 [==============================] - 0s 99us/sample - loss: 0.2036 - accuracy: 0.9691\n",
            "Epoch 8/30\n",
            "194/194 [==============================] - 0s 122us/sample - loss: 0.1307 - accuracy: 0.9742\n",
            "Epoch 9/30\n",
            "194/194 [==============================] - 0s 126us/sample - loss: 0.1047 - accuracy: 0.9639\n",
            "Epoch 10/30\n",
            "194/194 [==============================] - 0s 115us/sample - loss: 0.0857 - accuracy: 0.9742\n",
            "Epoch 11/30\n",
            "194/194 [==============================] - 0s 119us/sample - loss: 0.0895 - accuracy: 0.9691\n",
            "Epoch 12/30\n",
            "194/194 [==============================] - 0s 109us/sample - loss: 0.0692 - accuracy: 0.9691\n",
            "Epoch 13/30\n",
            "194/194 [==============================] - 0s 142us/sample - loss: 0.0736 - accuracy: 0.9742\n",
            "Epoch 14/30\n",
            "194/194 [==============================] - 0s 125us/sample - loss: 0.0753 - accuracy: 0.9691\n",
            "Epoch 15/30\n",
            "194/194 [==============================] - 0s 113us/sample - loss: 0.0625 - accuracy: 0.9742\n",
            "Epoch 16/30\n",
            "194/194 [==============================] - 0s 125us/sample - loss: 0.0721 - accuracy: 0.9794\n",
            "Epoch 17/30\n",
            "194/194 [==============================] - 0s 132us/sample - loss: 0.0753 - accuracy: 0.9845\n",
            "Epoch 18/30\n",
            "194/194 [==============================] - 0s 121us/sample - loss: 0.0696 - accuracy: 0.9742\n",
            "Epoch 19/30\n",
            "194/194 [==============================] - 0s 118us/sample - loss: 0.0608 - accuracy: 0.9742\n",
            "Epoch 20/30\n",
            "194/194 [==============================] - 0s 116us/sample - loss: 0.0695 - accuracy: 0.9845\n",
            "Epoch 21/30\n",
            "194/194 [==============================] - 0s 124us/sample - loss: 0.0736 - accuracy: 0.9742\n",
            "Epoch 22/30\n",
            "194/194 [==============================] - 0s 131us/sample - loss: 0.0539 - accuracy: 0.9845\n",
            "Epoch 23/30\n",
            "194/194 [==============================] - 0s 128us/sample - loss: 0.0645 - accuracy: 0.9845\n",
            "Epoch 24/30\n",
            "194/194 [==============================] - 0s 113us/sample - loss: 0.0486 - accuracy: 0.9845\n",
            "Epoch 25/30\n",
            "194/194 [==============================] - 0s 122us/sample - loss: 0.0599 - accuracy: 0.9845\n",
            "Epoch 26/30\n",
            "194/194 [==============================] - 0s 107us/sample - loss: 0.0574 - accuracy: 0.9845\n",
            "Epoch 27/30\n",
            "194/194 [==============================] - 0s 153us/sample - loss: 0.0607 - accuracy: 0.9794\n",
            "Epoch 28/30\n",
            "194/194 [==============================] - 0s 125us/sample - loss: 0.0682 - accuracy: 0.9691\n",
            "Epoch 29/30\n",
            "194/194 [==============================] - 0s 108us/sample - loss: 0.0658 - accuracy: 0.9794\n",
            "Epoch 30/30\n",
            "194/194 [==============================] - 0s 106us/sample - loss: 0.0633 - accuracy: 0.9742\n",
            "Train on 207 samples\n",
            "Epoch 1/30\n",
            "207/207 [==============================] - 1s 3ms/sample - loss: 0.6712 - accuracy: 0.6232\n",
            "Epoch 2/30\n",
            "207/207 [==============================] - 0s 103us/sample - loss: 0.6316 - accuracy: 0.8261\n",
            "Epoch 3/30\n",
            "207/207 [==============================] - 0s 94us/sample - loss: 0.5751 - accuracy: 0.8841\n",
            "Epoch 4/30\n",
            "207/207 [==============================] - 0s 107us/sample - loss: 0.4988 - accuracy: 0.9034\n",
            "Epoch 5/30\n",
            "207/207 [==============================] - 0s 116us/sample - loss: 0.4189 - accuracy: 0.9034\n",
            "Epoch 6/30\n",
            "207/207 [==============================] - 0s 105us/sample - loss: 0.3312 - accuracy: 0.9130\n",
            "Epoch 7/30\n",
            "207/207 [==============================] - 0s 102us/sample - loss: 0.2802 - accuracy: 0.9034\n",
            "Epoch 8/30\n",
            "207/207 [==============================] - 0s 97us/sample - loss: 0.2504 - accuracy: 0.9082\n",
            "Epoch 9/30\n",
            "207/207 [==============================] - 0s 105us/sample - loss: 0.2077 - accuracy: 0.9324\n",
            "Epoch 10/30\n",
            "207/207 [==============================] - 0s 103us/sample - loss: 0.2253 - accuracy: 0.9179\n",
            "Epoch 11/30\n",
            "207/207 [==============================] - 0s 129us/sample - loss: 0.2284 - accuracy: 0.9179\n",
            "Epoch 12/30\n",
            "207/207 [==============================] - 0s 121us/sample - loss: 0.2134 - accuracy: 0.9324\n",
            "Epoch 13/30\n",
            "207/207 [==============================] - 0s 117us/sample - loss: 0.2019 - accuracy: 0.9324\n",
            "Epoch 14/30\n",
            "207/207 [==============================] - 0s 109us/sample - loss: 0.1856 - accuracy: 0.9324\n",
            "Epoch 15/30\n",
            "207/207 [==============================] - 0s 122us/sample - loss: 0.1954 - accuracy: 0.9179\n",
            "Epoch 16/30\n",
            "207/207 [==============================] - 0s 111us/sample - loss: 0.2006 - accuracy: 0.9275\n",
            "Epoch 17/30\n",
            "207/207 [==============================] - 0s 107us/sample - loss: 0.1884 - accuracy: 0.9372\n",
            "Epoch 18/30\n",
            "207/207 [==============================] - 0s 96us/sample - loss: 0.1636 - accuracy: 0.9372\n",
            "Epoch 19/30\n",
            "207/207 [==============================] - 0s 112us/sample - loss: 0.1932 - accuracy: 0.9227\n",
            "Epoch 20/30\n",
            "207/207 [==============================] - 0s 126us/sample - loss: 0.1724 - accuracy: 0.9420\n",
            "Epoch 21/30\n",
            "207/207 [==============================] - 0s 111us/sample - loss: 0.1785 - accuracy: 0.9227\n",
            "Epoch 22/30\n",
            "207/207 [==============================] - 0s 110us/sample - loss: 0.1878 - accuracy: 0.9372\n",
            "Epoch 23/30\n",
            "207/207 [==============================] - 0s 121us/sample - loss: 0.1852 - accuracy: 0.9420\n",
            "Epoch 24/30\n",
            "207/207 [==============================] - 0s 100us/sample - loss: 0.1671 - accuracy: 0.9372\n",
            "Epoch 25/30\n",
            "207/207 [==============================] - 0s 128us/sample - loss: 0.1952 - accuracy: 0.9324\n",
            "Epoch 26/30\n",
            "207/207 [==============================] - 0s 104us/sample - loss: 0.1822 - accuracy: 0.9420\n",
            "Epoch 27/30\n",
            "207/207 [==============================] - 0s 104us/sample - loss: 0.1746 - accuracy: 0.9420\n",
            "Epoch 28/30\n",
            "207/207 [==============================] - 0s 117us/sample - loss: 0.1725 - accuracy: 0.9420\n",
            "Epoch 29/30\n",
            "207/207 [==============================] - 0s 101us/sample - loss: 0.1845 - accuracy: 0.9420\n",
            "Epoch 30/30\n",
            "207/207 [==============================] - 0s 107us/sample - loss: 0.1631 - accuracy: 0.9372\n",
            "Train on 260 samples\n",
            "Epoch 1/30\n",
            "260/260 [==============================] - 1s 2ms/sample - loss: 0.6862 - accuracy: 0.6077\n",
            "Epoch 2/30\n",
            "260/260 [==============================] - 0s 101us/sample - loss: 0.6535 - accuracy: 0.7346\n",
            "Epoch 3/30\n",
            "260/260 [==============================] - 0s 105us/sample - loss: 0.6234 - accuracy: 0.7462\n",
            "Epoch 4/30\n",
            "260/260 [==============================] - 0s 99us/sample - loss: 0.5712 - accuracy: 0.7923\n",
            "Epoch 5/30\n",
            "260/260 [==============================] - 0s 103us/sample - loss: 0.5289 - accuracy: 0.7962\n",
            "Epoch 6/30\n",
            "260/260 [==============================] - 0s 96us/sample - loss: 0.4842 - accuracy: 0.7923\n",
            "Epoch 7/30\n",
            "260/260 [==============================] - 0s 92us/sample - loss: 0.4578 - accuracy: 0.7885\n",
            "Epoch 8/30\n",
            "260/260 [==============================] - 0s 105us/sample - loss: 0.4470 - accuracy: 0.8077\n",
            "Epoch 9/30\n",
            "260/260 [==============================] - 0s 112us/sample - loss: 0.4581 - accuracy: 0.7846\n",
            "Epoch 10/30\n",
            "260/260 [==============================] - 0s 106us/sample - loss: 0.4312 - accuracy: 0.8038\n",
            "Epoch 11/30\n",
            "260/260 [==============================] - 0s 103us/sample - loss: 0.4449 - accuracy: 0.8038\n",
            "Epoch 12/30\n",
            "260/260 [==============================] - 0s 105us/sample - loss: 0.4318 - accuracy: 0.8192\n",
            "Epoch 13/30\n",
            "260/260 [==============================] - 0s 93us/sample - loss: 0.4298 - accuracy: 0.8115\n",
            "Epoch 14/30\n",
            "260/260 [==============================] - 0s 93us/sample - loss: 0.4307 - accuracy: 0.8192\n",
            "Epoch 15/30\n",
            "260/260 [==============================] - 0s 116us/sample - loss: 0.4367 - accuracy: 0.8154\n",
            "Epoch 16/30\n",
            "260/260 [==============================] - 0s 160us/sample - loss: 0.4260 - accuracy: 0.8115\n",
            "Epoch 17/30\n",
            "260/260 [==============================] - 0s 119us/sample - loss: 0.4278 - accuracy: 0.8192\n",
            "Epoch 18/30\n",
            "260/260 [==============================] - 0s 98us/sample - loss: 0.4204 - accuracy: 0.8077\n",
            "Epoch 19/30\n",
            "260/260 [==============================] - 0s 99us/sample - loss: 0.4044 - accuracy: 0.8077\n",
            "Epoch 20/30\n",
            "260/260 [==============================] - 0s 95us/sample - loss: 0.4280 - accuracy: 0.8038\n",
            "Epoch 21/30\n",
            "260/260 [==============================] - 0s 106us/sample - loss: 0.4122 - accuracy: 0.8192\n",
            "Epoch 22/30\n",
            "260/260 [==============================] - 0s 141us/sample - loss: 0.4201 - accuracy: 0.8154\n",
            "Epoch 23/30\n",
            "260/260 [==============================] - 0s 182us/sample - loss: 0.4292 - accuracy: 0.8115\n",
            "Epoch 24/30\n",
            "260/260 [==============================] - 0s 116us/sample - loss: 0.4248 - accuracy: 0.8115\n",
            "Epoch 25/30\n",
            "260/260 [==============================] - 0s 113us/sample - loss: 0.4080 - accuracy: 0.8154\n",
            "Epoch 26/30\n",
            "260/260 [==============================] - 0s 104us/sample - loss: 0.4324 - accuracy: 0.8038\n",
            "Epoch 27/30\n",
            "260/260 [==============================] - 0s 96us/sample - loss: 0.4004 - accuracy: 0.8115\n",
            "Epoch 28/30\n",
            "260/260 [==============================] - 0s 95us/sample - loss: 0.4129 - accuracy: 0.8192\n",
            "Epoch 29/30\n",
            "260/260 [==============================] - 0s 114us/sample - loss: 0.4052 - accuracy: 0.8115\n",
            "Epoch 30/30\n",
            "260/260 [==============================] - 0s 107us/sample - loss: 0.4181 - accuracy: 0.8038\n",
            "Train on 243 samples\n",
            "Epoch 1/30\n",
            "243/243 [==============================] - 1s 2ms/sample - loss: 0.6705 - accuracy: 0.6914\n",
            "Epoch 2/30\n",
            "243/243 [==============================] - 0s 105us/sample - loss: 0.6262 - accuracy: 0.8066\n",
            "Epoch 3/30\n",
            "243/243 [==============================] - 0s 98us/sample - loss: 0.5754 - accuracy: 0.8148\n",
            "Epoch 4/30\n",
            "243/243 [==============================] - 0s 106us/sample - loss: 0.4921 - accuracy: 0.8395\n",
            "Epoch 5/30\n",
            "243/243 [==============================] - 0s 100us/sample - loss: 0.4217 - accuracy: 0.8436\n",
            "Epoch 6/30\n",
            "243/243 [==============================] - 0s 107us/sample - loss: 0.3794 - accuracy: 0.8560\n",
            "Epoch 7/30\n",
            "243/243 [==============================] - 0s 92us/sample - loss: 0.3424 - accuracy: 0.8683\n",
            "Epoch 8/30\n",
            "243/243 [==============================] - 0s 99us/sample - loss: 0.3220 - accuracy: 0.8724\n",
            "Epoch 9/30\n",
            "243/243 [==============================] - 0s 102us/sample - loss: 0.3168 - accuracy: 0.8889\n",
            "Epoch 10/30\n",
            "243/243 [==============================] - 0s 118us/sample - loss: 0.3266 - accuracy: 0.8807\n",
            "Epoch 11/30\n",
            "243/243 [==============================] - 0s 107us/sample - loss: 0.2989 - accuracy: 0.8889\n",
            "Epoch 12/30\n",
            "243/243 [==============================] - 0s 104us/sample - loss: 0.2996 - accuracy: 0.8848\n",
            "Epoch 13/30\n",
            "243/243 [==============================] - 0s 99us/sample - loss: 0.2851 - accuracy: 0.8889\n",
            "Epoch 14/30\n",
            "243/243 [==============================] - 0s 88us/sample - loss: 0.2764 - accuracy: 0.8930\n",
            "Epoch 15/30\n",
            "243/243 [==============================] - 0s 104us/sample - loss: 0.2982 - accuracy: 0.8930\n",
            "Epoch 16/30\n",
            "243/243 [==============================] - 0s 104us/sample - loss: 0.2851 - accuracy: 0.8971\n",
            "Epoch 17/30\n",
            "243/243 [==============================] - 0s 112us/sample - loss: 0.3049 - accuracy: 0.8848\n",
            "Epoch 18/30\n",
            "243/243 [==============================] - 0s 110us/sample - loss: 0.2891 - accuracy: 0.8971\n",
            "Epoch 19/30\n",
            "243/243 [==============================] - 0s 111us/sample - loss: 0.2912 - accuracy: 0.8889\n",
            "Epoch 20/30\n",
            "243/243 [==============================] - 0s 102us/sample - loss: 0.2718 - accuracy: 0.9012\n",
            "Epoch 21/30\n",
            "243/243 [==============================] - 0s 111us/sample - loss: 0.3011 - accuracy: 0.8889\n",
            "Epoch 22/30\n",
            "243/243 [==============================] - 0s 124us/sample - loss: 0.2812 - accuracy: 0.8889\n",
            "Epoch 23/30\n",
            "243/243 [==============================] - 0s 111us/sample - loss: 0.2748 - accuracy: 0.8889\n",
            "Epoch 24/30\n",
            "243/243 [==============================] - 0s 114us/sample - loss: 0.2710 - accuracy: 0.8930\n",
            "Epoch 25/30\n",
            "243/243 [==============================] - 0s 117us/sample - loss: 0.3047 - accuracy: 0.8971\n",
            "Epoch 26/30\n",
            "243/243 [==============================] - 0s 120us/sample - loss: 0.2933 - accuracy: 0.8971\n",
            "Epoch 27/30\n",
            "243/243 [==============================] - 0s 111us/sample - loss: 0.2846 - accuracy: 0.9012\n",
            "Epoch 28/30\n",
            "243/243 [==============================] - 0s 106us/sample - loss: 0.2754 - accuracy: 0.8971\n",
            "Epoch 29/30\n",
            "243/243 [==============================] - 0s 100us/sample - loss: 0.2852 - accuracy: 0.8971\n",
            "Epoch 30/30\n",
            "243/243 [==============================] - 0s 104us/sample - loss: 0.2653 - accuracy: 0.9053\n",
            "Train on 271 samples\n",
            "Epoch 1/30\n",
            "271/271 [==============================] - 1s 2ms/sample - loss: 0.6774 - accuracy: 0.5424\n",
            "Epoch 2/30\n",
            "271/271 [==============================] - 0s 97us/sample - loss: 0.6423 - accuracy: 0.6827\n",
            "Epoch 3/30\n",
            "271/271 [==============================] - 0s 98us/sample - loss: 0.6027 - accuracy: 0.7528\n",
            "Epoch 4/30\n",
            "271/271 [==============================] - 0s 101us/sample - loss: 0.5562 - accuracy: 0.7970\n",
            "Epoch 5/30\n",
            "271/271 [==============================] - 0s 94us/sample - loss: 0.5099 - accuracy: 0.7970\n",
            "Epoch 6/30\n",
            "271/271 [==============================] - 0s 97us/sample - loss: 0.4656 - accuracy: 0.8081\n",
            "Epoch 7/30\n",
            "271/271 [==============================] - 0s 104us/sample - loss: 0.4486 - accuracy: 0.8118\n",
            "Epoch 8/30\n",
            "271/271 [==============================] - 0s 92us/sample - loss: 0.4167 - accuracy: 0.8118\n",
            "Epoch 9/30\n",
            "271/271 [==============================] - 0s 107us/sample - loss: 0.4109 - accuracy: 0.8192\n",
            "Epoch 10/30\n",
            "271/271 [==============================] - 0s 93us/sample - loss: 0.4098 - accuracy: 0.8339\n",
            "Epoch 11/30\n",
            "271/271 [==============================] - 0s 92us/sample - loss: 0.4035 - accuracy: 0.8229\n",
            "Epoch 12/30\n",
            "271/271 [==============================] - 0s 94us/sample - loss: 0.3932 - accuracy: 0.8303\n",
            "Epoch 13/30\n",
            "271/271 [==============================] - 0s 108us/sample - loss: 0.4041 - accuracy: 0.8339\n",
            "Epoch 14/30\n",
            "271/271 [==============================] - 0s 99us/sample - loss: 0.3819 - accuracy: 0.8339\n",
            "Epoch 15/30\n",
            "271/271 [==============================] - 0s 95us/sample - loss: 0.3962 - accuracy: 0.8303\n",
            "Epoch 16/30\n",
            "271/271 [==============================] - 0s 116us/sample - loss: 0.3908 - accuracy: 0.8339\n",
            "Epoch 17/30\n",
            "271/271 [==============================] - 0s 103us/sample - loss: 0.3889 - accuracy: 0.8413\n",
            "Epoch 18/30\n",
            "271/271 [==============================] - 0s 97us/sample - loss: 0.3799 - accuracy: 0.8376\n",
            "Epoch 19/30\n",
            "271/271 [==============================] - 0s 103us/sample - loss: 0.3929 - accuracy: 0.8339\n",
            "Epoch 20/30\n",
            "271/271 [==============================] - 0s 100us/sample - loss: 0.3917 - accuracy: 0.8413\n",
            "Epoch 21/30\n",
            "271/271 [==============================] - 0s 83us/sample - loss: 0.3964 - accuracy: 0.8413\n",
            "Epoch 22/30\n",
            "271/271 [==============================] - 0s 94us/sample - loss: 0.3819 - accuracy: 0.8376\n",
            "Epoch 23/30\n",
            "271/271 [==============================] - 0s 106us/sample - loss: 0.3905 - accuracy: 0.8339\n",
            "Epoch 24/30\n",
            "271/271 [==============================] - 0s 108us/sample - loss: 0.3709 - accuracy: 0.8487\n",
            "Epoch 25/30\n",
            "271/271 [==============================] - 0s 94us/sample - loss: 0.3887 - accuracy: 0.8339\n",
            "Epoch 26/30\n",
            "271/271 [==============================] - 0s 91us/sample - loss: 0.3752 - accuracy: 0.8450\n",
            "Epoch 27/30\n",
            "271/271 [==============================] - 0s 98us/sample - loss: 0.3794 - accuracy: 0.8413\n",
            "Epoch 28/30\n",
            "271/271 [==============================] - 0s 103us/sample - loss: 0.3794 - accuracy: 0.8376\n",
            "Epoch 29/30\n",
            "271/271 [==============================] - 0s 102us/sample - loss: 0.3656 - accuracy: 0.8450\n",
            "Epoch 30/30\n",
            "271/271 [==============================] - 0s 94us/sample - loss: 0.3701 - accuracy: 0.8413\n",
            "Train on 280 samples\n",
            "Epoch 1/30\n",
            "280/280 [==============================] - 1s 2ms/sample - loss: 0.6781 - accuracy: 0.6500\n",
            "Epoch 2/30\n",
            "280/280 [==============================] - 0s 108us/sample - loss: 0.6390 - accuracy: 0.7393\n",
            "Epoch 3/30\n",
            "280/280 [==============================] - 0s 95us/sample - loss: 0.6026 - accuracy: 0.7464\n",
            "Epoch 4/30\n",
            "280/280 [==============================] - 0s 102us/sample - loss: 0.5531 - accuracy: 0.7929\n",
            "Epoch 5/30\n",
            "280/280 [==============================] - 0s 104us/sample - loss: 0.4862 - accuracy: 0.8107\n",
            "Epoch 6/30\n",
            "280/280 [==============================] - 0s 105us/sample - loss: 0.4470 - accuracy: 0.8214\n",
            "Epoch 7/30\n",
            "280/280 [==============================] - 0s 105us/sample - loss: 0.4082 - accuracy: 0.8357\n",
            "Epoch 8/30\n",
            "280/280 [==============================] - 0s 115us/sample - loss: 0.3792 - accuracy: 0.8393\n",
            "Epoch 9/30\n",
            "280/280 [==============================] - 0s 118us/sample - loss: 0.3737 - accuracy: 0.8464\n",
            "Epoch 10/30\n",
            "280/280 [==============================] - 0s 120us/sample - loss: 0.3813 - accuracy: 0.8429\n",
            "Epoch 11/30\n",
            "280/280 [==============================] - 0s 99us/sample - loss: 0.3739 - accuracy: 0.8464\n",
            "Epoch 12/30\n",
            "280/280 [==============================] - 0s 104us/sample - loss: 0.3822 - accuracy: 0.8607\n",
            "Epoch 13/30\n",
            "280/280 [==============================] - 0s 102us/sample - loss: 0.3681 - accuracy: 0.8607\n",
            "Epoch 14/30\n",
            "280/280 [==============================] - 0s 91us/sample - loss: 0.3655 - accuracy: 0.8464\n",
            "Epoch 15/30\n",
            "280/280 [==============================] - 0s 90us/sample - loss: 0.3668 - accuracy: 0.8607\n",
            "Epoch 16/30\n",
            "280/280 [==============================] - 0s 90us/sample - loss: 0.3611 - accuracy: 0.8536\n",
            "Epoch 17/30\n",
            "280/280 [==============================] - 0s 98us/sample - loss: 0.3720 - accuracy: 0.8536\n",
            "Epoch 18/30\n",
            "280/280 [==============================] - 0s 106us/sample - loss: 0.3564 - accuracy: 0.8643\n",
            "Epoch 19/30\n",
            "280/280 [==============================] - 0s 101us/sample - loss: 0.3652 - accuracy: 0.8536\n",
            "Epoch 20/30\n",
            "280/280 [==============================] - 0s 99us/sample - loss: 0.3278 - accuracy: 0.8643\n",
            "Epoch 21/30\n",
            "280/280 [==============================] - 0s 121us/sample - loss: 0.3429 - accuracy: 0.8679\n",
            "Epoch 22/30\n",
            "280/280 [==============================] - 0s 97us/sample - loss: 0.3434 - accuracy: 0.8607\n",
            "Epoch 23/30\n",
            "280/280 [==============================] - 0s 86us/sample - loss: 0.3380 - accuracy: 0.8750\n",
            "Epoch 24/30\n",
            "280/280 [==============================] - 0s 91us/sample - loss: 0.3309 - accuracy: 0.8714\n",
            "Epoch 25/30\n",
            "280/280 [==============================] - 0s 99us/sample - loss: 0.3437 - accuracy: 0.8750\n",
            "Epoch 26/30\n",
            "280/280 [==============================] - 0s 94us/sample - loss: 0.3472 - accuracy: 0.8607\n",
            "Epoch 27/30\n",
            "280/280 [==============================] - 0s 96us/sample - loss: 0.3485 - accuracy: 0.8571\n",
            "Epoch 28/30\n",
            "280/280 [==============================] - 0s 91us/sample - loss: 0.3505 - accuracy: 0.8571\n",
            "Epoch 29/30\n",
            "280/280 [==============================] - 0s 120us/sample - loss: 0.3519 - accuracy: 0.8607\n",
            "Epoch 30/30\n",
            "280/280 [==============================] - 0s 94us/sample - loss: 0.3253 - accuracy: 0.8750\n",
            "Attack accuracy: 0.77625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcPWZJ9Thp4o",
        "colab_type": "code",
        "outputId": "9949a739-a343-4ade-e7c0-f8ba72a29e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the target model for 30 epochs\n",
        "# regularization increasing the percentage of frozen units in the dropout layers of the architecture of the target model\n",
        "%tensorflow_version 2.x\n",
        "\"\"\"\n",
        "Example membership inference attack against a deep net classifier on the CIFAR10 dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "WIDTH = 32\n",
        "HEIGHT = 32\n",
        "CHANNELS = 3\n",
        "SHADOW_DATASET_SIZE = 400\n",
        "ATTACK_TEST_DATASET_SIZE = 400\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer(\n",
        "    \"target_epochs\", 30, \"Number of epochs to train target and shadow models.\"\n",
        ")\n",
        "flags.DEFINE_integer(\"attack_epochs\", 30, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_integer(\"num_shadows\", 3, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    X_train = X_train.astype(\"float32\")\n",
        "    X_test = X_test.astype(\"float32\")\n",
        "    y_train = y_train.astype(\"float32\")\n",
        "    y_test = y_test.astype(\"float32\")\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    return (X_train[0:5000], y_train[0:5000]), (X_test[0:1000], y_test[0:1000])\n",
        "\n",
        "\n",
        "def target_model_fn():\n",
        "    # I have increased the percentange of dropout units in the dropout layers of the target model architcture\n",
        "    \"\"\"The architecture of the target (victim) model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(\n",
        "        layers.Conv2D(\n",
        "            32,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.8))\n",
        "\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def attack_model_fn():\n",
        "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
        "    Following the original paper, this attack model is specific to the class of the input.\n",
        "    AttachModelBundle creates multiple instances of this model for each class.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
        "\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def demo(argv):\n",
        "    del argv  # Unused.\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = get_data()\n",
        "\n",
        "    # Train the target model.\n",
        "    print(\"Training the target model...\")\n",
        "    target_model = target_model_fn()\n",
        "    target_model.fit(\n",
        "        X_train, y_train, epochs=FLAGS.target_epochs, validation_split=0.1, verbose=True\n",
        "    )\n",
        "\n",
        "    # Train the shadow models.\n",
        "    smb = ShadowModelBundle(\n",
        "        target_model_fn,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=FLAGS.num_shadows,\n",
        "    )\n",
        "\n",
        "    # We assume that attacker's data were not seen in target's training.\n",
        "    attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
        "        X_test, y_test, test_size=0.1\n",
        "    )\n",
        "    print(attacker_X_train.shape, attacker_X_test.shape)\n",
        "\n",
        "    print(\"Training the shadow models...\")\n",
        "    X_shadow, y_shadow = smb.fit_transform(\n",
        "        attacker_X_train,\n",
        "        attacker_y_train,\n",
        "        fit_kwargs=dict(\n",
        "            epochs=FLAGS.target_epochs,\n",
        "            verbose=True,\n",
        "            validation_data=(attacker_X_test, attacker_y_test),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
        "    amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
        "\n",
        "    # Fit the attack models.\n",
        "    print(\"Training the attack models...\")\n",
        "    amb.fit(\n",
        "        X_shadow, y_shadow, fit_kwargs=dict(epochs=FLAGS.attack_epochs, verbose=True)\n",
        "    )\n",
        "\n",
        "    # Test the success of the attack.\n",
        "\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "        target_model, data_in, data_out\n",
        "    )\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = amb.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print(\"Attack accuracy:\",attack_accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Training the target model...\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1120 11:24:10.251527 140442493179776 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1120 11:24:10.511222 140442493179776 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1120 11:24:11.002718 140442493179776 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 2.2285 - accuracy: 0.1520 - val_loss: 2.0982 - val_accuracy: 0.2580\n",
            "Epoch 2/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 2.0869 - accuracy: 0.2189 - val_loss: 2.1462 - val_accuracy: 0.1960\n",
            "Epoch 3/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.9626 - accuracy: 0.2564 - val_loss: 1.8226 - val_accuracy: 0.3400\n",
            "Epoch 4/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.8256 - accuracy: 0.3147 - val_loss: 1.7267 - val_accuracy: 0.3580\n",
            "Epoch 5/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.7347 - accuracy: 0.3533 - val_loss: 1.5946 - val_accuracy: 0.4660\n",
            "Epoch 6/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.6455 - accuracy: 0.3778 - val_loss: 1.5386 - val_accuracy: 0.4940\n",
            "Epoch 7/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.5790 - accuracy: 0.4162 - val_loss: 1.5328 - val_accuracy: 0.4440\n",
            "Epoch 8/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.5484 - accuracy: 0.4247 - val_loss: 1.5449 - val_accuracy: 0.4580\n",
            "Epoch 9/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.5157 - accuracy: 0.4444 - val_loss: 1.4192 - val_accuracy: 0.5280\n",
            "Epoch 10/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.4733 - accuracy: 0.4596 - val_loss: 1.4199 - val_accuracy: 0.5140\n",
            "Epoch 11/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.4232 - accuracy: 0.4793 - val_loss: 1.3900 - val_accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.3892 - accuracy: 0.4922 - val_loss: 1.3671 - val_accuracy: 0.5340\n",
            "Epoch 13/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.3575 - accuracy: 0.5011 - val_loss: 1.3876 - val_accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.3502 - accuracy: 0.5182 - val_loss: 1.3660 - val_accuracy: 0.5400\n",
            "Epoch 15/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.3163 - accuracy: 0.5202 - val_loss: 1.3093 - val_accuracy: 0.5400\n",
            "Epoch 16/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2716 - accuracy: 0.5411 - val_loss: 1.3217 - val_accuracy: 0.5240\n",
            "Epoch 17/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2772 - accuracy: 0.5398 - val_loss: 1.2761 - val_accuracy: 0.5420\n",
            "Epoch 18/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2354 - accuracy: 0.5438 - val_loss: 1.2904 - val_accuracy: 0.5480\n",
            "Epoch 19/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2121 - accuracy: 0.5589 - val_loss: 1.2739 - val_accuracy: 0.5500\n",
            "Epoch 20/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2088 - accuracy: 0.5620 - val_loss: 1.2604 - val_accuracy: 0.5480\n",
            "Epoch 21/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.2099 - accuracy: 0.5644 - val_loss: 1.2873 - val_accuracy: 0.5240\n",
            "Epoch 22/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.1880 - accuracy: 0.5804 - val_loss: 1.2863 - val_accuracy: 0.5400\n",
            "Epoch 23/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.1363 - accuracy: 0.5802 - val_loss: 1.2035 - val_accuracy: 0.5620\n",
            "Epoch 24/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.1165 - accuracy: 0.5880 - val_loss: 1.2115 - val_accuracy: 0.5520\n",
            "Epoch 25/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.1211 - accuracy: 0.5922 - val_loss: 1.2343 - val_accuracy: 0.5720\n",
            "Epoch 26/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.1018 - accuracy: 0.5993 - val_loss: 1.1856 - val_accuracy: 0.5760\n",
            "Epoch 27/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.0949 - accuracy: 0.6138 - val_loss: 1.2406 - val_accuracy: 0.5620\n",
            "Epoch 28/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.0698 - accuracy: 0.6098 - val_loss: 1.1842 - val_accuracy: 0.5940\n",
            "Epoch 29/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.0586 - accuracy: 0.6180 - val_loss: 1.2404 - val_accuracy: 0.5800\n",
            "Epoch 30/30\n",
            "4500/4500 [==============================] - 25s 6ms/sample - loss: 1.0503 - accuracy: 0.6291 - val_loss: 1.2064 - val_accuracy: 0.5860\n",
            "(900, 32, 32, 3) (100, 32, 32, 3)\n",
            "Training the shadow models...\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1120 11:36:48.506628 140442493179776 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W1120 11:36:48.701941 140442493179776 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3242 - accuracy: 0.0950 - val_loss: 2.3032 - val_accuracy: 0.0900\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2911 - accuracy: 0.1250 - val_loss: 2.3043 - val_accuracy: 0.0900\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.3043 - accuracy: 0.1025 - val_loss: 2.3001 - val_accuracy: 0.0900\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2896 - accuracy: 0.1350 - val_loss: 2.2926 - val_accuracy: 0.0900\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2630 - accuracy: 0.1250 - val_loss: 2.2777 - val_accuracy: 0.0700\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2672 - accuracy: 0.1200 - val_loss: 2.2444 - val_accuracy: 0.1000\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2645 - accuracy: 0.1425 - val_loss: 2.2236 - val_accuracy: 0.1500\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2284 - accuracy: 0.1425 - val_loss: 2.2232 - val_accuracy: 0.1400\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2044 - accuracy: 0.2100 - val_loss: 2.1988 - val_accuracy: 0.1800\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1746 - accuracy: 0.2150 - val_loss: 2.1510 - val_accuracy: 0.2000\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1358 - accuracy: 0.1800 - val_loss: 2.1246 - val_accuracy: 0.2000\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1519 - accuracy: 0.1900 - val_loss: 2.1098 - val_accuracy: 0.2100\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0898 - accuracy: 0.2475 - val_loss: 2.0690 - val_accuracy: 0.2600\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0562 - accuracy: 0.2500 - val_loss: 2.1274 - val_accuracy: 0.2100\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0988 - accuracy: 0.2275 - val_loss: 2.1356 - val_accuracy: 0.2100\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0931 - accuracy: 0.2275 - val_loss: 2.1168 - val_accuracy: 0.2100\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0198 - accuracy: 0.2375 - val_loss: 2.0811 - val_accuracy: 0.2600\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9665 - accuracy: 0.2425 - val_loss: 2.1347 - val_accuracy: 0.2000\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9629 - accuracy: 0.2775 - val_loss: 2.0828 - val_accuracy: 0.2700\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9499 - accuracy: 0.2675 - val_loss: 1.9777 - val_accuracy: 0.2800\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9262 - accuracy: 0.2700 - val_loss: 1.9901 - val_accuracy: 0.2700\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9534 - accuracy: 0.2450 - val_loss: 2.0290 - val_accuracy: 0.2700\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8956 - accuracy: 0.3025 - val_loss: 2.0006 - val_accuracy: 0.2400\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8699 - accuracy: 0.3125 - val_loss: 2.0425 - val_accuracy: 0.2700\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8454 - accuracy: 0.3100 - val_loss: 1.9048 - val_accuracy: 0.3400\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8359 - accuracy: 0.3500 - val_loss: 1.9860 - val_accuracy: 0.2900\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7703 - accuracy: 0.3600 - val_loss: 1.9626 - val_accuracy: 0.2800\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7790 - accuracy: 0.3650 - val_loss: 1.8614 - val_accuracy: 0.3300\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7121 - accuracy: 0.3825 - val_loss: 1.8436 - val_accuracy: 0.2900\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7086 - accuracy: 0.3725 - val_loss: 1.8540 - val_accuracy: 0.2900\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3516 - accuracy: 0.0850 - val_loss: 2.3060 - val_accuracy: 0.0700\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.3057 - accuracy: 0.0925 - val_loss: 2.3079 - val_accuracy: 0.0700\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2975 - accuracy: 0.1150 - val_loss: 2.3068 - val_accuracy: 0.0700\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2944 - accuracy: 0.1175 - val_loss: 2.3110 - val_accuracy: 0.0700\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2900 - accuracy: 0.1400 - val_loss: 2.3085 - val_accuracy: 0.0700\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.3002 - accuracy: 0.1225 - val_loss: 2.3096 - val_accuracy: 0.0700\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2831 - accuracy: 0.1400 - val_loss: 2.3160 - val_accuracy: 0.1500\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2793 - accuracy: 0.1375 - val_loss: 2.3089 - val_accuracy: 0.1500\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2737 - accuracy: 0.1325 - val_loss: 2.2939 - val_accuracy: 0.1900\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2680 - accuracy: 0.1625 - val_loss: 2.2746 - val_accuracy: 0.1700\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1952 - accuracy: 0.2075 - val_loss: 2.2449 - val_accuracy: 0.1600\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2324 - accuracy: 0.1925 - val_loss: 2.2203 - val_accuracy: 0.1900\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2407 - accuracy: 0.1700 - val_loss: 2.2108 - val_accuracy: 0.1900\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1693 - accuracy: 0.2475 - val_loss: 2.1793 - val_accuracy: 0.2400\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0867 - accuracy: 0.2300 - val_loss: 2.1573 - val_accuracy: 0.2500\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0620 - accuracy: 0.2350 - val_loss: 2.1677 - val_accuracy: 0.2100\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0588 - accuracy: 0.2725 - val_loss: 2.1132 - val_accuracy: 0.2400\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0076 - accuracy: 0.2525 - val_loss: 2.1166 - val_accuracy: 0.2300\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0251 - accuracy: 0.2775 - val_loss: 2.1049 - val_accuracy: 0.2400\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9640 - accuracy: 0.2700 - val_loss: 2.1831 - val_accuracy: 0.1800\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9656 - accuracy: 0.2750 - val_loss: 2.0672 - val_accuracy: 0.2400\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9570 - accuracy: 0.2675 - val_loss: 2.0505 - val_accuracy: 0.2300\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9020 - accuracy: 0.3225 - val_loss: 2.0216 - val_accuracy: 0.2800\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8859 - accuracy: 0.3450 - val_loss: 2.0793 - val_accuracy: 0.2400\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8636 - accuracy: 0.3050 - val_loss: 1.9723 - val_accuracy: 0.2800\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8381 - accuracy: 0.3350 - val_loss: 2.0169 - val_accuracy: 0.2500\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7408 - accuracy: 0.3650 - val_loss: 1.9127 - val_accuracy: 0.3100\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7530 - accuracy: 0.3625 - val_loss: 1.9387 - val_accuracy: 0.3200\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7347 - accuracy: 0.3575 - val_loss: 1.8142 - val_accuracy: 0.3100\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6662 - accuracy: 0.4100 - val_loss: 1.9533 - val_accuracy: 0.2900\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3156 - accuracy: 0.1150 - val_loss: 2.3070 - val_accuracy: 0.1000\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.3137 - accuracy: 0.1100 - val_loss: 2.2986 - val_accuracy: 0.0800\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2909 - accuracy: 0.1175 - val_loss: 2.2804 - val_accuracy: 0.1000\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2364 - accuracy: 0.1800 - val_loss: 2.1999 - val_accuracy: 0.1500\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2043 - accuracy: 0.1925 - val_loss: 2.1373 - val_accuracy: 0.1800\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1729 - accuracy: 0.2050 - val_loss: 2.1199 - val_accuracy: 0.2000\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1075 - accuracy: 0.1875 - val_loss: 2.1109 - val_accuracy: 0.1700\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0658 - accuracy: 0.2350 - val_loss: 2.0916 - val_accuracy: 0.2400\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0511 - accuracy: 0.2225 - val_loss: 2.0839 - val_accuracy: 0.2400\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0234 - accuracy: 0.2800 - val_loss: 2.0699 - val_accuracy: 0.2100\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0200 - accuracy: 0.2150 - val_loss: 2.0140 - val_accuracy: 0.2700\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9602 - accuracy: 0.2875 - val_loss: 1.9749 - val_accuracy: 0.2800\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9518 - accuracy: 0.2600 - val_loss: 1.9661 - val_accuracy: 0.2200\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9766 - accuracy: 0.2500 - val_loss: 1.9302 - val_accuracy: 0.3100\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8987 - accuracy: 0.3200 - val_loss: 1.8981 - val_accuracy: 0.3000\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8225 - accuracy: 0.3100 - val_loss: 1.8622 - val_accuracy: 0.3500\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8627 - accuracy: 0.3275 - val_loss: 1.8254 - val_accuracy: 0.3500\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7822 - accuracy: 0.3575 - val_loss: 1.7716 - val_accuracy: 0.3800\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7628 - accuracy: 0.3125 - val_loss: 1.7924 - val_accuracy: 0.2800\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7439 - accuracy: 0.3625 - val_loss: 1.7821 - val_accuracy: 0.3000\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6740 - accuracy: 0.3775 - val_loss: 1.7938 - val_accuracy: 0.3700\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6610 - accuracy: 0.3650 - val_loss: 1.6708 - val_accuracy: 0.3700\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6381 - accuracy: 0.3575 - val_loss: 1.6673 - val_accuracy: 0.4000\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5010 - accuracy: 0.4450 - val_loss: 1.6306 - val_accuracy: 0.3700\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5352 - accuracy: 0.4075 - val_loss: 1.6193 - val_accuracy: 0.4300\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5501 - accuracy: 0.4475 - val_loss: 1.6804 - val_accuracy: 0.3600\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4830 - accuracy: 0.4475 - val_loss: 1.7116 - val_accuracy: 0.3500\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4765 - accuracy: 0.4550 - val_loss: 1.6356 - val_accuracy: 0.3600\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4193 - accuracy: 0.4600 - val_loss: 1.6378 - val_accuracy: 0.3900\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3819 - accuracy: 0.4950 - val_loss: 1.6238 - val_accuracy: 0.4100\n",
            "Training the attack models...\n",
            "Train on 257 samples\n",
            "Epoch 1/30\n",
            "257/257 [==============================] - 1s 2ms/sample - loss: 0.6930 - accuracy: 0.5214\n",
            "Epoch 2/30\n",
            "257/257 [==============================] - 0s 109us/sample - loss: 0.6889 - accuracy: 0.5370\n",
            "Epoch 3/30\n",
            "257/257 [==============================] - 0s 96us/sample - loss: 0.6863 - accuracy: 0.5681\n",
            "Epoch 4/30\n",
            "257/257 [==============================] - 0s 104us/sample - loss: 0.6793 - accuracy: 0.6304\n",
            "Epoch 5/30\n",
            "257/257 [==============================] - 0s 92us/sample - loss: 0.6746 - accuracy: 0.5914\n",
            "Epoch 6/30\n",
            "257/257 [==============================] - 0s 94us/sample - loss: 0.6775 - accuracy: 0.5409\n",
            "Epoch 7/30\n",
            "257/257 [==============================] - 0s 99us/sample - loss: 0.6717 - accuracy: 0.5642\n",
            "Epoch 8/30\n",
            "257/257 [==============================] - 0s 110us/sample - loss: 0.6688 - accuracy: 0.6070\n",
            "Epoch 9/30\n",
            "257/257 [==============================] - 0s 117us/sample - loss: 0.6666 - accuracy: 0.5875\n",
            "Epoch 10/30\n",
            "257/257 [==============================] - 0s 102us/sample - loss: 0.6701 - accuracy: 0.6070\n",
            "Epoch 11/30\n",
            "257/257 [==============================] - 0s 106us/sample - loss: 0.6639 - accuracy: 0.6265\n",
            "Epoch 12/30\n",
            "257/257 [==============================] - 0s 112us/sample - loss: 0.6608 - accuracy: 0.6304\n",
            "Epoch 13/30\n",
            "257/257 [==============================] - 0s 95us/sample - loss: 0.6641 - accuracy: 0.5914\n",
            "Epoch 14/30\n",
            "257/257 [==============================] - 0s 102us/sample - loss: 0.6631 - accuracy: 0.5875\n",
            "Epoch 15/30\n",
            "257/257 [==============================] - 0s 113us/sample - loss: 0.6654 - accuracy: 0.6226\n",
            "Epoch 16/30\n",
            "257/257 [==============================] - 0s 130us/sample - loss: 0.6519 - accuracy: 0.6187\n",
            "Epoch 17/30\n",
            "257/257 [==============================] - 0s 105us/sample - loss: 0.6503 - accuracy: 0.6265\n",
            "Epoch 18/30\n",
            "257/257 [==============================] - 0s 107us/sample - loss: 0.6556 - accuracy: 0.6109\n",
            "Epoch 19/30\n",
            "257/257 [==============================] - 0s 114us/sample - loss: 0.6509 - accuracy: 0.6304\n",
            "Epoch 20/30\n",
            "257/257 [==============================] - 0s 103us/sample - loss: 0.6475 - accuracy: 0.6342\n",
            "Epoch 21/30\n",
            "257/257 [==============================] - 0s 103us/sample - loss: 0.6499 - accuracy: 0.6187\n",
            "Epoch 22/30\n",
            "257/257 [==============================] - 0s 105us/sample - loss: 0.6503 - accuracy: 0.6109\n",
            "Epoch 23/30\n",
            "257/257 [==============================] - 0s 129us/sample - loss: 0.6561 - accuracy: 0.6109\n",
            "Epoch 24/30\n",
            "257/257 [==============================] - 0s 165us/sample - loss: 0.6552 - accuracy: 0.6148\n",
            "Epoch 25/30\n",
            "257/257 [==============================] - 0s 137us/sample - loss: 0.6429 - accuracy: 0.6342\n",
            "Epoch 26/30\n",
            "257/257 [==============================] - 0s 99us/sample - loss: 0.6441 - accuracy: 0.6304\n",
            "Epoch 27/30\n",
            "257/257 [==============================] - 0s 122us/sample - loss: 0.6425 - accuracy: 0.6304\n",
            "Epoch 28/30\n",
            "257/257 [==============================] - 0s 106us/sample - loss: 0.6579 - accuracy: 0.6187\n",
            "Epoch 29/30\n",
            "257/257 [==============================] - 0s 109us/sample - loss: 0.6587 - accuracy: 0.6304\n",
            "Epoch 30/30\n",
            "257/257 [==============================] - 0s 123us/sample - loss: 0.6522 - accuracy: 0.6226\n",
            "Train on 199 samples\n",
            "Epoch 1/30\n",
            "199/199 [==============================] - 1s 3ms/sample - loss: 0.6917 - accuracy: 0.5176\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 0s 128us/sample - loss: 0.6846 - accuracy: 0.5829\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 0s 122us/sample - loss: 0.6830 - accuracy: 0.5678\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 0s 116us/sample - loss: 0.6802 - accuracy: 0.5829\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 0s 131us/sample - loss: 0.6747 - accuracy: 0.6231\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 0s 131us/sample - loss: 0.6713 - accuracy: 0.6181\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 0s 122us/sample - loss: 0.6698 - accuracy: 0.5980\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 0s 119us/sample - loss: 0.6594 - accuracy: 0.6131\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 0s 125us/sample - loss: 0.6570 - accuracy: 0.5930\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 0s 113us/sample - loss: 0.6473 - accuracy: 0.5980\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 0s 104us/sample - loss: 0.6515 - accuracy: 0.6131\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 0s 106us/sample - loss: 0.6387 - accuracy: 0.6181\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 0s 155us/sample - loss: 0.6516 - accuracy: 0.6030\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 0s 115us/sample - loss: 0.6388 - accuracy: 0.6432\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 0s 142us/sample - loss: 0.6375 - accuracy: 0.6181\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 0s 110us/sample - loss: 0.6291 - accuracy: 0.6281\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 0s 130us/sample - loss: 0.6380 - accuracy: 0.6382\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 0s 126us/sample - loss: 0.6323 - accuracy: 0.6281\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 0s 114us/sample - loss: 0.6308 - accuracy: 0.6583\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 0s 125us/sample - loss: 0.6287 - accuracy: 0.6482\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 0s 129us/sample - loss: 0.6178 - accuracy: 0.6583\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 0s 137us/sample - loss: 0.6168 - accuracy: 0.6784\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 0s 107us/sample - loss: 0.6219 - accuracy: 0.6482\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 0s 102us/sample - loss: 0.6144 - accuracy: 0.6834\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 0s 123us/sample - loss: 0.6038 - accuracy: 0.6432\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 0s 138us/sample - loss: 0.6113 - accuracy: 0.6683\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 0s 112us/sample - loss: 0.6161 - accuracy: 0.6583\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 0s 116us/sample - loss: 0.6272 - accuracy: 0.6533\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 0s 173us/sample - loss: 0.6066 - accuracy: 0.6683\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 0s 137us/sample - loss: 0.6158 - accuracy: 0.6734\n",
            "Train on 233 samples\n",
            "Epoch 1/30\n",
            "233/233 [==============================] - 1s 3ms/sample - loss: 0.6911 - accuracy: 0.5193\n",
            "Epoch 2/30\n",
            "233/233 [==============================] - 0s 104us/sample - loss: 0.6902 - accuracy: 0.5150\n",
            "Epoch 3/30\n",
            "233/233 [==============================] - 0s 110us/sample - loss: 0.6897 - accuracy: 0.5150\n",
            "Epoch 4/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.6850 - accuracy: 0.5451\n",
            "Epoch 5/30\n",
            "233/233 [==============================] - 0s 102us/sample - loss: 0.6833 - accuracy: 0.5494\n",
            "Epoch 6/30\n",
            "233/233 [==============================] - 0s 106us/sample - loss: 0.6818 - accuracy: 0.5665\n",
            "Epoch 7/30\n",
            "233/233 [==============================] - 0s 90us/sample - loss: 0.6755 - accuracy: 0.6180\n",
            "Epoch 8/30\n",
            "233/233 [==============================] - 0s 98us/sample - loss: 0.6707 - accuracy: 0.6052\n",
            "Epoch 9/30\n",
            "233/233 [==============================] - 0s 114us/sample - loss: 0.6687 - accuracy: 0.6223\n",
            "Epoch 10/30\n",
            "233/233 [==============================] - 0s 109us/sample - loss: 0.6678 - accuracy: 0.5794\n",
            "Epoch 11/30\n",
            "233/233 [==============================] - 0s 102us/sample - loss: 0.6657 - accuracy: 0.5794\n",
            "Epoch 12/30\n",
            "233/233 [==============================] - 0s 108us/sample - loss: 0.6535 - accuracy: 0.6352\n",
            "Epoch 13/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.6572 - accuracy: 0.6137\n",
            "Epoch 14/30\n",
            "233/233 [==============================] - 0s 105us/sample - loss: 0.6545 - accuracy: 0.6180\n",
            "Epoch 15/30\n",
            "233/233 [==============================] - 0s 99us/sample - loss: 0.6565 - accuracy: 0.6223\n",
            "Epoch 16/30\n",
            "233/233 [==============================] - 0s 109us/sample - loss: 0.6532 - accuracy: 0.5923\n",
            "Epoch 17/30\n",
            "233/233 [==============================] - 0s 100us/sample - loss: 0.6457 - accuracy: 0.6223\n",
            "Epoch 18/30\n",
            "233/233 [==============================] - 0s 128us/sample - loss: 0.6389 - accuracy: 0.6137\n",
            "Epoch 19/30\n",
            "233/233 [==============================] - 0s 102us/sample - loss: 0.6294 - accuracy: 0.6695\n",
            "Epoch 20/30\n",
            "233/233 [==============================] - 0s 106us/sample - loss: 0.6474 - accuracy: 0.6094\n",
            "Epoch 21/30\n",
            "233/233 [==============================] - 0s 103us/sample - loss: 0.6358 - accuracy: 0.6609\n",
            "Epoch 22/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.6578 - accuracy: 0.6009\n",
            "Epoch 23/30\n",
            "233/233 [==============================] - 0s 104us/sample - loss: 0.6326 - accuracy: 0.6223\n",
            "Epoch 24/30\n",
            "233/233 [==============================] - 0s 115us/sample - loss: 0.6446 - accuracy: 0.6481\n",
            "Epoch 25/30\n",
            "233/233 [==============================] - 0s 121us/sample - loss: 0.6259 - accuracy: 0.6481\n",
            "Epoch 26/30\n",
            "233/233 [==============================] - 0s 125us/sample - loss: 0.6292 - accuracy: 0.6481\n",
            "Epoch 27/30\n",
            "233/233 [==============================] - 0s 126us/sample - loss: 0.6296 - accuracy: 0.6481\n",
            "Epoch 28/30\n",
            "233/233 [==============================] - 0s 118us/sample - loss: 0.6138 - accuracy: 0.6481\n",
            "Epoch 29/30\n",
            "233/233 [==============================] - 0s 114us/sample - loss: 0.6308 - accuracy: 0.6137\n",
            "Epoch 30/30\n",
            "233/233 [==============================] - 0s 107us/sample - loss: 0.6280 - accuracy: 0.6609\n",
            "Train on 252 samples\n",
            "Epoch 1/30\n",
            "252/252 [==============================] - 1s 2ms/sample - loss: 0.6911 - accuracy: 0.5794\n",
            "Epoch 2/30\n",
            "252/252 [==============================] - 0s 95us/sample - loss: 0.6850 - accuracy: 0.5635\n",
            "Epoch 3/30\n",
            "252/252 [==============================] - 0s 99us/sample - loss: 0.6788 - accuracy: 0.6071\n",
            "Epoch 4/30\n",
            "252/252 [==============================] - 0s 91us/sample - loss: 0.6767 - accuracy: 0.5992\n",
            "Epoch 5/30\n",
            "252/252 [==============================] - 0s 100us/sample - loss: 0.6692 - accuracy: 0.6230\n",
            "Epoch 6/30\n",
            "252/252 [==============================] - 0s 103us/sample - loss: 0.6540 - accuracy: 0.6548\n",
            "Epoch 7/30\n",
            "252/252 [==============================] - 0s 92us/sample - loss: 0.6457 - accuracy: 0.6508\n",
            "Epoch 8/30\n",
            "252/252 [==============================] - 0s 85us/sample - loss: 0.6253 - accuracy: 0.6667\n",
            "Epoch 9/30\n",
            "252/252 [==============================] - 0s 110us/sample - loss: 0.6258 - accuracy: 0.6389\n",
            "Epoch 10/30\n",
            "252/252 [==============================] - 0s 106us/sample - loss: 0.6272 - accuracy: 0.6548\n",
            "Epoch 11/30\n",
            "252/252 [==============================] - 0s 110us/sample - loss: 0.6113 - accuracy: 0.6706\n",
            "Epoch 12/30\n",
            "252/252 [==============================] - 0s 117us/sample - loss: 0.5978 - accuracy: 0.6905\n",
            "Epoch 13/30\n",
            "252/252 [==============================] - 0s 114us/sample - loss: 0.6204 - accuracy: 0.6627\n",
            "Epoch 14/30\n",
            "252/252 [==============================] - 0s 93us/sample - loss: 0.5997 - accuracy: 0.6825\n",
            "Epoch 15/30\n",
            "252/252 [==============================] - 0s 92us/sample - loss: 0.6022 - accuracy: 0.6746\n",
            "Epoch 16/30\n",
            "252/252 [==============================] - 0s 92us/sample - loss: 0.5830 - accuracy: 0.7103\n",
            "Epoch 17/30\n",
            "252/252 [==============================] - 0s 95us/sample - loss: 0.5815 - accuracy: 0.6944\n",
            "Epoch 18/30\n",
            "252/252 [==============================] - 0s 111us/sample - loss: 0.5705 - accuracy: 0.7063\n",
            "Epoch 19/30\n",
            "252/252 [==============================] - 0s 94us/sample - loss: 0.5665 - accuracy: 0.7262\n",
            "Epoch 20/30\n",
            "252/252 [==============================] - 0s 98us/sample - loss: 0.6044 - accuracy: 0.6706\n",
            "Epoch 21/30\n",
            "252/252 [==============================] - 0s 93us/sample - loss: 0.5872 - accuracy: 0.7063\n",
            "Epoch 22/30\n",
            "252/252 [==============================] - 0s 89us/sample - loss: 0.5806 - accuracy: 0.7222\n",
            "Epoch 23/30\n",
            "252/252 [==============================] - 0s 99us/sample - loss: 0.5821 - accuracy: 0.6825\n",
            "Epoch 24/30\n",
            "252/252 [==============================] - 0s 88us/sample - loss: 0.5829 - accuracy: 0.7143\n",
            "Epoch 25/30\n",
            "252/252 [==============================] - 0s 97us/sample - loss: 0.5752 - accuracy: 0.6984\n",
            "Epoch 26/30\n",
            "252/252 [==============================] - 0s 108us/sample - loss: 0.5630 - accuracy: 0.6905\n",
            "Epoch 27/30\n",
            "252/252 [==============================] - 0s 100us/sample - loss: 0.5617 - accuracy: 0.7262\n",
            "Epoch 28/30\n",
            "252/252 [==============================] - 0s 102us/sample - loss: 0.5764 - accuracy: 0.6865\n",
            "Epoch 29/30\n",
            "252/252 [==============================] - 0s 166us/sample - loss: 0.5683 - accuracy: 0.6984\n",
            "Epoch 30/30\n",
            "252/252 [==============================] - 0s 103us/sample - loss: 0.5621 - accuracy: 0.7103\n",
            "Train on 221 samples\n",
            "Epoch 1/30\n",
            "221/221 [==============================] - 1s 3ms/sample - loss: 0.6940 - accuracy: 0.4751\n",
            "Epoch 2/30\n",
            "221/221 [==============================] - 0s 103us/sample - loss: 0.6933 - accuracy: 0.4887\n",
            "Epoch 3/30\n",
            "221/221 [==============================] - 0s 94us/sample - loss: 0.6901 - accuracy: 0.5475\n",
            "Epoch 4/30\n",
            "221/221 [==============================] - 0s 84us/sample - loss: 0.6878 - accuracy: 0.5475\n",
            "Epoch 5/30\n",
            "221/221 [==============================] - 0s 108us/sample - loss: 0.6867 - accuracy: 0.5520\n",
            "Epoch 6/30\n",
            "221/221 [==============================] - 0s 99us/sample - loss: 0.6855 - accuracy: 0.5475\n",
            "Epoch 7/30\n",
            "221/221 [==============================] - 0s 99us/sample - loss: 0.6841 - accuracy: 0.5520\n",
            "Epoch 8/30\n",
            "221/221 [==============================] - 0s 96us/sample - loss: 0.6824 - accuracy: 0.5520\n",
            "Epoch 9/30\n",
            "221/221 [==============================] - 0s 91us/sample - loss: 0.6821 - accuracy: 0.5566\n",
            "Epoch 10/30\n",
            "221/221 [==============================] - 0s 96us/sample - loss: 0.6782 - accuracy: 0.5566\n",
            "Epoch 11/30\n",
            "221/221 [==============================] - 0s 94us/sample - loss: 0.6774 - accuracy: 0.5566\n",
            "Epoch 12/30\n",
            "221/221 [==============================] - 0s 103us/sample - loss: 0.6785 - accuracy: 0.5837\n",
            "Epoch 13/30\n",
            "221/221 [==============================] - 0s 108us/sample - loss: 0.6742 - accuracy: 0.5792\n",
            "Epoch 14/30\n",
            "221/221 [==============================] - 0s 100us/sample - loss: 0.6677 - accuracy: 0.5792\n",
            "Epoch 15/30\n",
            "221/221 [==============================] - 0s 115us/sample - loss: 0.6720 - accuracy: 0.5747\n",
            "Epoch 16/30\n",
            "221/221 [==============================] - 0s 115us/sample - loss: 0.6699 - accuracy: 0.5656\n",
            "Epoch 17/30\n",
            "221/221 [==============================] - 0s 112us/sample - loss: 0.6647 - accuracy: 0.6244\n",
            "Epoch 18/30\n",
            "221/221 [==============================] - 0s 142us/sample - loss: 0.6579 - accuracy: 0.6063\n",
            "Epoch 19/30\n",
            "221/221 [==============================] - 0s 109us/sample - loss: 0.6657 - accuracy: 0.5928\n",
            "Epoch 20/30\n",
            "221/221 [==============================] - 0s 103us/sample - loss: 0.6553 - accuracy: 0.6063\n",
            "Epoch 21/30\n",
            "221/221 [==============================] - 0s 116us/sample - loss: 0.6536 - accuracy: 0.5928\n",
            "Epoch 22/30\n",
            "221/221 [==============================] - 0s 100us/sample - loss: 0.6488 - accuracy: 0.5882\n",
            "Epoch 23/30\n",
            "221/221 [==============================] - 0s 96us/sample - loss: 0.6326 - accuracy: 0.5882\n",
            "Epoch 24/30\n",
            "221/221 [==============================] - 0s 103us/sample - loss: 0.6353 - accuracy: 0.6516\n",
            "Epoch 25/30\n",
            "221/221 [==============================] - 0s 93us/sample - loss: 0.6350 - accuracy: 0.6380\n",
            "Epoch 26/30\n",
            "221/221 [==============================] - 0s 120us/sample - loss: 0.6371 - accuracy: 0.6109\n",
            "Epoch 27/30\n",
            "221/221 [==============================] - 0s 138us/sample - loss: 0.6260 - accuracy: 0.6335\n",
            "Epoch 28/30\n",
            "221/221 [==============================] - 0s 100us/sample - loss: 0.6413 - accuracy: 0.6199\n",
            "Epoch 29/30\n",
            "221/221 [==============================] - 0s 108us/sample - loss: 0.6345 - accuracy: 0.6109\n",
            "Epoch 30/30\n",
            "221/221 [==============================] - 0s 107us/sample - loss: 0.6205 - accuracy: 0.6471\n",
            "Train on 205 samples\n",
            "Epoch 1/30\n",
            "205/205 [==============================] - 1s 3ms/sample - loss: 0.6925 - accuracy: 0.5415\n",
            "Epoch 2/30\n",
            "205/205 [==============================] - 0s 107us/sample - loss: 0.6881 - accuracy: 0.6195\n",
            "Epoch 3/30\n",
            "205/205 [==============================] - 0s 105us/sample - loss: 0.6835 - accuracy: 0.6293\n",
            "Epoch 4/30\n",
            "205/205 [==============================] - 0s 125us/sample - loss: 0.6777 - accuracy: 0.6390\n",
            "Epoch 5/30\n",
            "205/205 [==============================] - 0s 107us/sample - loss: 0.6705 - accuracy: 0.6488\n",
            "Epoch 6/30\n",
            "205/205 [==============================] - 0s 134us/sample - loss: 0.6583 - accuracy: 0.6585\n",
            "Epoch 7/30\n",
            "205/205 [==============================] - 0s 119us/sample - loss: 0.6494 - accuracy: 0.6537\n",
            "Epoch 8/30\n",
            "205/205 [==============================] - 0s 123us/sample - loss: 0.6359 - accuracy: 0.6732\n",
            "Epoch 9/30\n",
            "205/205 [==============================] - 0s 108us/sample - loss: 0.6210 - accuracy: 0.6829\n",
            "Epoch 10/30\n",
            "205/205 [==============================] - 0s 109us/sample - loss: 0.6018 - accuracy: 0.7317\n",
            "Epoch 11/30\n",
            "205/205 [==============================] - 0s 121us/sample - loss: 0.6085 - accuracy: 0.6927\n",
            "Epoch 12/30\n",
            "205/205 [==============================] - 0s 98us/sample - loss: 0.5998 - accuracy: 0.6780\n",
            "Epoch 13/30\n",
            "205/205 [==============================] - 0s 118us/sample - loss: 0.5830 - accuracy: 0.6829\n",
            "Epoch 14/30\n",
            "205/205 [==============================] - 0s 123us/sample - loss: 0.5893 - accuracy: 0.6976\n",
            "Epoch 15/30\n",
            "205/205 [==============================] - 0s 114us/sample - loss: 0.5719 - accuracy: 0.7024\n",
            "Epoch 16/30\n",
            "205/205 [==============================] - 0s 123us/sample - loss: 0.5783 - accuracy: 0.6976\n",
            "Epoch 17/30\n",
            "205/205 [==============================] - 0s 111us/sample - loss: 0.5928 - accuracy: 0.7122\n",
            "Epoch 18/30\n",
            "205/205 [==============================] - 0s 118us/sample - loss: 0.6045 - accuracy: 0.7171\n",
            "Epoch 19/30\n",
            "205/205 [==============================] - 0s 107us/sample - loss: 0.5604 - accuracy: 0.7366\n",
            "Epoch 20/30\n",
            "205/205 [==============================] - 0s 104us/sample - loss: 0.5629 - accuracy: 0.7171\n",
            "Epoch 21/30\n",
            "205/205 [==============================] - 0s 116us/sample - loss: 0.5709 - accuracy: 0.7024\n",
            "Epoch 22/30\n",
            "205/205 [==============================] - 0s 157us/sample - loss: 0.5699 - accuracy: 0.7317\n",
            "Epoch 23/30\n",
            "205/205 [==============================] - 0s 97us/sample - loss: 0.5671 - accuracy: 0.6732\n",
            "Epoch 24/30\n",
            "205/205 [==============================] - 0s 117us/sample - loss: 0.5843 - accuracy: 0.6976\n",
            "Epoch 25/30\n",
            "205/205 [==============================] - 0s 101us/sample - loss: 0.5597 - accuracy: 0.7268\n",
            "Epoch 26/30\n",
            "205/205 [==============================] - 0s 107us/sample - loss: 0.5793 - accuracy: 0.7122\n",
            "Epoch 27/30\n",
            "205/205 [==============================] - 0s 95us/sample - loss: 0.5510 - accuracy: 0.7561\n",
            "Epoch 28/30\n",
            "205/205 [==============================] - 0s 93us/sample - loss: 0.5623 - accuracy: 0.7366\n",
            "Epoch 29/30\n",
            "205/205 [==============================] - 0s 110us/sample - loss: 0.5806 - accuracy: 0.6976\n",
            "Epoch 30/30\n",
            "205/205 [==============================] - 0s 109us/sample - loss: 0.5148 - accuracy: 0.7366\n",
            "Train on 275 samples\n",
            "Epoch 1/30\n",
            "275/275 [==============================] - 1s 3ms/sample - loss: 0.6913 - accuracy: 0.5455\n",
            "Epoch 2/30\n",
            "275/275 [==============================] - 0s 100us/sample - loss: 0.6876 - accuracy: 0.5273\n",
            "Epoch 3/30\n",
            "275/275 [==============================] - 0s 90us/sample - loss: 0.6856 - accuracy: 0.5564\n",
            "Epoch 4/30\n",
            "275/275 [==============================] - 0s 102us/sample - loss: 0.6798 - accuracy: 0.5709\n",
            "Epoch 5/30\n",
            "275/275 [==============================] - 0s 97us/sample - loss: 0.6764 - accuracy: 0.6182\n",
            "Epoch 6/30\n",
            "275/275 [==============================] - 0s 92us/sample - loss: 0.6729 - accuracy: 0.6000\n",
            "Epoch 7/30\n",
            "275/275 [==============================] - 0s 96us/sample - loss: 0.6636 - accuracy: 0.6036\n",
            "Epoch 8/30\n",
            "275/275 [==============================] - 0s 121us/sample - loss: 0.6593 - accuracy: 0.6109\n",
            "Epoch 9/30\n",
            "275/275 [==============================] - 0s 97us/sample - loss: 0.6546 - accuracy: 0.6109\n",
            "Epoch 10/30\n",
            "275/275 [==============================] - 0s 98us/sample - loss: 0.6445 - accuracy: 0.6509\n",
            "Epoch 11/30\n",
            "275/275 [==============================] - 0s 107us/sample - loss: 0.6419 - accuracy: 0.6327\n",
            "Epoch 12/30\n",
            "275/275 [==============================] - 0s 100us/sample - loss: 0.6485 - accuracy: 0.5927\n",
            "Epoch 13/30\n",
            "275/275 [==============================] - 0s 95us/sample - loss: 0.6374 - accuracy: 0.6327\n",
            "Epoch 14/30\n",
            "275/275 [==============================] - 0s 122us/sample - loss: 0.6430 - accuracy: 0.6218\n",
            "Epoch 15/30\n",
            "275/275 [==============================] - 0s 99us/sample - loss: 0.6447 - accuracy: 0.6073\n",
            "Epoch 16/30\n",
            "275/275 [==============================] - 0s 92us/sample - loss: 0.6270 - accuracy: 0.6618\n",
            "Epoch 17/30\n",
            "275/275 [==============================] - 0s 93us/sample - loss: 0.6367 - accuracy: 0.6436\n",
            "Epoch 18/30\n",
            "275/275 [==============================] - 0s 93us/sample - loss: 0.6412 - accuracy: 0.6109\n",
            "Epoch 19/30\n",
            "275/275 [==============================] - 0s 99us/sample - loss: 0.6263 - accuracy: 0.6400\n",
            "Epoch 20/30\n",
            "275/275 [==============================] - 0s 86us/sample - loss: 0.6244 - accuracy: 0.6364\n",
            "Epoch 21/30\n",
            "275/275 [==============================] - 0s 89us/sample - loss: 0.6275 - accuracy: 0.6436\n",
            "Epoch 22/30\n",
            "275/275 [==============================] - 0s 98us/sample - loss: 0.6180 - accuracy: 0.6400\n",
            "Epoch 23/30\n",
            "275/275 [==============================] - 0s 111us/sample - loss: 0.6236 - accuracy: 0.6582\n",
            "Epoch 24/30\n",
            "275/275 [==============================] - 0s 90us/sample - loss: 0.6224 - accuracy: 0.6582\n",
            "Epoch 25/30\n",
            "275/275 [==============================] - 0s 84us/sample - loss: 0.6166 - accuracy: 0.6364\n",
            "Epoch 26/30\n",
            "275/275 [==============================] - 0s 101us/sample - loss: 0.6139 - accuracy: 0.6582\n",
            "Epoch 27/30\n",
            "275/275 [==============================] - 0s 96us/sample - loss: 0.6192 - accuracy: 0.6545\n",
            "Epoch 28/30\n",
            "275/275 [==============================] - 0s 88us/sample - loss: 0.6124 - accuracy: 0.6509\n",
            "Epoch 29/30\n",
            "275/275 [==============================] - 0s 87us/sample - loss: 0.6104 - accuracy: 0.6545\n",
            "Epoch 30/30\n",
            "275/275 [==============================] - 0s 106us/sample - loss: 0.6174 - accuracy: 0.6582\n",
            "Train on 247 samples\n",
            "Epoch 1/30\n",
            "247/247 [==============================] - 1s 2ms/sample - loss: 0.6919 - accuracy: 0.5182\n",
            "Epoch 2/30\n",
            "247/247 [==============================] - 0s 98us/sample - loss: 0.6881 - accuracy: 0.5506\n",
            "Epoch 3/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6816 - accuracy: 0.5547\n",
            "Epoch 4/30\n",
            "247/247 [==============================] - 0s 105us/sample - loss: 0.6755 - accuracy: 0.5870\n",
            "Epoch 5/30\n",
            "247/247 [==============================] - 0s 105us/sample - loss: 0.6733 - accuracy: 0.6032\n",
            "Epoch 6/30\n",
            "247/247 [==============================] - 0s 95us/sample - loss: 0.6674 - accuracy: 0.6073\n",
            "Epoch 7/30\n",
            "247/247 [==============================] - 0s 89us/sample - loss: 0.6615 - accuracy: 0.5992\n",
            "Epoch 8/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6569 - accuracy: 0.6194\n",
            "Epoch 9/30\n",
            "247/247 [==============================] - 0s 99us/sample - loss: 0.6629 - accuracy: 0.5992\n",
            "Epoch 10/30\n",
            "247/247 [==============================] - 0s 105us/sample - loss: 0.6606 - accuracy: 0.6235\n",
            "Epoch 11/30\n",
            "247/247 [==============================] - 0s 130us/sample - loss: 0.6525 - accuracy: 0.6194\n",
            "Epoch 12/30\n",
            "247/247 [==============================] - 0s 104us/sample - loss: 0.6437 - accuracy: 0.6316\n",
            "Epoch 13/30\n",
            "247/247 [==============================] - 0s 99us/sample - loss: 0.6452 - accuracy: 0.5951\n",
            "Epoch 14/30\n",
            "247/247 [==============================] - 0s 111us/sample - loss: 0.6358 - accuracy: 0.6316\n",
            "Epoch 15/30\n",
            "247/247 [==============================] - 0s 102us/sample - loss: 0.6435 - accuracy: 0.6356\n",
            "Epoch 16/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6512 - accuracy: 0.6275\n",
            "Epoch 17/30\n",
            "247/247 [==============================] - 0s 99us/sample - loss: 0.6374 - accuracy: 0.6235\n",
            "Epoch 18/30\n",
            "247/247 [==============================] - 0s 102us/sample - loss: 0.6363 - accuracy: 0.6478\n",
            "Epoch 19/30\n",
            "247/247 [==============================] - 0s 96us/sample - loss: 0.6376 - accuracy: 0.6275\n",
            "Epoch 20/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6449 - accuracy: 0.6437\n",
            "Epoch 21/30\n",
            "247/247 [==============================] - 0s 123us/sample - loss: 0.6310 - accuracy: 0.6194\n",
            "Epoch 22/30\n",
            "247/247 [==============================] - 0s 103us/sample - loss: 0.6388 - accuracy: 0.6154\n",
            "Epoch 23/30\n",
            "247/247 [==============================] - 0s 92us/sample - loss: 0.6277 - accuracy: 0.6437\n",
            "Epoch 24/30\n",
            "247/247 [==============================] - 0s 107us/sample - loss: 0.6297 - accuracy: 0.6680\n",
            "Epoch 25/30\n",
            "247/247 [==============================] - 0s 109us/sample - loss: 0.6278 - accuracy: 0.6518\n",
            "Epoch 26/30\n",
            "247/247 [==============================] - 0s 87us/sample - loss: 0.6250 - accuracy: 0.6680\n",
            "Epoch 27/30\n",
            "247/247 [==============================] - 0s 89us/sample - loss: 0.6230 - accuracy: 0.6316\n",
            "Epoch 28/30\n",
            "247/247 [==============================] - 0s 95us/sample - loss: 0.6291 - accuracy: 0.6478\n",
            "Epoch 29/30\n",
            "247/247 [==============================] - 0s 94us/sample - loss: 0.6190 - accuracy: 0.6761\n",
            "Epoch 30/30\n",
            "247/247 [==============================] - 0s 93us/sample - loss: 0.6361 - accuracy: 0.6194\n",
            "Train on 247 samples\n",
            "Epoch 1/30\n",
            "247/247 [==============================] - 1s 2ms/sample - loss: 0.6951 - accuracy: 0.4899\n",
            "Epoch 2/30\n",
            "247/247 [==============================] - 0s 93us/sample - loss: 0.6911 - accuracy: 0.5628\n",
            "Epoch 3/30\n",
            "247/247 [==============================] - 0s 98us/sample - loss: 0.6880 - accuracy: 0.5668\n",
            "Epoch 4/30\n",
            "247/247 [==============================] - 0s 90us/sample - loss: 0.6865 - accuracy: 0.5506\n",
            "Epoch 5/30\n",
            "247/247 [==============================] - 0s 89us/sample - loss: 0.6843 - accuracy: 0.5304\n",
            "Epoch 6/30\n",
            "247/247 [==============================] - 0s 89us/sample - loss: 0.6810 - accuracy: 0.5749\n",
            "Epoch 7/30\n",
            "247/247 [==============================] - 0s 93us/sample - loss: 0.6773 - accuracy: 0.5951\n",
            "Epoch 8/30\n",
            "247/247 [==============================] - 0s 90us/sample - loss: 0.6758 - accuracy: 0.5992\n",
            "Epoch 9/30\n",
            "247/247 [==============================] - 0s 90us/sample - loss: 0.6737 - accuracy: 0.6032\n",
            "Epoch 10/30\n",
            "247/247 [==============================] - 0s 88us/sample - loss: 0.6658 - accuracy: 0.6235\n",
            "Epoch 11/30\n",
            "247/247 [==============================] - 0s 102us/sample - loss: 0.6643 - accuracy: 0.6032\n",
            "Epoch 12/30\n",
            "247/247 [==============================] - 0s 89us/sample - loss: 0.6655 - accuracy: 0.6194\n",
            "Epoch 13/30\n",
            "247/247 [==============================] - 0s 95us/sample - loss: 0.6617 - accuracy: 0.5992\n",
            "Epoch 14/30\n",
            "247/247 [==============================] - 0s 125us/sample - loss: 0.6554 - accuracy: 0.6235\n",
            "Epoch 15/30\n",
            "247/247 [==============================] - 0s 94us/sample - loss: 0.6546 - accuracy: 0.6235\n",
            "Epoch 16/30\n",
            "247/247 [==============================] - 0s 107us/sample - loss: 0.6509 - accuracy: 0.6194\n",
            "Epoch 17/30\n",
            "247/247 [==============================] - 0s 100us/sample - loss: 0.6436 - accuracy: 0.6478\n",
            "Epoch 18/30\n",
            "247/247 [==============================] - 0s 99us/sample - loss: 0.6539 - accuracy: 0.5870\n",
            "Epoch 19/30\n",
            "247/247 [==============================] - 0s 104us/sample - loss: 0.6527 - accuracy: 0.6397\n",
            "Epoch 20/30\n",
            "247/247 [==============================] - 0s 93us/sample - loss: 0.6518 - accuracy: 0.6356\n",
            "Epoch 21/30\n",
            "247/247 [==============================] - 0s 108us/sample - loss: 0.6443 - accuracy: 0.6640\n",
            "Epoch 22/30\n",
            "247/247 [==============================] - 0s 100us/sample - loss: 0.6504 - accuracy: 0.5992\n",
            "Epoch 23/30\n",
            "247/247 [==============================] - 0s 105us/sample - loss: 0.6422 - accuracy: 0.6194\n",
            "Epoch 24/30\n",
            "247/247 [==============================] - 0s 106us/sample - loss: 0.6379 - accuracy: 0.6073\n",
            "Epoch 25/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6398 - accuracy: 0.6478\n",
            "Epoch 26/30\n",
            "247/247 [==============================] - 0s 94us/sample - loss: 0.6366 - accuracy: 0.6599\n",
            "Epoch 27/30\n",
            "247/247 [==============================] - 0s 97us/sample - loss: 0.6428 - accuracy: 0.6032\n",
            "Epoch 28/30\n",
            "247/247 [==============================] - 0s 92us/sample - loss: 0.6397 - accuracy: 0.6478\n",
            "Epoch 29/30\n",
            "247/247 [==============================] - 0s 92us/sample - loss: 0.6373 - accuracy: 0.6073\n",
            "Epoch 30/30\n",
            "247/247 [==============================] - 0s 103us/sample - loss: 0.6363 - accuracy: 0.6316\n",
            "Train on 264 samples\n",
            "Epoch 1/30\n",
            "264/264 [==============================] - 1s 2ms/sample - loss: 0.6905 - accuracy: 0.5568\n",
            "Epoch 2/30\n",
            "264/264 [==============================] - 0s 108us/sample - loss: 0.6887 - accuracy: 0.5265\n",
            "Epoch 3/30\n",
            "264/264 [==============================] - 0s 92us/sample - loss: 0.6852 - accuracy: 0.5379\n",
            "Epoch 4/30\n",
            "264/264 [==============================] - 0s 101us/sample - loss: 0.6766 - accuracy: 0.5758\n",
            "Epoch 5/30\n",
            "264/264 [==============================] - 0s 97us/sample - loss: 0.6757 - accuracy: 0.5871\n",
            "Epoch 6/30\n",
            "264/264 [==============================] - 0s 106us/sample - loss: 0.6623 - accuracy: 0.6136\n",
            "Epoch 7/30\n",
            "264/264 [==============================] - 0s 97us/sample - loss: 0.6585 - accuracy: 0.6098\n",
            "Epoch 8/30\n",
            "264/264 [==============================] - 0s 119us/sample - loss: 0.6571 - accuracy: 0.5947\n",
            "Epoch 9/30\n",
            "264/264 [==============================] - 0s 99us/sample - loss: 0.6540 - accuracy: 0.6098\n",
            "Epoch 10/30\n",
            "264/264 [==============================] - 0s 120us/sample - loss: 0.6519 - accuracy: 0.5985\n",
            "Epoch 11/30\n",
            "264/264 [==============================] - 0s 102us/sample - loss: 0.6431 - accuracy: 0.6326\n",
            "Epoch 12/30\n",
            "264/264 [==============================] - 0s 120us/sample - loss: 0.6461 - accuracy: 0.6250\n",
            "Epoch 13/30\n",
            "264/264 [==============================] - 0s 95us/sample - loss: 0.6474 - accuracy: 0.6174\n",
            "Epoch 14/30\n",
            "264/264 [==============================] - 0s 92us/sample - loss: 0.6460 - accuracy: 0.6023\n",
            "Epoch 15/30\n",
            "264/264 [==============================] - 0s 98us/sample - loss: 0.6419 - accuracy: 0.6212\n",
            "Epoch 16/30\n",
            "264/264 [==============================] - 0s 101us/sample - loss: 0.6402 - accuracy: 0.6174\n",
            "Epoch 17/30\n",
            "264/264 [==============================] - 0s 101us/sample - loss: 0.6353 - accuracy: 0.6326\n",
            "Epoch 18/30\n",
            "264/264 [==============================] - 0s 114us/sample - loss: 0.6332 - accuracy: 0.6136\n",
            "Epoch 19/30\n",
            "264/264 [==============================] - 0s 97us/sample - loss: 0.6498 - accuracy: 0.6136\n",
            "Epoch 20/30\n",
            "264/264 [==============================] - 0s 102us/sample - loss: 0.6302 - accuracy: 0.6212\n",
            "Epoch 21/30\n",
            "264/264 [==============================] - 0s 115us/sample - loss: 0.6368 - accuracy: 0.6326\n",
            "Epoch 22/30\n",
            "264/264 [==============================] - 0s 97us/sample - loss: 0.6451 - accuracy: 0.6098\n",
            "Epoch 23/30\n",
            "264/264 [==============================] - 0s 97us/sample - loss: 0.6366 - accuracy: 0.6591\n",
            "Epoch 24/30\n",
            "264/264 [==============================] - 0s 93us/sample - loss: 0.6418 - accuracy: 0.6250\n",
            "Epoch 25/30\n",
            "264/264 [==============================] - 0s 102us/sample - loss: 0.6333 - accuracy: 0.6212\n",
            "Epoch 26/30\n",
            "264/264 [==============================] - 0s 103us/sample - loss: 0.6402 - accuracy: 0.6250\n",
            "Epoch 27/30\n",
            "264/264 [==============================] - 0s 102us/sample - loss: 0.6265 - accuracy: 0.6288\n",
            "Epoch 28/30\n",
            "264/264 [==============================] - 0s 101us/sample - loss: 0.6369 - accuracy: 0.6288\n",
            "Epoch 29/30\n",
            "264/264 [==============================] - 0s 99us/sample - loss: 0.6216 - accuracy: 0.6364\n",
            "Epoch 30/30\n",
            "264/264 [==============================] - 0s 104us/sample - loss: 0.6391 - accuracy: 0.6023\n",
            "Attack accuracy: 0.5775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPaf8ZPYi5hJ",
        "colab_type": "code",
        "outputId": "a49156cd-2250-499e-fe3f-6aa677e256c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the target model for 30 epochs\n",
        "# regularization stopping the training if overfitting has been detected (early stopping)\n",
        "%tensorflow_version 2.x\n",
        "\"\"\"\n",
        "Example membership inference attack against a deep net classifier on the CIFAR10 dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "WIDTH = 32\n",
        "HEIGHT = 32\n",
        "CHANNELS = 3\n",
        "SHADOW_DATASET_SIZE = 400\n",
        "ATTACK_TEST_DATASET_SIZE = 400\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_integer(\n",
        "    \"target_epochs\", 30, \"Number of epochs to train target and shadow models.\"\n",
        ")\n",
        "flags.DEFINE_integer(\"attack_epochs\", 30, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_integer(\"num_shadows\", 3, \"Number of epochs to train attack models.\")\n",
        "flags.DEFINE_string('f', '', '')\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    X_train = X_train.astype(\"float32\")\n",
        "    X_test = X_test.astype(\"float32\")\n",
        "    y_train = y_train.astype(\"float32\")\n",
        "    y_test = y_test.astype(\"float32\")\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    return (X_train[0:5000], y_train[0:5000]), (X_test[0:1000], y_test[0:1000])\n",
        "\n",
        "\n",
        "def target_model_fn():\n",
        "    \"\"\"The architecture of the target (victim) model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(\n",
        "        layers.Conv2D(\n",
        "            32,\n",
        "            (3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"same\",\n",
        "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def attack_model_fn():\n",
        "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
        "    Following the original paper, this attack model is specific to the class of the input.\n",
        "    AttachModelBundle creates multiple instances of this model for each class.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
        "\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def demo(argv):\n",
        "    del argv  # Unused.\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = get_data()\n",
        "\n",
        "    # Train the target model.\n",
        "    print(\"Training the target model...\")\n",
        "    target_model = target_model_fn()\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='max', restore_best_weights=True)\n",
        "    target_model.fit(\n",
        "        X_train, y_train, epochs=FLAGS.target_epochs, validation_split=0.1, verbose=True, callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Train the shadow models.\n",
        "    smb = ShadowModelBundle(\n",
        "        target_model_fn,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=FLAGS.num_shadows,\n",
        "    )\n",
        "\n",
        "    # We assume that attacker's data were not seen in target's training.\n",
        "    attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
        "        X_test, y_test, test_size=0.1\n",
        "    )\n",
        "    print(attacker_X_train.shape, attacker_X_test.shape)\n",
        "\n",
        "    print(\"Training the shadow models...\")\n",
        "    X_shadow, y_shadow = smb.fit_transform(\n",
        "        attacker_X_train,\n",
        "        attacker_y_train,\n",
        "        fit_kwargs=dict(\n",
        "            epochs=FLAGS.target_epochs,\n",
        "            verbose=True,\n",
        "            validation_data=(attacker_X_test, attacker_y_test),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
        "    amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
        "\n",
        "    # Fit the attack models.\n",
        "    print(\"Training the attack models...\")\n",
        "    amb.fit(\n",
        "        X_shadow, y_shadow, fit_kwargs=dict(epochs=FLAGS.attack_epochs, verbose=True)\n",
        "    )\n",
        "\n",
        "    # Test the success of the attack.\n",
        "\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
        "        target_model, data_in, data_out\n",
        "    )\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = amb.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print(\"Attack accuracy:\",attack_accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(demo)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training the target model...\n",
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/30\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 2.1055 - accuracy: 0.2022 - val_loss: 1.8558 - val_accuracy: 0.2940\n",
            "Epoch 2/30\n",
            "4500/4500 [==============================] - 34s 8ms/sample - loss: 1.7801 - accuracy: 0.3404 - val_loss: 1.6563 - val_accuracy: 0.3940\n",
            "Epoch 3/30\n",
            "4500/4500 [==============================] - 31s 7ms/sample - loss: 1.6291 - accuracy: 0.4031 - val_loss: 1.5766 - val_accuracy: 0.4480\n",
            "Epoch 4/30\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 1.4902 - accuracy: 0.4504 - val_loss: 1.4393 - val_accuracy: 0.5080\n",
            "Epoch 5/30\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 1.3869 - accuracy: 0.4976 - val_loss: 1.3700 - val_accuracy: 0.5540\n",
            "Epoch 6/30\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 1.2984 - accuracy: 0.5333 - val_loss: 1.3009 - val_accuracy: 0.5620\n",
            "Epoch 7/30\n",
            "4500/4500 [==============================] - 26s 6ms/sample - loss: 1.1853 - accuracy: 0.5816 - val_loss: 1.3726 - val_accuracy: 0.5200\n",
            "Epoch 8/30\n",
            "4480/4500 [============================>.] - ETA: 0s - loss: 1.1121 - accuracy: 0.5902Restoring model weights from the end of the best epoch\n",
            "4500/4500 [==============================] - 27s 6ms/sample - loss: 1.1108 - accuracy: 0.5904 - val_loss: 1.2845 - val_accuracy: 0.5620\n",
            "Epoch 00008: early stopping\n",
            "(900, 32, 32, 3) (100, 32, 32, 3)\n",
            "Training the shadow models...\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3154 - accuracy: 0.0925 - val_loss: 2.2975 - val_accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2875 - accuracy: 0.1500 - val_loss: 2.2763 - val_accuracy: 0.2100\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2142 - accuracy: 0.1650 - val_loss: 2.1261 - val_accuracy: 0.2600\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1476 - accuracy: 0.2000 - val_loss: 2.1641 - val_accuracy: 0.2800\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.0939 - accuracy: 0.2500 - val_loss: 2.0197 - val_accuracy: 0.2500\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9844 - accuracy: 0.2775 - val_loss: 1.9536 - val_accuracy: 0.3500\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9195 - accuracy: 0.2775 - val_loss: 2.0334 - val_accuracy: 0.2700\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.9238 - accuracy: 0.3025 - val_loss: 1.9873 - val_accuracy: 0.2800\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.8606 - accuracy: 0.3100 - val_loss: 1.8142 - val_accuracy: 0.3600\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6860 - accuracy: 0.3725 - val_loss: 1.7780 - val_accuracy: 0.3500\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7100 - accuracy: 0.4025 - val_loss: 1.8941 - val_accuracy: 0.3700\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.6637 - accuracy: 0.4125 - val_loss: 1.6881 - val_accuracy: 0.3700\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5563 - accuracy: 0.4350 - val_loss: 1.6591 - val_accuracy: 0.4600\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4545 - accuracy: 0.4650 - val_loss: 1.8983 - val_accuracy: 0.3600\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3616 - accuracy: 0.5050 - val_loss: 1.6782 - val_accuracy: 0.4400\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1532 - accuracy: 0.5775 - val_loss: 1.6886 - val_accuracy: 0.4200\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1335 - accuracy: 0.6000 - val_loss: 1.8249 - val_accuracy: 0.4200\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.0480 - accuracy: 0.6425 - val_loss: 1.8086 - val_accuracy: 0.3800\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.9634 - accuracy: 0.6525 - val_loss: 1.8333 - val_accuracy: 0.3600\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.9238 - accuracy: 0.6900 - val_loss: 1.9249 - val_accuracy: 0.3700\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.7059 - accuracy: 0.7600 - val_loss: 1.9378 - val_accuracy: 0.4000\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6041 - accuracy: 0.7950 - val_loss: 1.8950 - val_accuracy: 0.4500\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6185 - accuracy: 0.7950 - val_loss: 2.0786 - val_accuracy: 0.4000\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4778 - accuracy: 0.8275 - val_loss: 2.1390 - val_accuracy: 0.4500\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4866 - accuracy: 0.8175 - val_loss: 2.2037 - val_accuracy: 0.4000\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4451 - accuracy: 0.8425 - val_loss: 2.4842 - val_accuracy: 0.3900\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3227 - accuracy: 0.9000 - val_loss: 2.1968 - val_accuracy: 0.3800\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3222 - accuracy: 0.8850 - val_loss: 2.4363 - val_accuracy: 0.3900\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2606 - accuracy: 0.9200 - val_loss: 2.8341 - val_accuracy: 0.3700\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3423 - accuracy: 0.8950 - val_loss: 2.5373 - val_accuracy: 0.4100\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.3304 - accuracy: 0.0850 - val_loss: 2.2950 - val_accuracy: 0.1100\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.2746 - accuracy: 0.1200 - val_loss: 2.2799 - val_accuracy: 0.1700\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.2041 - accuracy: 0.1775 - val_loss: 2.1697 - val_accuracy: 0.2500\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 2.0694 - accuracy: 0.2625 - val_loss: 2.0915 - val_accuracy: 0.2300\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.0108 - accuracy: 0.2425 - val_loss: 2.0674 - val_accuracy: 0.2400\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 1.9467 - accuracy: 0.3100 - val_loss: 2.1710 - val_accuracy: 0.2900\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 2.0971 - accuracy: 0.2275 - val_loss: 2.1896 - val_accuracy: 0.2400\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 2.0664 - accuracy: 0.2325 - val_loss: 2.0790 - val_accuracy: 0.3100\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.9204 - accuracy: 0.3075 - val_loss: 1.9980 - val_accuracy: 0.2800\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.8468 - accuracy: 0.3100 - val_loss: 1.9343 - val_accuracy: 0.2900\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.7220 - accuracy: 0.3500 - val_loss: 1.8783 - val_accuracy: 0.3400\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 1.6890 - accuracy: 0.4050 - val_loss: 1.9178 - val_accuracy: 0.3900\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.5630 - accuracy: 0.4150 - val_loss: 1.8837 - val_accuracy: 0.3100\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.5779 - accuracy: 0.4175 - val_loss: 1.7900 - val_accuracy: 0.3500\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 1.4391 - accuracy: 0.4600 - val_loss: 1.8087 - val_accuracy: 0.3700\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3464 - accuracy: 0.4825 - val_loss: 1.8358 - val_accuracy: 0.3600\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2962 - accuracy: 0.5125 - val_loss: 1.9761 - val_accuracy: 0.4200\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1923 - accuracy: 0.5600 - val_loss: 1.9021 - val_accuracy: 0.3900\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 1.0278 - accuracy: 0.6200 - val_loss: 1.9165 - val_accuracy: 0.4300\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8847 - accuracy: 0.6950 - val_loss: 2.0877 - val_accuracy: 0.3500\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8265 - accuracy: 0.6875 - val_loss: 2.1107 - val_accuracy: 0.4000\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 0.8217 - accuracy: 0.6900 - val_loss: 2.0596 - val_accuracy: 0.3900\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8651 - accuracy: 0.6975 - val_loss: 2.2593 - val_accuracy: 0.3800\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 0.6730 - accuracy: 0.7250 - val_loss: 2.2441 - val_accuracy: 0.4300\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.5765 - accuracy: 0.7700 - val_loss: 2.2863 - val_accuracy: 0.3700\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 0.5288 - accuracy: 0.8175 - val_loss: 2.5250 - val_accuracy: 0.3400\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4841 - accuracy: 0.8400 - val_loss: 2.4890 - val_accuracy: 0.3500\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3828 - accuracy: 0.8500 - val_loss: 2.6380 - val_accuracy: 0.3600\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3178 - accuracy: 0.8975 - val_loss: 2.9139 - val_accuracy: 0.3700\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3547 - accuracy: 0.8650 - val_loss: 2.8220 - val_accuracy: 0.3800\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/30\n",
            "400/400 [==============================] - 4s 11ms/sample - loss: 2.3214 - accuracy: 0.0800 - val_loss: 2.2970 - val_accuracy: 0.1400\n",
            "Epoch 2/30\n",
            "400/400 [==============================] - 3s 8ms/sample - loss: 2.2739 - accuracy: 0.1200 - val_loss: 2.2363 - val_accuracy: 0.2400\n",
            "Epoch 3/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 2.1974 - accuracy: 0.2000 - val_loss: 2.1108 - val_accuracy: 0.2300\n",
            "Epoch 4/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 2.1390 - accuracy: 0.2100 - val_loss: 2.0778 - val_accuracy: 0.1900\n",
            "Epoch 5/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 2.0931 - accuracy: 0.2250 - val_loss: 2.1915 - val_accuracy: 0.2200\n",
            "Epoch 6/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 2.0969 - accuracy: 0.2400 - val_loss: 2.0385 - val_accuracy: 0.2700\n",
            "Epoch 7/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 2.0268 - accuracy: 0.2250 - val_loss: 1.9958 - val_accuracy: 0.2700\n",
            "Epoch 8/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.9616 - accuracy: 0.2875 - val_loss: 2.0797 - val_accuracy: 0.2700\n",
            "Epoch 9/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 1.9622 - accuracy: 0.2650 - val_loss: 1.9913 - val_accuracy: 0.2900\n",
            "Epoch 10/30\n",
            "400/400 [==============================] - 3s 7ms/sample - loss: 1.8470 - accuracy: 0.3200 - val_loss: 1.8768 - val_accuracy: 0.3800\n",
            "Epoch 11/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.7775 - accuracy: 0.3450 - val_loss: 1.8775 - val_accuracy: 0.3400\n",
            "Epoch 12/30\n",
            "400/400 [==============================] - 3s 6ms/sample - loss: 1.6946 - accuracy: 0.3550 - val_loss: 1.8802 - val_accuracy: 0.3500\n",
            "Epoch 13/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.5523 - accuracy: 0.4425 - val_loss: 1.6992 - val_accuracy: 0.3900\n",
            "Epoch 14/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.4459 - accuracy: 0.4775 - val_loss: 1.6812 - val_accuracy: 0.4000\n",
            "Epoch 15/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.3486 - accuracy: 0.5300 - val_loss: 1.7429 - val_accuracy: 0.3700\n",
            "Epoch 16/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.2563 - accuracy: 0.5125 - val_loss: 1.7342 - val_accuracy: 0.4300\n",
            "Epoch 17/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.1982 - accuracy: 0.5750 - val_loss: 1.7808 - val_accuracy: 0.3900\n",
            "Epoch 18/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 1.0189 - accuracy: 0.6350 - val_loss: 1.7302 - val_accuracy: 0.4000\n",
            "Epoch 19/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8059 - accuracy: 0.7275 - val_loss: 2.0215 - val_accuracy: 0.3600\n",
            "Epoch 20/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.8325 - accuracy: 0.6925 - val_loss: 1.8625 - val_accuracy: 0.3800\n",
            "Epoch 21/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6844 - accuracy: 0.7625 - val_loss: 2.1641 - val_accuracy: 0.4200\n",
            "Epoch 22/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.7116 - accuracy: 0.7225 - val_loss: 2.2645 - val_accuracy: 0.3900\n",
            "Epoch 23/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.6242 - accuracy: 0.7700 - val_loss: 2.0538 - val_accuracy: 0.3900\n",
            "Epoch 24/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4839 - accuracy: 0.8550 - val_loss: 2.4227 - val_accuracy: 0.4500\n",
            "Epoch 25/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4098 - accuracy: 0.8550 - val_loss: 2.6085 - val_accuracy: 0.3800\n",
            "Epoch 26/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.4570 - accuracy: 0.8275 - val_loss: 2.4365 - val_accuracy: 0.4000\n",
            "Epoch 27/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3235 - accuracy: 0.9100 - val_loss: 2.7444 - val_accuracy: 0.3800\n",
            "Epoch 28/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.3540 - accuracy: 0.8775 - val_loss: 2.5016 - val_accuracy: 0.3900\n",
            "Epoch 29/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2395 - accuracy: 0.9175 - val_loss: 3.2346 - val_accuracy: 0.3800\n",
            "Epoch 30/30\n",
            "400/400 [==============================] - 2s 6ms/sample - loss: 0.2346 - accuracy: 0.9175 - val_loss: 3.7875 - val_accuracy: 0.3100\n",
            "Training the attack models...\n",
            "Train on 253 samples\n",
            "Epoch 1/30\n",
            "253/253 [==============================] - 1s 2ms/sample - loss: 0.6848 - accuracy: 0.6206\n",
            "Epoch 2/30\n",
            "253/253 [==============================] - 0s 91us/sample - loss: 0.6679 - accuracy: 0.7075\n",
            "Epoch 3/30\n",
            "253/253 [==============================] - 0s 115us/sample - loss: 0.6387 - accuracy: 0.7628\n",
            "Epoch 4/30\n",
            "253/253 [==============================] - 0s 147us/sample - loss: 0.6014 - accuracy: 0.7708\n",
            "Epoch 5/30\n",
            "253/253 [==============================] - 0s 124us/sample - loss: 0.5533 - accuracy: 0.7510\n",
            "Epoch 6/30\n",
            "253/253 [==============================] - 0s 105us/sample - loss: 0.5051 - accuracy: 0.7628\n",
            "Epoch 7/30\n",
            "253/253 [==============================] - 0s 109us/sample - loss: 0.4814 - accuracy: 0.7510\n",
            "Epoch 8/30\n",
            "253/253 [==============================] - 0s 117us/sample - loss: 0.4647 - accuracy: 0.7747\n",
            "Epoch 9/30\n",
            "253/253 [==============================] - 0s 120us/sample - loss: 0.4685 - accuracy: 0.7589\n",
            "Epoch 10/30\n",
            "253/253 [==============================] - 0s 160us/sample - loss: 0.4682 - accuracy: 0.7628\n",
            "Epoch 11/30\n",
            "253/253 [==============================] - 0s 147us/sample - loss: 0.4585 - accuracy: 0.7787\n",
            "Epoch 12/30\n",
            "253/253 [==============================] - 0s 115us/sample - loss: 0.4606 - accuracy: 0.7589\n",
            "Epoch 13/30\n",
            "253/253 [==============================] - 0s 109us/sample - loss: 0.4514 - accuracy: 0.7747\n",
            "Epoch 14/30\n",
            "253/253 [==============================] - 0s 117us/sample - loss: 0.4560 - accuracy: 0.7668\n",
            "Epoch 15/30\n",
            "253/253 [==============================] - 0s 123us/sample - loss: 0.4473 - accuracy: 0.7708\n",
            "Epoch 16/30\n",
            "253/253 [==============================] - 0s 123us/sample - loss: 0.4568 - accuracy: 0.7628\n",
            "Epoch 17/30\n",
            "253/253 [==============================] - 0s 144us/sample - loss: 0.4365 - accuracy: 0.7787\n",
            "Epoch 18/30\n",
            "253/253 [==============================] - 0s 134us/sample - loss: 0.4537 - accuracy: 0.7747\n",
            "Epoch 19/30\n",
            "253/253 [==============================] - 0s 125us/sample - loss: 0.4471 - accuracy: 0.7628\n",
            "Epoch 20/30\n",
            "253/253 [==============================] - 0s 114us/sample - loss: 0.4492 - accuracy: 0.7628\n",
            "Epoch 21/30\n",
            "253/253 [==============================] - 0s 113us/sample - loss: 0.4423 - accuracy: 0.7628\n",
            "Epoch 22/30\n",
            "253/253 [==============================] - 0s 114us/sample - loss: 0.4428 - accuracy: 0.7826\n",
            "Epoch 23/30\n",
            "253/253 [==============================] - 0s 142us/sample - loss: 0.4425 - accuracy: 0.7866\n",
            "Epoch 24/30\n",
            "253/253 [==============================] - 0s 127us/sample - loss: 0.4449 - accuracy: 0.7747\n",
            "Epoch 25/30\n",
            "253/253 [==============================] - 0s 169us/sample - loss: 0.4532 - accuracy: 0.7826\n",
            "Epoch 26/30\n",
            "253/253 [==============================] - 0s 105us/sample - loss: 0.4480 - accuracy: 0.7668\n",
            "Epoch 27/30\n",
            "253/253 [==============================] - 0s 105us/sample - loss: 0.4394 - accuracy: 0.7826\n",
            "Epoch 28/30\n",
            "253/253 [==============================] - 0s 111us/sample - loss: 0.4355 - accuracy: 0.7787\n",
            "Epoch 29/30\n",
            "253/253 [==============================] - 0s 117us/sample - loss: 0.4356 - accuracy: 0.7708\n",
            "Epoch 30/30\n",
            "253/253 [==============================] - 0s 138us/sample - loss: 0.4377 - accuracy: 0.7866\n",
            "Train on 218 samples\n",
            "Epoch 1/30\n",
            "218/218 [==============================] - 1s 3ms/sample - loss: 0.6810 - accuracy: 0.6009\n",
            "Epoch 2/30\n",
            "218/218 [==============================] - 0s 117us/sample - loss: 0.6492 - accuracy: 0.7294\n",
            "Epoch 3/30\n",
            "218/218 [==============================] - 0s 102us/sample - loss: 0.6171 - accuracy: 0.7615\n",
            "Epoch 4/30\n",
            "218/218 [==============================] - 0s 120us/sample - loss: 0.5873 - accuracy: 0.7752\n",
            "Epoch 5/30\n",
            "218/218 [==============================] - 0s 121us/sample - loss: 0.5300 - accuracy: 0.7798\n",
            "Epoch 6/30\n",
            "218/218 [==============================] - 0s 115us/sample - loss: 0.4976 - accuracy: 0.7752\n",
            "Epoch 7/30\n",
            "218/218 [==============================] - 0s 117us/sample - loss: 0.4507 - accuracy: 0.7844\n",
            "Epoch 8/30\n",
            "218/218 [==============================] - 0s 115us/sample - loss: 0.4405 - accuracy: 0.7890\n",
            "Epoch 9/30\n",
            "218/218 [==============================] - 0s 108us/sample - loss: 0.4243 - accuracy: 0.8257\n",
            "Epoch 10/30\n",
            "218/218 [==============================] - 0s 102us/sample - loss: 0.4128 - accuracy: 0.8073\n",
            "Epoch 11/30\n",
            "218/218 [==============================] - 0s 120us/sample - loss: 0.3858 - accuracy: 0.8303\n",
            "Epoch 12/30\n",
            "218/218 [==============================] - 0s 112us/sample - loss: 0.4153 - accuracy: 0.8119\n",
            "Epoch 13/30\n",
            "218/218 [==============================] - 0s 104us/sample - loss: 0.4106 - accuracy: 0.8028\n",
            "Epoch 14/30\n",
            "218/218 [==============================] - 0s 96us/sample - loss: 0.3904 - accuracy: 0.8257\n",
            "Epoch 15/30\n",
            "218/218 [==============================] - 0s 102us/sample - loss: 0.4066 - accuracy: 0.8211\n",
            "Epoch 16/30\n",
            "218/218 [==============================] - 0s 132us/sample - loss: 0.3938 - accuracy: 0.8303\n",
            "Epoch 17/30\n",
            "218/218 [==============================] - 0s 123us/sample - loss: 0.4032 - accuracy: 0.8165\n",
            "Epoch 18/30\n",
            "218/218 [==============================] - 0s 114us/sample - loss: 0.3957 - accuracy: 0.8440\n",
            "Epoch 19/30\n",
            "218/218 [==============================] - 0s 104us/sample - loss: 0.3879 - accuracy: 0.8303\n",
            "Epoch 20/30\n",
            "218/218 [==============================] - 0s 111us/sample - loss: 0.3929 - accuracy: 0.8532\n",
            "Epoch 21/30\n",
            "218/218 [==============================] - 0s 102us/sample - loss: 0.3679 - accuracy: 0.8440\n",
            "Epoch 22/30\n",
            "218/218 [==============================] - 0s 104us/sample - loss: 0.3909 - accuracy: 0.8578\n",
            "Epoch 23/30\n",
            "218/218 [==============================] - 0s 105us/sample - loss: 0.3793 - accuracy: 0.8440\n",
            "Epoch 24/30\n",
            "218/218 [==============================] - 0s 93us/sample - loss: 0.3786 - accuracy: 0.8349\n",
            "Epoch 25/30\n",
            "218/218 [==============================] - 0s 118us/sample - loss: 0.3626 - accuracy: 0.8532\n",
            "Epoch 26/30\n",
            "218/218 [==============================] - 0s 161us/sample - loss: 0.3792 - accuracy: 0.8486\n",
            "Epoch 27/30\n",
            "218/218 [==============================] - 0s 130us/sample - loss: 0.3751 - accuracy: 0.8440\n",
            "Epoch 28/30\n",
            "218/218 [==============================] - 0s 129us/sample - loss: 0.3793 - accuracy: 0.8394\n",
            "Epoch 29/30\n",
            "218/218 [==============================] - 0s 117us/sample - loss: 0.3908 - accuracy: 0.8303\n",
            "Epoch 30/30\n",
            "218/218 [==============================] - 0s 120us/sample - loss: 0.3575 - accuracy: 0.8670\n",
            "Train on 260 samples\n",
            "Epoch 1/30\n",
            "260/260 [==============================] - 1s 3ms/sample - loss: 0.6852 - accuracy: 0.5577\n",
            "Epoch 2/30\n",
            "260/260 [==============================] - 0s 127us/sample - loss: 0.6537 - accuracy: 0.7423\n",
            "Epoch 3/30\n",
            "260/260 [==============================] - 0s 102us/sample - loss: 0.6121 - accuracy: 0.8154\n",
            "Epoch 4/30\n",
            "260/260 [==============================] - 0s 115us/sample - loss: 0.5566 - accuracy: 0.8269\n",
            "Epoch 5/30\n",
            "260/260 [==============================] - 0s 117us/sample - loss: 0.4951 - accuracy: 0.8115\n",
            "Epoch 6/30\n",
            "260/260 [==============================] - 0s 114us/sample - loss: 0.4371 - accuracy: 0.8192\n",
            "Epoch 7/30\n",
            "260/260 [==============================] - 0s 141us/sample - loss: 0.4167 - accuracy: 0.8462\n",
            "Epoch 8/30\n",
            "260/260 [==============================] - 0s 123us/sample - loss: 0.3882 - accuracy: 0.8423\n",
            "Epoch 9/30\n",
            "260/260 [==============================] - 0s 134us/sample - loss: 0.3835 - accuracy: 0.8385\n",
            "Epoch 10/30\n",
            "260/260 [==============================] - 0s 104us/sample - loss: 0.3930 - accuracy: 0.8346\n",
            "Epoch 11/30\n",
            "260/260 [==============================] - 0s 111us/sample - loss: 0.3661 - accuracy: 0.8423\n",
            "Epoch 12/30\n",
            "260/260 [==============================] - 0s 102us/sample - loss: 0.3740 - accuracy: 0.8423\n",
            "Epoch 13/30\n",
            "260/260 [==============================] - 0s 115us/sample - loss: 0.3856 - accuracy: 0.8423\n",
            "Epoch 14/30\n",
            "260/260 [==============================] - 0s 124us/sample - loss: 0.3740 - accuracy: 0.8423\n",
            "Epoch 15/30\n",
            "260/260 [==============================] - 0s 139us/sample - loss: 0.3683 - accuracy: 0.8538\n",
            "Epoch 16/30\n",
            "260/260 [==============================] - 0s 114us/sample - loss: 0.3668 - accuracy: 0.8500\n",
            "Epoch 17/30\n",
            "260/260 [==============================] - 0s 121us/sample - loss: 0.3700 - accuracy: 0.8577\n",
            "Epoch 18/30\n",
            "260/260 [==============================] - 0s 130us/sample - loss: 0.3654 - accuracy: 0.8577\n",
            "Epoch 19/30\n",
            "260/260 [==============================] - 0s 175us/sample - loss: 0.3752 - accuracy: 0.8500\n",
            "Epoch 20/30\n",
            "260/260 [==============================] - 0s 223us/sample - loss: 0.3671 - accuracy: 0.8538\n",
            "Epoch 21/30\n",
            "260/260 [==============================] - 0s 132us/sample - loss: 0.3584 - accuracy: 0.8538\n",
            "Epoch 22/30\n",
            "260/260 [==============================] - 0s 136us/sample - loss: 0.3507 - accuracy: 0.8538\n",
            "Epoch 23/30\n",
            "260/260 [==============================] - 0s 110us/sample - loss: 0.3749 - accuracy: 0.8423\n",
            "Epoch 24/30\n",
            "260/260 [==============================] - 0s 114us/sample - loss: 0.3482 - accuracy: 0.8654\n",
            "Epoch 25/30\n",
            "260/260 [==============================] - 0s 141us/sample - loss: 0.3586 - accuracy: 0.8538\n",
            "Epoch 26/30\n",
            "260/260 [==============================] - 0s 111us/sample - loss: 0.3632 - accuracy: 0.8423\n",
            "Epoch 27/30\n",
            "260/260 [==============================] - 0s 142us/sample - loss: 0.3415 - accuracy: 0.8577\n",
            "Epoch 28/30\n",
            "260/260 [==============================] - 0s 141us/sample - loss: 0.3564 - accuracy: 0.8577\n",
            "Epoch 29/30\n",
            "260/260 [==============================] - 0s 110us/sample - loss: 0.3513 - accuracy: 0.8538\n",
            "Epoch 30/30\n",
            "260/260 [==============================] - 0s 105us/sample - loss: 0.3593 - accuracy: 0.8538\n",
            "Train on 242 samples\n",
            "Epoch 1/30\n",
            "242/242 [==============================] - 1s 2ms/sample - loss: 0.6798 - accuracy: 0.6694\n",
            "Epoch 2/30\n",
            "242/242 [==============================] - 0s 106us/sample - loss: 0.6265 - accuracy: 0.8554\n",
            "Epoch 3/30\n",
            "242/242 [==============================] - 0s 95us/sample - loss: 0.5519 - accuracy: 0.8843\n",
            "Epoch 4/30\n",
            "242/242 [==============================] - 0s 100us/sample - loss: 0.4474 - accuracy: 0.8843\n",
            "Epoch 5/30\n",
            "242/242 [==============================] - 0s 128us/sample - loss: 0.3477 - accuracy: 0.9091\n",
            "Epoch 6/30\n",
            "242/242 [==============================] - 0s 122us/sample - loss: 0.2613 - accuracy: 0.9215\n",
            "Epoch 7/30\n",
            "242/242 [==============================] - 0s 124us/sample - loss: 0.2338 - accuracy: 0.9215\n",
            "Epoch 8/30\n",
            "242/242 [==============================] - 0s 127us/sample - loss: 0.2193 - accuracy: 0.9174\n",
            "Epoch 9/30\n",
            "242/242 [==============================] - 0s 114us/sample - loss: 0.2354 - accuracy: 0.9215\n",
            "Epoch 10/30\n",
            "242/242 [==============================] - 0s 113us/sample - loss: 0.2195 - accuracy: 0.9298\n",
            "Epoch 11/30\n",
            "242/242 [==============================] - 0s 97us/sample - loss: 0.2119 - accuracy: 0.9174\n",
            "Epoch 12/30\n",
            "242/242 [==============================] - 0s 111us/sample - loss: 0.2074 - accuracy: 0.9298\n",
            "Epoch 13/30\n",
            "242/242 [==============================] - 0s 108us/sample - loss: 0.2008 - accuracy: 0.9256\n",
            "Epoch 14/30\n",
            "242/242 [==============================] - 0s 120us/sample - loss: 0.1980 - accuracy: 0.9298\n",
            "Epoch 15/30\n",
            "242/242 [==============================] - 0s 110us/sample - loss: 0.1940 - accuracy: 0.9380\n",
            "Epoch 16/30\n",
            "242/242 [==============================] - 0s 104us/sample - loss: 0.1998 - accuracy: 0.9256\n",
            "Epoch 17/30\n",
            "242/242 [==============================] - 0s 121us/sample - loss: 0.2031 - accuracy: 0.9174\n",
            "Epoch 18/30\n",
            "242/242 [==============================] - 0s 101us/sample - loss: 0.2151 - accuracy: 0.9380\n",
            "Epoch 19/30\n",
            "242/242 [==============================] - 0s 115us/sample - loss: 0.1906 - accuracy: 0.9339\n",
            "Epoch 20/30\n",
            "242/242 [==============================] - 0s 109us/sample - loss: 0.1907 - accuracy: 0.9339\n",
            "Epoch 21/30\n",
            "242/242 [==============================] - 0s 108us/sample - loss: 0.1960 - accuracy: 0.9298\n",
            "Epoch 22/30\n",
            "242/242 [==============================] - 0s 113us/sample - loss: 0.1821 - accuracy: 0.9421\n",
            "Epoch 23/30\n",
            "242/242 [==============================] - 0s 121us/sample - loss: 0.1857 - accuracy: 0.9380\n",
            "Epoch 24/30\n",
            "242/242 [==============================] - 0s 115us/sample - loss: 0.1914 - accuracy: 0.9215\n",
            "Epoch 25/30\n",
            "242/242 [==============================] - 0s 99us/sample - loss: 0.2006 - accuracy: 0.9256\n",
            "Epoch 26/30\n",
            "242/242 [==============================] - 0s 97us/sample - loss: 0.1948 - accuracy: 0.9256\n",
            "Epoch 27/30\n",
            "242/242 [==============================] - 0s 105us/sample - loss: 0.1985 - accuracy: 0.9174\n",
            "Epoch 28/30\n",
            "242/242 [==============================] - 0s 120us/sample - loss: 0.2075 - accuracy: 0.9174\n",
            "Epoch 29/30\n",
            "242/242 [==============================] - 0s 115us/sample - loss: 0.1983 - accuracy: 0.9421\n",
            "Epoch 30/30\n",
            "242/242 [==============================] - 0s 116us/sample - loss: 0.1938 - accuracy: 0.9380\n",
            "Train on 226 samples\n",
            "Epoch 1/30\n",
            "226/226 [==============================] - 1s 3ms/sample - loss: 0.6804 - accuracy: 0.6991\n",
            "Epoch 2/30\n",
            "226/226 [==============================] - 0s 106us/sample - loss: 0.6468 - accuracy: 0.8850\n",
            "Epoch 3/30\n",
            "226/226 [==============================] - 0s 107us/sample - loss: 0.6010 - accuracy: 0.8982\n",
            "Epoch 4/30\n",
            "226/226 [==============================] - 0s 113us/sample - loss: 0.5302 - accuracy: 0.9159\n",
            "Epoch 5/30\n",
            "226/226 [==============================] - 0s 115us/sample - loss: 0.4391 - accuracy: 0.9204\n",
            "Epoch 6/30\n",
            "226/226 [==============================] - 0s 112us/sample - loss: 0.3478 - accuracy: 0.9115\n",
            "Epoch 7/30\n",
            "226/226 [==============================] - 0s 127us/sample - loss: 0.2818 - accuracy: 0.9071\n",
            "Epoch 8/30\n",
            "226/226 [==============================] - 0s 123us/sample - loss: 0.2503 - accuracy: 0.9071\n",
            "Epoch 9/30\n",
            "226/226 [==============================] - 0s 135us/sample - loss: 0.2290 - accuracy: 0.9115\n",
            "Epoch 10/30\n",
            "226/226 [==============================] - 0s 129us/sample - loss: 0.2201 - accuracy: 0.9336\n",
            "Epoch 11/30\n",
            "226/226 [==============================] - 0s 130us/sample - loss: 0.2325 - accuracy: 0.9204\n",
            "Epoch 12/30\n",
            "226/226 [==============================] - 0s 133us/sample - loss: 0.1825 - accuracy: 0.9381\n",
            "Epoch 13/30\n",
            "226/226 [==============================] - 0s 122us/sample - loss: 0.2010 - accuracy: 0.9336\n",
            "Epoch 14/30\n",
            "226/226 [==============================] - 0s 130us/sample - loss: 0.1945 - accuracy: 0.9381\n",
            "Epoch 15/30\n",
            "226/226 [==============================] - 0s 122us/sample - loss: 0.1783 - accuracy: 0.9469\n",
            "Epoch 16/30\n",
            "226/226 [==============================] - 0s 112us/sample - loss: 0.2065 - accuracy: 0.9248\n",
            "Epoch 17/30\n",
            "226/226 [==============================] - 0s 111us/sample - loss: 0.1931 - accuracy: 0.9336\n",
            "Epoch 18/30\n",
            "226/226 [==============================] - 0s 111us/sample - loss: 0.1830 - accuracy: 0.9425\n",
            "Epoch 19/30\n",
            "226/226 [==============================] - 0s 107us/sample - loss: 0.1922 - accuracy: 0.9381\n",
            "Epoch 20/30\n",
            "226/226 [==============================] - 0s 117us/sample - loss: 0.1913 - accuracy: 0.9381\n",
            "Epoch 21/30\n",
            "226/226 [==============================] - 0s 126us/sample - loss: 0.1941 - accuracy: 0.9425\n",
            "Epoch 22/30\n",
            "226/226 [==============================] - 0s 113us/sample - loss: 0.1909 - accuracy: 0.9336\n",
            "Epoch 23/30\n",
            "226/226 [==============================] - 0s 113us/sample - loss: 0.1693 - accuracy: 0.9336\n",
            "Epoch 24/30\n",
            "226/226 [==============================] - 0s 121us/sample - loss: 0.1951 - accuracy: 0.9292\n",
            "Epoch 25/30\n",
            "226/226 [==============================] - 0s 108us/sample - loss: 0.1844 - accuracy: 0.9425\n",
            "Epoch 26/30\n",
            "226/226 [==============================] - 0s 115us/sample - loss: 0.1899 - accuracy: 0.9425\n",
            "Epoch 27/30\n",
            "226/226 [==============================] - 0s 127us/sample - loss: 0.1805 - accuracy: 0.9336\n",
            "Epoch 28/30\n",
            "226/226 [==============================] - 0s 131us/sample - loss: 0.1700 - accuracy: 0.9381\n",
            "Epoch 29/30\n",
            "226/226 [==============================] - 0s 107us/sample - loss: 0.1643 - accuracy: 0.9469\n",
            "Epoch 30/30\n",
            "226/226 [==============================] - 0s 153us/sample - loss: 0.1895 - accuracy: 0.9292\n",
            "Train on 208 samples\n",
            "Epoch 1/30\n",
            "208/208 [==============================] - 1s 3ms/sample - loss: 0.6755 - accuracy: 0.5865\n",
            "Epoch 2/30\n",
            "208/208 [==============================] - 0s 124us/sample - loss: 0.6164 - accuracy: 0.8654\n",
            "Epoch 3/30\n",
            "208/208 [==============================] - 0s 136us/sample - loss: 0.5471 - accuracy: 0.8894\n",
            "Epoch 4/30\n",
            "208/208 [==============================] - 0s 120us/sample - loss: 0.4470 - accuracy: 0.9375\n",
            "Epoch 5/30\n",
            "208/208 [==============================] - 0s 120us/sample - loss: 0.3418 - accuracy: 0.9327\n",
            "Epoch 6/30\n",
            "208/208 [==============================] - 0s 112us/sample - loss: 0.2780 - accuracy: 0.9279\n",
            "Epoch 7/30\n",
            "208/208 [==============================] - 0s 122us/sample - loss: 0.2135 - accuracy: 0.9375\n",
            "Epoch 8/30\n",
            "208/208 [==============================] - 0s 116us/sample - loss: 0.2083 - accuracy: 0.9279\n",
            "Epoch 9/30\n",
            "208/208 [==============================] - 0s 162us/sample - loss: 0.1780 - accuracy: 0.9471\n",
            "Epoch 10/30\n",
            "208/208 [==============================] - 0s 137us/sample - loss: 0.1948 - accuracy: 0.9279\n",
            "Epoch 11/30\n",
            "208/208 [==============================] - 0s 135us/sample - loss: 0.1905 - accuracy: 0.9327\n",
            "Epoch 12/30\n",
            "208/208 [==============================] - 0s 135us/sample - loss: 0.1878 - accuracy: 0.9375\n",
            "Epoch 13/30\n",
            "208/208 [==============================] - 0s 122us/sample - loss: 0.1925 - accuracy: 0.9375\n",
            "Epoch 14/30\n",
            "208/208 [==============================] - 0s 132us/sample - loss: 0.1818 - accuracy: 0.9423\n",
            "Epoch 15/30\n",
            "208/208 [==============================] - 0s 109us/sample - loss: 0.1759 - accuracy: 0.9327\n",
            "Epoch 16/30\n",
            "208/208 [==============================] - 0s 110us/sample - loss: 0.1901 - accuracy: 0.9375\n",
            "Epoch 17/30\n",
            "208/208 [==============================] - 0s 124us/sample - loss: 0.1920 - accuracy: 0.9327\n",
            "Epoch 18/30\n",
            "208/208 [==============================] - 0s 119us/sample - loss: 0.1932 - accuracy: 0.9375\n",
            "Epoch 19/30\n",
            "208/208 [==============================] - 0s 117us/sample - loss: 0.1916 - accuracy: 0.9375\n",
            "Epoch 20/30\n",
            "208/208 [==============================] - 0s 108us/sample - loss: 0.1804 - accuracy: 0.9423\n",
            "Epoch 21/30\n",
            "208/208 [==============================] - 0s 114us/sample - loss: 0.1912 - accuracy: 0.9375\n",
            "Epoch 22/30\n",
            "208/208 [==============================] - 0s 130us/sample - loss: 0.1870 - accuracy: 0.9327\n",
            "Epoch 23/30\n",
            "208/208 [==============================] - 0s 126us/sample - loss: 0.1817 - accuracy: 0.9423\n",
            "Epoch 24/30\n",
            "208/208 [==============================] - 0s 94us/sample - loss: 0.1991 - accuracy: 0.9327\n",
            "Epoch 25/30\n",
            "208/208 [==============================] - 0s 110us/sample - loss: 0.1790 - accuracy: 0.9375\n",
            "Epoch 26/30\n",
            "208/208 [==============================] - 0s 118us/sample - loss: 0.1755 - accuracy: 0.9423\n",
            "Epoch 27/30\n",
            "208/208 [==============================] - 0s 128us/sample - loss: 0.1771 - accuracy: 0.9375\n",
            "Epoch 28/30\n",
            "208/208 [==============================] - 0s 109us/sample - loss: 0.1957 - accuracy: 0.9327\n",
            "Epoch 29/30\n",
            "208/208 [==============================] - 0s 139us/sample - loss: 0.1939 - accuracy: 0.9327\n",
            "Epoch 30/30\n",
            "208/208 [==============================] - 0s 101us/sample - loss: 0.1728 - accuracy: 0.9375\n",
            "Train on 257 samples\n",
            "Epoch 1/30\n",
            "257/257 [==============================] - 1s 4ms/sample - loss: 0.6786 - accuracy: 0.5875\n",
            "Epoch 2/30\n",
            "257/257 [==============================] - 0s 157us/sample - loss: 0.6537 - accuracy: 0.5875\n",
            "Epoch 3/30\n",
            "257/257 [==============================] - 0s 123us/sample - loss: 0.6170 - accuracy: 0.6693\n",
            "Epoch 4/30\n",
            "257/257 [==============================] - 0s 124us/sample - loss: 0.5936 - accuracy: 0.7510\n",
            "Epoch 5/30\n",
            "257/257 [==============================] - 0s 115us/sample - loss: 0.5616 - accuracy: 0.7665\n",
            "Epoch 6/30\n",
            "257/257 [==============================] - 0s 110us/sample - loss: 0.5536 - accuracy: 0.7782\n",
            "Epoch 7/30\n",
            "257/257 [==============================] - 0s 133us/sample - loss: 0.5278 - accuracy: 0.7860\n",
            "Epoch 8/30\n",
            "257/257 [==============================] - 0s 106us/sample - loss: 0.5071 - accuracy: 0.7860\n",
            "Epoch 9/30\n",
            "257/257 [==============================] - 0s 104us/sample - loss: 0.4886 - accuracy: 0.7938\n",
            "Epoch 10/30\n",
            "257/257 [==============================] - 0s 95us/sample - loss: 0.5101 - accuracy: 0.7860\n",
            "Epoch 11/30\n",
            "257/257 [==============================] - 0s 104us/sample - loss: 0.4874 - accuracy: 0.7782\n",
            "Epoch 12/30\n",
            "257/257 [==============================] - 0s 103us/sample - loss: 0.4748 - accuracy: 0.7821\n",
            "Epoch 13/30\n",
            "257/257 [==============================] - 0s 127us/sample - loss: 0.4789 - accuracy: 0.7860\n",
            "Epoch 14/30\n",
            "257/257 [==============================] - 0s 115us/sample - loss: 0.4658 - accuracy: 0.7977\n",
            "Epoch 15/30\n",
            "257/257 [==============================] - 0s 125us/sample - loss: 0.4806 - accuracy: 0.7938\n",
            "Epoch 16/30\n",
            "257/257 [==============================] - 0s 121us/sample - loss: 0.4560 - accuracy: 0.7977\n",
            "Epoch 17/30\n",
            "257/257 [==============================] - 0s 117us/sample - loss: 0.4551 - accuracy: 0.8016\n",
            "Epoch 18/30\n",
            "257/257 [==============================] - 0s 137us/sample - loss: 0.4534 - accuracy: 0.7938\n",
            "Epoch 19/30\n",
            "257/257 [==============================] - 0s 111us/sample - loss: 0.4705 - accuracy: 0.7821\n",
            "Epoch 20/30\n",
            "257/257 [==============================] - 0s 115us/sample - loss: 0.4538 - accuracy: 0.7977\n",
            "Epoch 21/30\n",
            "257/257 [==============================] - 0s 116us/sample - loss: 0.4505 - accuracy: 0.8016\n",
            "Epoch 22/30\n",
            "257/257 [==============================] - 0s 144us/sample - loss: 0.4608 - accuracy: 0.7821\n",
            "Epoch 23/30\n",
            "257/257 [==============================] - 0s 140us/sample - loss: 0.4540 - accuracy: 0.7938\n",
            "Epoch 24/30\n",
            "257/257 [==============================] - 0s 116us/sample - loss: 0.4597 - accuracy: 0.7899\n",
            "Epoch 25/30\n",
            "257/257 [==============================] - 0s 116us/sample - loss: 0.4530 - accuracy: 0.7977\n",
            "Epoch 26/30\n",
            "257/257 [==============================] - 0s 116us/sample - loss: 0.4651 - accuracy: 0.7977\n",
            "Epoch 27/30\n",
            "257/257 [==============================] - 0s 117us/sample - loss: 0.4564 - accuracy: 0.8016\n",
            "Epoch 28/30\n",
            "257/257 [==============================] - 0s 104us/sample - loss: 0.4622 - accuracy: 0.7977\n",
            "Epoch 29/30\n",
            "257/257 [==============================] - 0s 115us/sample - loss: 0.4497 - accuracy: 0.8054\n",
            "Epoch 30/30\n",
            "257/257 [==============================] - 0s 127us/sample - loss: 0.4692 - accuracy: 0.7899\n",
            "Train on 225 samples\n",
            "Epoch 1/30\n",
            "225/225 [==============================] - 1s 3ms/sample - loss: 0.6861 - accuracy: 0.5467\n",
            "Epoch 2/30\n",
            "225/225 [==============================] - 0s 158us/sample - loss: 0.6545 - accuracy: 0.7600\n",
            "Epoch 3/30\n",
            "225/225 [==============================] - 0s 122us/sample - loss: 0.6165 - accuracy: 0.8000\n",
            "Epoch 4/30\n",
            "225/225 [==============================] - 0s 121us/sample - loss: 0.5775 - accuracy: 0.7822\n",
            "Epoch 5/30\n",
            "225/225 [==============================] - 0s 111us/sample - loss: 0.5081 - accuracy: 0.8089\n",
            "Epoch 6/30\n",
            "225/225 [==============================] - 0s 120us/sample - loss: 0.4764 - accuracy: 0.7956\n",
            "Epoch 7/30\n",
            "225/225 [==============================] - 0s 112us/sample - loss: 0.4395 - accuracy: 0.8133\n",
            "Epoch 8/30\n",
            "225/225 [==============================] - 0s 127us/sample - loss: 0.4175 - accuracy: 0.8000\n",
            "Epoch 9/30\n",
            "225/225 [==============================] - 0s 120us/sample - loss: 0.3974 - accuracy: 0.8089\n",
            "Epoch 10/30\n",
            "225/225 [==============================] - 0s 151us/sample - loss: 0.4002 - accuracy: 0.8044\n",
            "Epoch 11/30\n",
            "225/225 [==============================] - 0s 119us/sample - loss: 0.3945 - accuracy: 0.8178\n",
            "Epoch 12/30\n",
            "225/225 [==============================] - 0s 121us/sample - loss: 0.3780 - accuracy: 0.8178\n",
            "Epoch 13/30\n",
            "225/225 [==============================] - 0s 122us/sample - loss: 0.3915 - accuracy: 0.8089\n",
            "Epoch 14/30\n",
            "225/225 [==============================] - 0s 119us/sample - loss: 0.3734 - accuracy: 0.8178\n",
            "Epoch 15/30\n",
            "225/225 [==============================] - 0s 116us/sample - loss: 0.3781 - accuracy: 0.8178\n",
            "Epoch 16/30\n",
            "225/225 [==============================] - 0s 118us/sample - loss: 0.3824 - accuracy: 0.8178\n",
            "Epoch 17/30\n",
            "225/225 [==============================] - 0s 110us/sample - loss: 0.3835 - accuracy: 0.8089\n",
            "Epoch 18/30\n",
            "225/225 [==============================] - 0s 139us/sample - loss: 0.3770 - accuracy: 0.8178\n",
            "Epoch 19/30\n",
            "225/225 [==============================] - 0s 141us/sample - loss: 0.3944 - accuracy: 0.8044\n",
            "Epoch 20/30\n",
            "225/225 [==============================] - 0s 115us/sample - loss: 0.3726 - accuracy: 0.8267\n",
            "Epoch 21/30\n",
            "225/225 [==============================] - 0s 107us/sample - loss: 0.3703 - accuracy: 0.8178\n",
            "Epoch 22/30\n",
            "225/225 [==============================] - 0s 117us/sample - loss: 0.3695 - accuracy: 0.8267\n",
            "Epoch 23/30\n",
            "225/225 [==============================] - 0s 109us/sample - loss: 0.3768 - accuracy: 0.8089\n",
            "Epoch 24/30\n",
            "225/225 [==============================] - 0s 122us/sample - loss: 0.3781 - accuracy: 0.8133\n",
            "Epoch 25/30\n",
            "225/225 [==============================] - 0s 155us/sample - loss: 0.3640 - accuracy: 0.8222\n",
            "Epoch 26/30\n",
            "225/225 [==============================] - 0s 104us/sample - loss: 0.3737 - accuracy: 0.8222\n",
            "Epoch 27/30\n",
            "225/225 [==============================] - 0s 130us/sample - loss: 0.3841 - accuracy: 0.7911\n",
            "Epoch 28/30\n",
            "225/225 [==============================] - 0s 118us/sample - loss: 0.3730 - accuracy: 0.8178\n",
            "Epoch 29/30\n",
            "225/225 [==============================] - 0s 132us/sample - loss: 0.3594 - accuracy: 0.8356\n",
            "Epoch 30/30\n",
            "225/225 [==============================] - 0s 121us/sample - loss: 0.3845 - accuracy: 0.8000\n",
            "Train on 249 samples\n",
            "Epoch 1/30\n",
            "249/249 [==============================] - 1s 2ms/sample - loss: 0.6956 - accuracy: 0.5141\n",
            "Epoch 2/30\n",
            "249/249 [==============================] - 0s 120us/sample - loss: 0.6660 - accuracy: 0.7470\n",
            "Epoch 3/30\n",
            "249/249 [==============================] - 0s 108us/sample - loss: 0.6337 - accuracy: 0.7831\n",
            "Epoch 4/30\n",
            "249/249 [==============================] - 0s 114us/sample - loss: 0.5947 - accuracy: 0.7550\n",
            "Epoch 5/30\n",
            "249/249 [==============================] - 0s 93us/sample - loss: 0.5477 - accuracy: 0.7791\n",
            "Epoch 6/30\n",
            "249/249 [==============================] - 0s 106us/sample - loss: 0.5104 - accuracy: 0.7912\n",
            "Epoch 7/30\n",
            "249/249 [==============================] - 0s 104us/sample - loss: 0.4750 - accuracy: 0.7912\n",
            "Epoch 8/30\n",
            "249/249 [==============================] - 0s 108us/sample - loss: 0.4843 - accuracy: 0.7671\n",
            "Epoch 9/30\n",
            "249/249 [==============================] - 0s 113us/sample - loss: 0.4746 - accuracy: 0.7831\n",
            "Epoch 10/30\n",
            "249/249 [==============================] - 0s 112us/sample - loss: 0.4723 - accuracy: 0.7671\n",
            "Epoch 11/30\n",
            "249/249 [==============================] - 0s 117us/sample - loss: 0.4866 - accuracy: 0.7671\n",
            "Epoch 12/30\n",
            "249/249 [==============================] - 0s 112us/sample - loss: 0.4860 - accuracy: 0.7751\n",
            "Epoch 13/30\n",
            "249/249 [==============================] - 0s 127us/sample - loss: 0.4814 - accuracy: 0.7791\n",
            "Epoch 14/30\n",
            "249/249 [==============================] - 0s 125us/sample - loss: 0.4721 - accuracy: 0.7871\n",
            "Epoch 15/30\n",
            "249/249 [==============================] - 0s 118us/sample - loss: 0.4625 - accuracy: 0.7992\n",
            "Epoch 16/30\n",
            "249/249 [==============================] - 0s 107us/sample - loss: 0.4754 - accuracy: 0.7791\n",
            "Epoch 17/30\n",
            "249/249 [==============================] - 0s 115us/sample - loss: 0.4660 - accuracy: 0.7831\n",
            "Epoch 18/30\n",
            "249/249 [==============================] - 0s 132us/sample - loss: 0.4661 - accuracy: 0.7791\n",
            "Epoch 19/30\n",
            "249/249 [==============================] - 0s 127us/sample - loss: 0.4680 - accuracy: 0.7871\n",
            "Epoch 20/30\n",
            "249/249 [==============================] - 0s 144us/sample - loss: 0.4590 - accuracy: 0.7831\n",
            "Epoch 21/30\n",
            "249/249 [==============================] - 0s 128us/sample - loss: 0.4643 - accuracy: 0.7831\n",
            "Epoch 22/30\n",
            "249/249 [==============================] - 0s 119us/sample - loss: 0.4663 - accuracy: 0.7751\n",
            "Epoch 23/30\n",
            "249/249 [==============================] - 0s 117us/sample - loss: 0.4528 - accuracy: 0.7831\n",
            "Epoch 24/30\n",
            "249/249 [==============================] - 0s 111us/sample - loss: 0.4488 - accuracy: 0.7912\n",
            "Epoch 25/30\n",
            "249/249 [==============================] - 0s 129us/sample - loss: 0.4458 - accuracy: 0.7952\n",
            "Epoch 26/30\n",
            "249/249 [==============================] - 0s 141us/sample - loss: 0.4543 - accuracy: 0.7791\n",
            "Epoch 27/30\n",
            "249/249 [==============================] - 0s 143us/sample - loss: 0.4538 - accuracy: 0.7912\n",
            "Epoch 28/30\n",
            "249/249 [==============================] - 0s 123us/sample - loss: 0.4437 - accuracy: 0.7952\n",
            "Epoch 29/30\n",
            "249/249 [==============================] - 0s 133us/sample - loss: 0.4461 - accuracy: 0.7912\n",
            "Epoch 30/30\n",
            "249/249 [==============================] - 0s 149us/sample - loss: 0.4573 - accuracy: 0.7871\n",
            "Train on 262 samples\n",
            "Epoch 1/30\n",
            "262/262 [==============================] - 1s 2ms/sample - loss: 0.6757 - accuracy: 0.5763\n",
            "Epoch 2/30\n",
            "262/262 [==============================] - 0s 119us/sample - loss: 0.6242 - accuracy: 0.7099\n",
            "Epoch 3/30\n",
            "262/262 [==============================] - 0s 122us/sample - loss: 0.5818 - accuracy: 0.7748\n",
            "Epoch 4/30\n",
            "262/262 [==============================] - 0s 106us/sample - loss: 0.5279 - accuracy: 0.7977\n",
            "Epoch 5/30\n",
            "262/262 [==============================] - 0s 123us/sample - loss: 0.4674 - accuracy: 0.8092\n",
            "Epoch 6/30\n",
            "262/262 [==============================] - 0s 118us/sample - loss: 0.4439 - accuracy: 0.8321\n",
            "Epoch 7/30\n",
            "262/262 [==============================] - 0s 121us/sample - loss: 0.3972 - accuracy: 0.8321\n",
            "Epoch 8/30\n",
            "262/262 [==============================] - 0s 127us/sample - loss: 0.4067 - accuracy: 0.8435\n",
            "Epoch 9/30\n",
            "262/262 [==============================] - 0s 117us/sample - loss: 0.3757 - accuracy: 0.8511\n",
            "Epoch 10/30\n",
            "262/262 [==============================] - 0s 133us/sample - loss: 0.3796 - accuracy: 0.8359\n",
            "Epoch 11/30\n",
            "262/262 [==============================] - 0s 113us/sample - loss: 0.3867 - accuracy: 0.8321\n",
            "Epoch 12/30\n",
            "262/262 [==============================] - 0s 119us/sample - loss: 0.3876 - accuracy: 0.8359\n",
            "Epoch 13/30\n",
            "262/262 [==============================] - 0s 126us/sample - loss: 0.3761 - accuracy: 0.8359\n",
            "Epoch 14/30\n",
            "262/262 [==============================] - 0s 114us/sample - loss: 0.3777 - accuracy: 0.8435\n",
            "Epoch 15/30\n",
            "262/262 [==============================] - 0s 380us/sample - loss: 0.3916 - accuracy: 0.8397\n",
            "Epoch 16/30\n",
            "262/262 [==============================] - 0s 141us/sample - loss: 0.3878 - accuracy: 0.8321\n",
            "Epoch 17/30\n",
            "262/262 [==============================] - 0s 107us/sample - loss: 0.3734 - accuracy: 0.8397\n",
            "Epoch 18/30\n",
            "262/262 [==============================] - 0s 108us/sample - loss: 0.3604 - accuracy: 0.8435\n",
            "Epoch 19/30\n",
            "262/262 [==============================] - 0s 123us/sample - loss: 0.3745 - accuracy: 0.8550\n",
            "Epoch 20/30\n",
            "262/262 [==============================] - 0s 103us/sample - loss: 0.3686 - accuracy: 0.8511\n",
            "Epoch 21/30\n",
            "262/262 [==============================] - 0s 105us/sample - loss: 0.3787 - accuracy: 0.8511\n",
            "Epoch 22/30\n",
            "262/262 [==============================] - 0s 107us/sample - loss: 0.3590 - accuracy: 0.8321\n",
            "Epoch 23/30\n",
            "262/262 [==============================] - 0s 107us/sample - loss: 0.3861 - accuracy: 0.8359\n",
            "Epoch 24/30\n",
            "262/262 [==============================] - 0s 108us/sample - loss: 0.3807 - accuracy: 0.8435\n",
            "Epoch 25/30\n",
            "262/262 [==============================] - 0s 108us/sample - loss: 0.3695 - accuracy: 0.8550\n",
            "Epoch 26/30\n",
            "262/262 [==============================] - 0s 109us/sample - loss: 0.3642 - accuracy: 0.8435\n",
            "Epoch 27/30\n",
            "262/262 [==============================] - 0s 91us/sample - loss: 0.3551 - accuracy: 0.8435\n",
            "Epoch 28/30\n",
            "262/262 [==============================] - 0s 125us/sample - loss: 0.3730 - accuracy: 0.8359\n",
            "Epoch 29/30\n",
            "262/262 [==============================] - 0s 117us/sample - loss: 0.3736 - accuracy: 0.8397\n",
            "Epoch 30/30\n",
            "262/262 [==============================] - 0s 110us/sample - loss: 0.3740 - accuracy: 0.8321\n",
            "Attack accuracy: 0.54125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}